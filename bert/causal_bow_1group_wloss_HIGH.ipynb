{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from data import CausalBertDataset, MLMDataset\n",
    "from causal_bert import CausalBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 256\n",
    "epoch = 500\n",
    "hidden_size = 64\n",
    "lr = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def true_casual_effect(data_loader, effect='ate', estimation='q'):\n",
    "    assert effect == 'ate' and estimation == 'q', f'unallowed effect/estimation: {effect}/{estimation}'\n",
    "    \n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    Q1 = dataset.treatment * dataset.response + (1 - dataset.treatment) * dataset.pseudo_response\n",
    "    Q1 = Q1.cpu().data.numpy().squeeze()\n",
    "\n",
    "    Q0 = dataset.treatment * dataset.pseudo_response + (1 - dataset.treatment) * dataset.response\n",
    "    Q0 = Q0.cpu().data.numpy().squeeze()\n",
    "\n",
    "    treatment = dataset.treatment.cpu().data.numpy().squeeze()\n",
    "    prop_scores = dataset.prop_scores.cpu().data.numpy().squeeze()\n",
    "    \n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (treatment * (Q1 - Q0))\n",
    "            return phi.sum() / treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            return (Q1 - Q0).mean()\n",
    "        \n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            return phi / treatment.mean()\n",
    "        elif effect == 'ate': \n",
    "            return phi\n",
    "        \n",
    "def est_casual_effect(data_loader, model, effect='ate', estimation='q', evaluate=True, **kwargs):\n",
    "    # We use `real_treatment` here to emphasize the estimations use real instead of estimated treatment.\n",
    "    real_response, real_treatment, real_prop_scores = [], [], []\n",
    "    prop_scores, Q1, Q0 = [], [], []\n",
    "    \n",
    "    if evaluate:\n",
    "        g_loss = kwargs.get('g_loss')\n",
    "        q_loss = kwargs.get('q_loss')\n",
    "        g_loss_test, q1_loss_test, q0_loss_test  = [], [], []\n",
    "        \n",
    "    model.eval()\n",
    "    for idx, (tokens, treatment, response, real_prop_score) in enumerate(data_loader):\n",
    "        real_response.append(response.cpu().data.numpy().squeeze())\n",
    "        real_treatment.append(treatment.cpu().data.numpy().squeeze())\n",
    "        real_prop_scores.append(real_prop_score.cpu().data.numpy().squeeze())\n",
    "\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        prop_scores.append(prop_score.cpu().data.numpy().squeeze())\n",
    "        Q1.append(q1.cpu().data.numpy().squeeze())\n",
    "        Q0.append(q0.cpu().data.numpy().squeeze())\n",
    "        \n",
    "        # Evaulate loss\n",
    "        if evaluate:\n",
    "            g_loss_val  = g_loss(prop_score, treatment)\n",
    "            q1_loss_val = q_loss(q1[treatment==1], response[treatment==1])\n",
    "            q0_loss_val = q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "            g_loss_test.append(g_loss_val.item())\n",
    "            q1_loss_test.append(q1_loss_val.item())\n",
    "            q0_loss_test.append(q0_loss_val.item())\n",
    "    \n",
    "    g_loss = np.array(g_loss_test).mean() if evaluate else None\n",
    "    q1_loss = np.array(q1_loss_test).mean() if evaluate else None\n",
    "    q0_loss = np.array(q0_loss_test).mean() if evaluate else None\n",
    "\n",
    "    Q1 = np.concatenate(Q1, axis=0)\n",
    "    Q0 = np.concatenate(Q0, axis=0)\n",
    "    prop_scores = np.concatenate(prop_scores, axis=0)\n",
    "    \n",
    "    real_response = np.concatenate(real_response, axis=0)\n",
    "    real_treatment = np.concatenate(real_treatment, axis=0)\n",
    "    real_prop_scores = np.concatenate(real_prop_scores, axis=0)\n",
    "    \n",
    "    # Evaluate accuracy.\n",
    "    if evaluate:\n",
    "        dataset = data_loader.dataset\n",
    "        \n",
    "        real_q1_prob = sigmoid(dataset.alpha + dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        real_q0_prob = sigmoid(dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        thre = (real_q1_prob + real_q0_prob) / 2\n",
    "\n",
    "    # prop score: real and estimated must locate one the same side of 0.5.\n",
    "    prop_accu = (1. * (((real_prop_scores - .5) * (prop_scores - .5)) > 0.)).mean() if evaluate else None\n",
    "    # q: estimate is more close to corresponding real value than the other.\n",
    "    q1_accu = (1. * (dataset.alpha > 0) * (Q1 > thre)).mean() if evaluate else None\n",
    "    q0_accu = (1. * (dataset.alpha > 0) * (Q0 < thre)).mean() if evaluate else None\n",
    "\n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (real_treatment * (Q1 - Q0))\n",
    "            effect = phi.sum() / real_treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            effect = (Q1 - Q0).mean()\n",
    "\n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            effect = phi / real_treatment.mean()\n",
    "        elif effect == 'ate':\n",
    "            effect = phi\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return effect, g_loss, q1_loss, q0_loss, prop_accu, q1_accu, q0_accu\n",
    "\n",
    "def show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch):\n",
    "    train_loss_hist = np.array(train_loss_hist)\n",
    "    test_loss_hist = np.array(test_loss_hist)\n",
    "    est_effect = np.array(est_effect)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    lns1 = ax.plot(np.arange(epoch), test_loss_hist, label='Eval loss')\n",
    "    ax_r = plt.twinx()\n",
    "    lns2 = ax_r.plot(np.arange(epoch), est_effect, color='coral', label='Estimate ATE')\n",
    "    lns3 = ax_r.plot(np.arange(epoch), np.ones(epoch) * real, color='red', ls='--', label='Real ATE')\n",
    "    lns4 = ax_r.plot(np.arange(epoch), np.ones(epoch) * unadjust, color='green', ls='--', label='Unadjusted ATE')\n",
    "\n",
    "    lns = lns1+lns2+lns3+lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax_r.legend(lns, labs, loc=0)\n",
    "    ax.set_ylabel('Eval loss')\n",
    "    ax_r.set_ylabel('ATEs')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "alpha = 0.75\n",
    "beta = 25.\n",
    "c = 0.2\n",
    "i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training set in 69.51 sec\n",
      "Load validation set in 67.07 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "                             group=list(range(1)), max_length=512, min_length=10,\n",
    "                             truncate_method='first', device=device, seed=1)\n",
    "\n",
    "print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "start = time.time()\n",
    "testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                            alpha=alpha, beta=beta, c=c, i=i, \n",
    "                            group=[9], max_length=512, min_length=10,\n",
    "                            truncate_method='first', device=device)\n",
    "\n",
    "print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [effect: ate], [estimation: q], [value: 0.16917]\n",
      "Unadjusted: [value: 0.2379]\n"
     ]
    }
   ],
   "source": [
    "real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "token_embed = model.get_input_embeddings()\n",
    "model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=hidden_size, prop_is_logit=True).to(device)\n",
    "\n",
    "pos_portion = trainset.treatment.mean()\n",
    "pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "epoch_iter = len(train_loader)\n",
    "total_steps = epoch * epoch_iter\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "q_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCELoss()\n",
    "prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Please specify the effect and estimation we want to use here.\n",
    "effect = 'ate'\n",
    "estimation = 'q'\n",
    "\n",
    "effect = effect.lower()\n",
    "estimation = estimation.lower()\n",
    "assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 500, time cost: 44.23 sec, \n",
      "          Loss: [Train: 2.40012], [Test: 2.39328],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.00000], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.06623], [test: 0.06623]\n",
      "********************************************************************************\n",
      "epoch: 2 / 500, time cost: 46.93 sec, \n",
      "          Loss: [Train: 2.37382], [Test: 2.36554],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.10174], [test: 0.10173]\n",
      "********************************************************************************\n",
      "epoch: 3 / 500, time cost: 50.24 sec, \n",
      "          Loss: [Train: 2.34548], [Test: 2.34082],\n",
      "          Accuracy: [prop score:  0.93655], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14891], [test: 0.14890]\n",
      "********************************************************************************\n",
      "epoch: 4 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.32501], [Test: 2.32703],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19338], [test: 0.19336]\n",
      "********************************************************************************\n",
      "epoch: 5 / 500, time cost: 50.47 sec, \n",
      "          Loss: [Train: 2.31492], [Test: 2.32256],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.22215], [test: 0.22213]\n",
      "********************************************************************************\n",
      "epoch: 6 / 500, time cost: 46.25 sec, \n",
      "          Loss: [Train: 2.31247], [Test: 2.32203],\n",
      "          Accuracy: [prop score:  0.93648], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23498], [test: 0.23496]\n",
      "********************************************************************************\n",
      "epoch: 7 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.31188], [Test: 2.32173],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23972], [test: 0.23970]\n",
      "********************************************************************************\n",
      "epoch: 8 / 500, time cost: 46.45 sec, \n",
      "          Loss: [Train: 2.31101], [Test: 2.32125],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24188], [test: 0.24186]\n",
      "********************************************************************************\n",
      "epoch: 9 / 500, time cost: 48.59 sec, \n",
      "          Loss: [Train: 2.31128], [Test: 2.32133],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24094], [test: 0.24092]\n",
      "********************************************************************************\n",
      "epoch: 10 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.31133], [Test: 2.32120],\n",
      "          Accuracy: [prop score:  0.93642], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23972], [test: 0.23970]\n",
      "********************************************************************************\n",
      "epoch: 11 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.31074], [Test: 2.32079],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24019], [test: 0.24017]\n",
      "********************************************************************************\n",
      "epoch: 12 / 500, time cost: 46.45 sec, \n",
      "          Loss: [Train: 2.31055], [Test: 2.32046],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24109], [test: 0.24107]\n",
      "********************************************************************************\n",
      "epoch: 13 / 500, time cost: 48.81 sec, \n",
      "          Loss: [Train: 2.31062], [Test: 2.32055],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24099], [test: 0.24097]\n",
      "********************************************************************************\n",
      "epoch: 14 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.31051], [Test: 2.32084],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24013], [test: 0.24012]\n",
      "********************************************************************************\n",
      "epoch: 15 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.31023], [Test: 2.32067],\n",
      "          Accuracy: [prop score:  0.93641], [q1: 0.93641], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24101], [test: 0.24099]\n",
      "********************************************************************************\n",
      "epoch: 16 / 500, time cost: 46.67 sec, \n",
      "          Loss: [Train: 2.31044], [Test: 2.31993],\n",
      "          Accuracy: [prop score:  0.93653], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24034], [test: 0.24032]\n",
      "********************************************************************************\n",
      "epoch: 17 / 500, time cost: 48.78 sec, \n",
      "          Loss: [Train: 2.30923], [Test: 2.32026],\n",
      "          Accuracy: [prop score:  0.93657], [q1: 0.93657], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24222], [test: 0.24220]\n",
      "********************************************************************************\n",
      "epoch: 18 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 2.30960], [Test: 2.31966],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24200], [test: 0.24198]\n",
      "********************************************************************************\n",
      "epoch: 19 / 500, time cost: 48.86 sec, \n",
      "          Loss: [Train: 2.30931], [Test: 2.31946],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24033], [test: 0.24031]\n",
      "********************************************************************************\n",
      "epoch: 20 / 500, time cost: 46.69 sec, \n",
      "          Loss: [Train: 2.30909], [Test: 2.31975],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24067], [test: 0.24064]\n",
      "********************************************************************************\n",
      "epoch: 21 / 500, time cost: 48.39 sec, \n",
      "          Loss: [Train: 2.30842], [Test: 2.31891],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24105], [test: 0.24103]\n",
      "********************************************************************************\n",
      "epoch: 22 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.30861], [Test: 2.31910],\n",
      "          Accuracy: [prop score:  0.93649], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23917], [test: 0.23916]\n",
      "********************************************************************************\n",
      "epoch: 23 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.30848], [Test: 2.31886],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24088], [test: 0.24086]\n",
      "********************************************************************************\n",
      "epoch: 24 / 500, time cost: 46.48 sec, \n",
      "          Loss: [Train: 2.30808], [Test: 2.31881],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24082], [test: 0.24080]\n",
      "********************************************************************************\n",
      "epoch: 25 / 500, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.30791], [Test: 2.31820],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24035], [test: 0.24033]\n",
      "********************************************************************************\n",
      "epoch: 26 / 500, time cost: 46.37 sec, \n",
      "          Loss: [Train: 2.30788], [Test: 2.31852],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23994], [test: 0.23992]\n",
      "********************************************************************************\n",
      "epoch: 27 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.30738], [Test: 2.31792],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24202], [test: 0.24200]\n",
      "********************************************************************************\n",
      "epoch: 28 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.30724], [Test: 2.31793],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24072], [test: 0.24071]\n",
      "********************************************************************************\n",
      "epoch: 29 / 500, time cost: 48.31 sec, \n",
      "          Loss: [Train: 2.30703], [Test: 2.31697],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23976], [test: 0.23974]\n",
      "********************************************************************************\n",
      "epoch: 30 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.30654], [Test: 2.31718],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23918], [test: 0.23917]\n",
      "********************************************************************************\n",
      "epoch: 31 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.30608], [Test: 2.31703],\n",
      "          Accuracy: [prop score:  0.93649], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24075], [test: 0.24073]\n",
      "********************************************************************************\n",
      "epoch: 32 / 500, time cost: 45.82 sec, \n",
      "          Loss: [Train: 2.30601], [Test: 2.31674],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24075], [test: 0.24073]\n",
      "********************************************************************************\n",
      "epoch: 33 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.30581], [Test: 2.31635],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24050], [test: 0.24048]\n",
      "********************************************************************************\n",
      "epoch: 34 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.30547], [Test: 2.31633],\n",
      "          Accuracy: [prop score:  0.93653], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23992], [test: 0.23991]\n",
      "********************************************************************************\n",
      "epoch: 35 / 500, time cost: 48.24 sec, \n",
      "          Loss: [Train: 2.30514], [Test: 2.31627],\n",
      "          Accuracy: [prop score:  0.93659], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24116], [test: 0.24115]\n",
      "********************************************************************************\n",
      "epoch: 36 / 500, time cost: 46.23 sec, \n",
      "          Loss: [Train: 2.30493], [Test: 2.31616],\n",
      "          Accuracy: [prop score:  0.93655], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24094], [test: 0.24093]\n",
      "********************************************************************************\n",
      "epoch: 37 / 500, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.30408], [Test: 2.31562],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24057], [test: 0.24055]\n",
      "********************************************************************************\n",
      "epoch: 38 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.30407], [Test: 2.31482],\n",
      "          Accuracy: [prop score:  0.93654], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24089], [test: 0.24087]\n",
      "********************************************************************************\n",
      "epoch: 39 / 500, time cost: 48.28 sec, \n",
      "          Loss: [Train: 2.30381], [Test: 2.31482],\n",
      "          Accuracy: [prop score:  0.93662], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24054], [test: 0.24053]\n",
      "********************************************************************************\n",
      "epoch: 40 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.30340], [Test: 2.31500],\n",
      "          Accuracy: [prop score:  0.93658], [q1: 0.93645], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24258], [test: 0.24257]\n",
      "********************************************************************************\n",
      "epoch: 41 / 500, time cost: 47.53 sec, \n",
      "          Loss: [Train: 2.30303], [Test: 2.31435],\n",
      "          Accuracy: [prop score:  0.93660], [q1: 0.93643], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24168], [test: 0.24167]\n",
      "********************************************************************************\n",
      "epoch: 42 / 500, time cost: 45.50 sec, \n",
      "          Loss: [Train: 2.30257], [Test: 2.31396],\n",
      "          Accuracy: [prop score:  0.93668], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24078], [test: 0.24078]\n",
      "********************************************************************************\n",
      "epoch: 43 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.30253], [Test: 2.31364],\n",
      "          Accuracy: [prop score:  0.93664], [q1: 0.93641], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24056], [test: 0.24055]\n",
      "********************************************************************************\n",
      "epoch: 44 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.30204], [Test: 2.31323],\n",
      "          Accuracy: [prop score:  0.93682], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24036], [test: 0.24035]\n",
      "********************************************************************************\n",
      "epoch: 45 / 500, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.30139], [Test: 2.31302],\n",
      "          Accuracy: [prop score:  0.93683], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24198], [test: 0.24197]\n",
      "********************************************************************************\n",
      "epoch: 46 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 2.30172], [Test: 2.31250],\n",
      "          Accuracy: [prop score:  0.93709], [q1: 0.93662], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23982], [test: 0.23981]\n",
      "********************************************************************************\n",
      "epoch: 47 / 500, time cost: 48.34 sec, \n",
      "          Loss: [Train: 2.30101], [Test: 2.31219],\n",
      "          Accuracy: [prop score:  0.93699], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.24034], [test: 0.24033]\n",
      "********************************************************************************\n",
      "epoch: 48 / 500, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.30008], [Test: 2.31186],\n",
      "          Accuracy: [prop score:  0.93714], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23892], [test: 0.23891]\n",
      "********************************************************************************\n",
      "epoch: 49 / 500, time cost: 48.55 sec, \n",
      "          Loss: [Train: 2.29980], [Test: 2.31150],\n",
      "          Accuracy: [prop score:  0.93716], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23929], [test: 0.23929]\n",
      "********************************************************************************\n",
      "epoch: 50 / 500, time cost: 45.94 sec, \n",
      "          Loss: [Train: 2.29973], [Test: 2.31111],\n",
      "          Accuracy: [prop score:  0.93722], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23845], [test: 0.23845]\n",
      "********************************************************************************\n",
      "epoch: 51 / 500, time cost: 48.76 sec, \n",
      "          Loss: [Train: 2.29947], [Test: 2.31016],\n",
      "          Accuracy: [prop score:  0.93719], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23746], [test: 0.23746]\n",
      "********************************************************************************\n",
      "epoch: 52 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.29868], [Test: 2.31008],\n",
      "          Accuracy: [prop score:  0.93727], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23801], [test: 0.23801]\n",
      "********************************************************************************\n",
      "epoch: 53 / 500, time cost: 48.42 sec, \n",
      "          Loss: [Train: 2.29821], [Test: 2.30976],\n",
      "          Accuracy: [prop score:  0.93745], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23875], [test: 0.23875]\n",
      "********************************************************************************\n",
      "epoch: 54 / 500, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.29755], [Test: 2.30914],\n",
      "          Accuracy: [prop score:  0.93751], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23740], [test: 0.23740]\n",
      "********************************************************************************\n",
      "epoch: 55 / 500, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.29702], [Test: 2.30913],\n",
      "          Accuracy: [prop score:  0.93753], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23813], [test: 0.23813]\n",
      "********************************************************************************\n",
      "epoch: 56 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.29678], [Test: 2.30865],\n",
      "          Accuracy: [prop score:  0.93758], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23760], [test: 0.23760]\n",
      "********************************************************************************\n",
      "epoch: 57 / 500, time cost: 48.12 sec, \n",
      "          Loss: [Train: 2.29622], [Test: 2.30843],\n",
      "          Accuracy: [prop score:  0.93779], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23942], [test: 0.23942]\n",
      "********************************************************************************\n",
      "epoch: 58 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.29589], [Test: 2.30709],\n",
      "          Accuracy: [prop score:  0.93782], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23772], [test: 0.23773]\n",
      "********************************************************************************\n",
      "epoch: 59 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.29539], [Test: 2.30723],\n",
      "          Accuracy: [prop score:  0.93797], [q1: 0.93658], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23786], [test: 0.23787]\n",
      "********************************************************************************\n",
      "epoch: 60 / 500, time cost: 45.94 sec, \n",
      "          Loss: [Train: 2.29472], [Test: 2.30698],\n",
      "          Accuracy: [prop score:  0.93793], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23732], [test: 0.23733]\n",
      "********************************************************************************\n",
      "epoch: 61 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.29406], [Test: 2.30580],\n",
      "          Accuracy: [prop score:  0.93802], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23799], [test: 0.23800]\n",
      "********************************************************************************\n",
      "epoch: 62 / 500, time cost: 45.99 sec, \n",
      "          Loss: [Train: 2.29357], [Test: 2.30565],\n",
      "          Accuracy: [prop score:  0.93821], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23731], [test: 0.23732]\n",
      "********************************************************************************\n",
      "epoch: 63 / 500, time cost: 48.09 sec, \n",
      "          Loss: [Train: 2.29313], [Test: 2.30507],\n",
      "          Accuracy: [prop score:  0.93841], [q1: 0.93656], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23851], [test: 0.23852]\n",
      "********************************************************************************\n",
      "epoch: 64 / 500, time cost: 46.34 sec, \n",
      "          Loss: [Train: 2.29258], [Test: 2.30451],\n",
      "          Accuracy: [prop score:  0.93850], [q1: 0.93658], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23277], [test: 0.23278]\n",
      "********************************************************************************\n",
      "epoch: 65 / 500, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.29132], [Test: 2.30410],\n",
      "          Accuracy: [prop score:  0.93865], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23499], [test: 0.23502]\n",
      "********************************************************************************\n",
      "epoch: 66 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.29153], [Test: 2.30332],\n",
      "          Accuracy: [prop score:  0.93870], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23652], [test: 0.23654]\n",
      "********************************************************************************\n",
      "epoch: 67 / 500, time cost: 47.41 sec, \n",
      "          Loss: [Train: 2.28990], [Test: 2.30284],\n",
      "          Accuracy: [prop score:  0.93883], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23658], [test: 0.23661]\n",
      "********************************************************************************\n",
      "epoch: 68 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.28992], [Test: 2.30268],\n",
      "          Accuracy: [prop score:  0.93900], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23663], [test: 0.23666]\n",
      "********************************************************************************\n",
      "epoch: 69 / 500, time cost: 47.48 sec, \n",
      "          Loss: [Train: 2.28975], [Test: 2.30231],\n",
      "          Accuracy: [prop score:  0.93907], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23475], [test: 0.23478]\n",
      "********************************************************************************\n",
      "epoch: 70 / 500, time cost: 45.30 sec, \n",
      "          Loss: [Train: 2.28896], [Test: 2.30141],\n",
      "          Accuracy: [prop score:  0.93923], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23747], [test: 0.23750]\n",
      "********************************************************************************\n",
      "epoch: 71 / 500, time cost: 47.57 sec, \n",
      "          Loss: [Train: 2.28779], [Test: 2.30088],\n",
      "          Accuracy: [prop score:  0.93922], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23427], [test: 0.23431]\n",
      "********************************************************************************\n",
      "epoch: 72 / 500, time cost: 45.31 sec, \n",
      "          Loss: [Train: 2.28714], [Test: 2.30067],\n",
      "          Accuracy: [prop score:  0.93930], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23662], [test: 0.23665]\n",
      "********************************************************************************\n",
      "epoch: 73 / 500, time cost: 47.63 sec, \n",
      "          Loss: [Train: 2.28663], [Test: 2.29979],\n",
      "          Accuracy: [prop score:  0.93938], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23423], [test: 0.23427]\n",
      "********************************************************************************\n",
      "epoch: 74 / 500, time cost: 45.52 sec, \n",
      "          Loss: [Train: 2.28654], [Test: 2.29971],\n",
      "          Accuracy: [prop score:  0.93951], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23615], [test: 0.23619]\n",
      "********************************************************************************\n",
      "epoch: 75 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.28564], [Test: 2.29879],\n",
      "          Accuracy: [prop score:  0.93946], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23380], [test: 0.23384]\n",
      "********************************************************************************\n",
      "epoch: 76 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.28533], [Test: 2.29858],\n",
      "          Accuracy: [prop score:  0.93968], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23171], [test: 0.23176]\n",
      "********************************************************************************\n",
      "epoch: 77 / 500, time cost: 47.72 sec, \n",
      "          Loss: [Train: 2.28444], [Test: 2.29819],\n",
      "          Accuracy: [prop score:  0.93974], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23476], [test: 0.23481]\n",
      "********************************************************************************\n",
      "epoch: 78 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.28443], [Test: 2.29783],\n",
      "          Accuracy: [prop score:  0.93964], [q1: 0.93640], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23487], [test: 0.23493]\n",
      "********************************************************************************\n",
      "epoch: 79 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.28357], [Test: 2.29716],\n",
      "          Accuracy: [prop score:  0.93978], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23287], [test: 0.23293]\n",
      "********************************************************************************\n",
      "epoch: 80 / 500, time cost: 45.68 sec, \n",
      "          Loss: [Train: 2.28285], [Test: 2.29616],\n",
      "          Accuracy: [prop score:  0.93984], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23235], [test: 0.23242]\n",
      "********************************************************************************\n",
      "epoch: 81 / 500, time cost: 47.29 sec, \n",
      "          Loss: [Train: 2.28218], [Test: 2.29598],\n",
      "          Accuracy: [prop score:  0.93995], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23214], [test: 0.23221]\n",
      "********************************************************************************\n",
      "epoch: 82 / 500, time cost: 45.66 sec, \n",
      "          Loss: [Train: 2.28096], [Test: 2.29583],\n",
      "          Accuracy: [prop score:  0.94007], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.23405], [test: 0.23412]\n",
      "********************************************************************************\n",
      "epoch: 83 / 500, time cost: 47.81 sec, \n",
      "          Loss: [Train: 2.28077], [Test: 2.29482],\n",
      "          Accuracy: [prop score:  0.94008], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.22945], [test: 0.22951]\n",
      "********************************************************************************\n",
      "epoch: 84 / 500, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.28026], [Test: 2.29471],\n",
      "          Accuracy: [prop score:  0.94006], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.22954], [test: 0.22960]\n",
      "********************************************************************************\n",
      "epoch: 85 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.27973], [Test: 2.29463],\n",
      "          Accuracy: [prop score:  0.94025], [q1: 0.93664], [q0: 0.99999],\n",
      "          Effect: [ate-q], [train: 0.23069], [test: 0.23076]\n",
      "********************************************************************************\n",
      "epoch: 86 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.27912], [Test: 2.29337],\n",
      "          Accuracy: [prop score:  0.94019], [q1: 0.93644], [q0: 0.99998],\n",
      "          Effect: [ate-q], [train: 0.22970], [test: 0.22976]\n",
      "********************************************************************************\n",
      "epoch: 87 / 500, time cost: 47.34 sec, \n",
      "          Loss: [Train: 2.27883], [Test: 2.29359],\n",
      "          Accuracy: [prop score:  0.94025], [q1: 0.93654], [q0: 0.99995],\n",
      "          Effect: [ate-q], [train: 0.22845], [test: 0.22853]\n",
      "********************************************************************************\n",
      "epoch: 88 / 500, time cost: 46.41 sec, \n",
      "          Loss: [Train: 2.27838], [Test: 2.29294],\n",
      "          Accuracy: [prop score:  0.94021], [q1: 0.93643], [q0: 0.99995],\n",
      "          Effect: [ate-q], [train: 0.23030], [test: 0.23039]\n",
      "********************************************************************************\n",
      "epoch: 89 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.27781], [Test: 2.29290],\n",
      "          Accuracy: [prop score:  0.94032], [q1: 0.93650], [q0: 0.99992],\n",
      "          Effect: [ate-q], [train: 0.22784], [test: 0.22791]\n",
      "********************************************************************************\n",
      "epoch: 90 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.27730], [Test: 2.29205],\n",
      "          Accuracy: [prop score:  0.94029], [q1: 0.93642], [q0: 0.99990],\n",
      "          Effect: [ate-q], [train: 0.22818], [test: 0.22827]\n",
      "********************************************************************************\n",
      "epoch: 91 / 500, time cost: 48.26 sec, \n",
      "          Loss: [Train: 2.27673], [Test: 2.29182],\n",
      "          Accuracy: [prop score:  0.94044], [q1: 0.93652], [q0: 0.99987],\n",
      "          Effect: [ate-q], [train: 0.22829], [test: 0.22838]\n",
      "********************************************************************************\n",
      "epoch: 92 / 500, time cost: 46.50 sec, \n",
      "          Loss: [Train: 2.27646], [Test: 2.29128],\n",
      "          Accuracy: [prop score:  0.94048], [q1: 0.93648], [q0: 0.99986],\n",
      "          Effect: [ate-q], [train: 0.22925], [test: 0.22934]\n",
      "********************************************************************************\n",
      "epoch: 93 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.27579], [Test: 2.29096],\n",
      "          Accuracy: [prop score:  0.94036], [q1: 0.93644], [q0: 0.99983],\n",
      "          Effect: [ate-q], [train: 0.22720], [test: 0.22728]\n",
      "********************************************************************************\n",
      "epoch: 94 / 500, time cost: 46.58 sec, \n",
      "          Loss: [Train: 2.27496], [Test: 2.29101],\n",
      "          Accuracy: [prop score:  0.94035], [q1: 0.93638], [q0: 0.99976],\n",
      "          Effect: [ate-q], [train: 0.22718], [test: 0.22727]\n",
      "********************************************************************************\n",
      "epoch: 95 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.27437], [Test: 2.29042],\n",
      "          Accuracy: [prop score:  0.94050], [q1: 0.93646], [q0: 0.99974],\n",
      "          Effect: [ate-q], [train: 0.22576], [test: 0.22586]\n",
      "********************************************************************************\n",
      "epoch: 96 / 500, time cost: 44.70 sec, \n",
      "          Loss: [Train: 2.27386], [Test: 2.28998],\n",
      "          Accuracy: [prop score:  0.94048], [q1: 0.93646], [q0: 0.99957],\n",
      "          Effect: [ate-q], [train: 0.22569], [test: 0.22578]\n",
      "********************************************************************************\n",
      "epoch: 97 / 500, time cost: 47.49 sec, \n",
      "          Loss: [Train: 2.27346], [Test: 2.28948],\n",
      "          Accuracy: [prop score:  0.94059], [q1: 0.93640], [q0: 0.99952],\n",
      "          Effect: [ate-q], [train: 0.22413], [test: 0.22422]\n",
      "********************************************************************************\n",
      "epoch: 98 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.27293], [Test: 2.28934],\n",
      "          Accuracy: [prop score:  0.94057], [q1: 0.93632], [q0: 0.99942],\n",
      "          Effect: [ate-q], [train: 0.22587], [test: 0.22597]\n",
      "********************************************************************************\n",
      "epoch: 99 / 500, time cost: 47.17 sec, \n",
      "          Loss: [Train: 2.27248], [Test: 2.28850],\n",
      "          Accuracy: [prop score:  0.94071], [q1: 0.93628], [q0: 0.99938],\n",
      "          Effect: [ate-q], [train: 0.22602], [test: 0.22613]\n",
      "********************************************************************************\n",
      "epoch: 100 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.27230], [Test: 2.28863],\n",
      "          Accuracy: [prop score:  0.94087], [q1: 0.93636], [q0: 0.99929],\n",
      "          Effect: [ate-q], [train: 0.22813], [test: 0.22823]\n",
      "********************************************************************************\n",
      "epoch: 101 / 500, time cost: 48.56 sec, \n",
      "          Loss: [Train: 2.27157], [Test: 2.28778],\n",
      "          Accuracy: [prop score:  0.94082], [q1: 0.93608], [q0: 0.99917],\n",
      "          Effect: [ate-q], [train: 0.22412], [test: 0.22422]\n",
      "********************************************************************************\n",
      "epoch: 102 / 500, time cost: 46.52 sec, \n",
      "          Loss: [Train: 2.27075], [Test: 2.28824],\n",
      "          Accuracy: [prop score:  0.94085], [q1: 0.93604], [q0: 0.99903],\n",
      "          Effect: [ate-q], [train: 0.22525], [test: 0.22538]\n",
      "********************************************************************************\n",
      "epoch: 103 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.27083], [Test: 2.28775],\n",
      "          Accuracy: [prop score:  0.94091], [q1: 0.93595], [q0: 0.99902],\n",
      "          Effect: [ate-q], [train: 0.22620], [test: 0.22630]\n",
      "********************************************************************************\n",
      "epoch: 104 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 2.27004], [Test: 2.28695],\n",
      "          Accuracy: [prop score:  0.94091], [q1: 0.93575], [q0: 0.99877],\n",
      "          Effect: [ate-q], [train: 0.22473], [test: 0.22485]\n",
      "********************************************************************************\n",
      "epoch: 105 / 500, time cost: 48.34 sec, \n",
      "          Loss: [Train: 2.26950], [Test: 2.28719],\n",
      "          Accuracy: [prop score:  0.94100], [q1: 0.93571], [q0: 0.99860],\n",
      "          Effect: [ate-q], [train: 0.22442], [test: 0.22454]\n",
      "********************************************************************************\n",
      "epoch: 106 / 500, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.26934], [Test: 2.28640],\n",
      "          Accuracy: [prop score:  0.94088], [q1: 0.93573], [q0: 0.99848],\n",
      "          Effect: [ate-q], [train: 0.22678], [test: 0.22690]\n",
      "********************************************************************************\n",
      "epoch: 107 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.26912], [Test: 2.28623],\n",
      "          Accuracy: [prop score:  0.94106], [q1: 0.93526], [q0: 0.99837],\n",
      "          Effect: [ate-q], [train: 0.22027], [test: 0.22038]\n",
      "********************************************************************************\n",
      "epoch: 108 / 500, time cost: 46.15 sec, \n",
      "          Loss: [Train: 2.26859], [Test: 2.28607],\n",
      "          Accuracy: [prop score:  0.94110], [q1: 0.93535], [q0: 0.99812],\n",
      "          Effect: [ate-q], [train: 0.22229], [test: 0.22242]\n",
      "********************************************************************************\n",
      "epoch: 109 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.26836], [Test: 2.28610],\n",
      "          Accuracy: [prop score:  0.94106], [q1: 0.93499], [q0: 0.99812],\n",
      "          Effect: [ate-q], [train: 0.22209], [test: 0.22222]\n",
      "********************************************************************************\n",
      "epoch: 110 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.26811], [Test: 2.28534],\n",
      "          Accuracy: [prop score:  0.94122], [q1: 0.93496], [q0: 0.99811],\n",
      "          Effect: [ate-q], [train: 0.22348], [test: 0.22361]\n",
      "********************************************************************************\n",
      "epoch: 111 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.26784], [Test: 2.28507],\n",
      "          Accuracy: [prop score:  0.94121], [q1: 0.93462], [q0: 0.99777],\n",
      "          Effect: [ate-q], [train: 0.22169], [test: 0.22181]\n",
      "********************************************************************************\n",
      "epoch: 112 / 500, time cost: 45.84 sec, \n",
      "          Loss: [Train: 2.26687], [Test: 2.28517],\n",
      "          Accuracy: [prop score:  0.94132], [q1: 0.93436], [q0: 0.99750],\n",
      "          Effect: [ate-q], [train: 0.22020], [test: 0.22033]\n",
      "********************************************************************************\n",
      "epoch: 113 / 500, time cost: 48.14 sec, \n",
      "          Loss: [Train: 2.26678], [Test: 2.28453],\n",
      "          Accuracy: [prop score:  0.94139], [q1: 0.93447], [q0: 0.99753],\n",
      "          Effect: [ate-q], [train: 0.22303], [test: 0.22316]\n",
      "********************************************************************************\n",
      "epoch: 114 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.26637], [Test: 2.28496],\n",
      "          Accuracy: [prop score:  0.94131], [q1: 0.93428], [q0: 0.99711],\n",
      "          Effect: [ate-q], [train: 0.22108], [test: 0.22122]\n",
      "********************************************************************************\n",
      "epoch: 115 / 500, time cost: 48.35 sec, \n",
      "          Loss: [Train: 2.26518], [Test: 2.28431],\n",
      "          Accuracy: [prop score:  0.94138], [q1: 0.93391], [q0: 0.99699],\n",
      "          Effect: [ate-q], [train: 0.21950], [test: 0.21963]\n",
      "********************************************************************************\n",
      "epoch: 116 / 500, time cost: 46.75 sec, \n",
      "          Loss: [Train: 2.26616], [Test: 2.28383],\n",
      "          Accuracy: [prop score:  0.94145], [q1: 0.93379], [q0: 0.99715],\n",
      "          Effect: [ate-q], [train: 0.22260], [test: 0.22274]\n",
      "********************************************************************************\n",
      "epoch: 117 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.26491], [Test: 2.28405],\n",
      "          Accuracy: [prop score:  0.94151], [q1: 0.93386], [q0: 0.99681],\n",
      "          Effect: [ate-q], [train: 0.22207], [test: 0.22221]\n",
      "********************************************************************************\n",
      "epoch: 118 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.26450], [Test: 2.28375],\n",
      "          Accuracy: [prop score:  0.94149], [q1: 0.93417], [q0: 0.99666],\n",
      "          Effect: [ate-q], [train: 0.22441], [test: 0.22455]\n",
      "********************************************************************************\n",
      "epoch: 119 / 500, time cost: 48.25 sec, \n",
      "          Loss: [Train: 2.26474], [Test: 2.28315],\n",
      "          Accuracy: [prop score:  0.94146], [q1: 0.93313], [q0: 0.99669],\n",
      "          Effect: [ate-q], [train: 0.22081], [test: 0.22095]\n",
      "********************************************************************************\n",
      "epoch: 120 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.26392], [Test: 2.28340],\n",
      "          Accuracy: [prop score:  0.94147], [q1: 0.93392], [q0: 0.99630],\n",
      "          Effect: [ate-q], [train: 0.22467], [test: 0.22481]\n",
      "********************************************************************************\n",
      "epoch: 121 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.26355], [Test: 2.28335],\n",
      "          Accuracy: [prop score:  0.94150], [q1: 0.93336], [q0: 0.99630],\n",
      "          Effect: [ate-q], [train: 0.22339], [test: 0.22352]\n",
      "********************************************************************************\n",
      "epoch: 122 / 500, time cost: 46.39 sec, \n",
      "          Loss: [Train: 2.26293], [Test: 2.28253],\n",
      "          Accuracy: [prop score:  0.94160], [q1: 0.93256], [q0: 0.99572],\n",
      "          Effect: [ate-q], [train: 0.21789], [test: 0.21803]\n",
      "********************************************************************************\n",
      "epoch: 123 / 500, time cost: 48.14 sec, \n",
      "          Loss: [Train: 2.26298], [Test: 2.28241],\n",
      "          Accuracy: [prop score:  0.94164], [q1: 0.93327], [q0: 0.99595],\n",
      "          Effect: [ate-q], [train: 0.22429], [test: 0.22443]\n",
      "********************************************************************************\n",
      "epoch: 124 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.26227], [Test: 2.28249],\n",
      "          Accuracy: [prop score:  0.94175], [q1: 0.93282], [q0: 0.99573],\n",
      "          Effect: [ate-q], [train: 0.22231], [test: 0.22245]\n",
      "********************************************************************************\n",
      "epoch: 125 / 500, time cost: 48.61 sec, \n",
      "          Loss: [Train: 2.26192], [Test: 2.28251],\n",
      "          Accuracy: [prop score:  0.94191], [q1: 0.93163], [q0: 0.99587],\n",
      "          Effect: [ate-q], [train: 0.21949], [test: 0.21963]\n",
      "********************************************************************************\n",
      "epoch: 126 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 2.26192], [Test: 2.28172],\n",
      "          Accuracy: [prop score:  0.94196], [q1: 0.93221], [q0: 0.99556],\n",
      "          Effect: [ate-q], [train: 0.22167], [test: 0.22183]\n",
      "********************************************************************************\n",
      "epoch: 127 / 500, time cost: 48.34 sec, \n",
      "          Loss: [Train: 2.26120], [Test: 2.28163],\n",
      "          Accuracy: [prop score:  0.94192], [q1: 0.93076], [q0: 0.99537],\n",
      "          Effect: [ate-q], [train: 0.21705], [test: 0.21720]\n",
      "********************************************************************************\n",
      "epoch: 128 / 500, time cost: 46.61 sec, \n",
      "          Loss: [Train: 2.26149], [Test: 2.28170],\n",
      "          Accuracy: [prop score:  0.94189], [q1: 0.93145], [q0: 0.99493],\n",
      "          Effect: [ate-q], [train: 0.21879], [test: 0.21897]\n",
      "********************************************************************************\n",
      "epoch: 129 / 500, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.26062], [Test: 2.28125],\n",
      "          Accuracy: [prop score:  0.94194], [q1: 0.93109], [q0: 0.99509],\n",
      "          Effect: [ate-q], [train: 0.21944], [test: 0.21958]\n",
      "********************************************************************************\n",
      "epoch: 130 / 500, time cost: 45.43 sec, \n",
      "          Loss: [Train: 2.26026], [Test: 2.28097],\n",
      "          Accuracy: [prop score:  0.94203], [q1: 0.93134], [q0: 0.99500],\n",
      "          Effect: [ate-q], [train: 0.22116], [test: 0.22130]\n",
      "********************************************************************************\n",
      "epoch: 131 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.25981], [Test: 2.28107],\n",
      "          Accuracy: [prop score:  0.94215], [q1: 0.92869], [q0: 0.99482],\n",
      "          Effect: [ate-q], [train: 0.21435], [test: 0.21451]\n",
      "********************************************************************************\n",
      "epoch: 132 / 500, time cost: 45.36 sec, \n",
      "          Loss: [Train: 2.25951], [Test: 2.28068],\n",
      "          Accuracy: [prop score:  0.94216], [q1: 0.92955], [q0: 0.99475],\n",
      "          Effect: [ate-q], [train: 0.21728], [test: 0.21744]\n",
      "********************************************************************************\n",
      "epoch: 133 / 500, time cost: 47.59 sec, \n",
      "          Loss: [Train: 2.25963], [Test: 2.28079],\n",
      "          Accuracy: [prop score:  0.94228], [q1: 0.92928], [q0: 0.99449],\n",
      "          Effect: [ate-q], [train: 0.21676], [test: 0.21691]\n",
      "********************************************************************************\n",
      "epoch: 134 / 500, time cost: 45.49 sec, \n",
      "          Loss: [Train: 2.25876], [Test: 2.28065],\n",
      "          Accuracy: [prop score:  0.94230], [q1: 0.93065], [q0: 0.99434],\n",
      "          Effect: [ate-q], [train: 0.22111], [test: 0.22125]\n",
      "********************************************************************************\n",
      "epoch: 135 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.25879], [Test: 2.28016],\n",
      "          Accuracy: [prop score:  0.94224], [q1: 0.92994], [q0: 0.99418],\n",
      "          Effect: [ate-q], [train: 0.21987], [test: 0.22002]\n",
      "********************************************************************************\n",
      "epoch: 136 / 500, time cost: 45.70 sec, \n",
      "          Loss: [Train: 2.25798], [Test: 2.28003],\n",
      "          Accuracy: [prop score:  0.94232], [q1: 0.92977], [q0: 0.99440],\n",
      "          Effect: [ate-q], [train: 0.22161], [test: 0.22180]\n",
      "********************************************************************************\n",
      "epoch: 137 / 500, time cost: 48.43 sec, \n",
      "          Loss: [Train: 2.25796], [Test: 2.27982],\n",
      "          Accuracy: [prop score:  0.94247], [q1: 0.92797], [q0: 0.99379],\n",
      "          Effect: [ate-q], [train: 0.21509], [test: 0.21524]\n",
      "********************************************************************************\n",
      "epoch: 138 / 500, time cost: 46.34 sec, \n",
      "          Loss: [Train: 2.25803], [Test: 2.27947],\n",
      "          Accuracy: [prop score:  0.94248], [q1: 0.92828], [q0: 0.99374],\n",
      "          Effect: [ate-q], [train: 0.21691], [test: 0.21708]\n",
      "********************************************************************************\n",
      "epoch: 139 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.25727], [Test: 2.27919],\n",
      "          Accuracy: [prop score:  0.94255], [q1: 0.92822], [q0: 0.99361],\n",
      "          Effect: [ate-q], [train: 0.21774], [test: 0.21791]\n",
      "********************************************************************************\n",
      "epoch: 140 / 500, time cost: 46.05 sec, \n",
      "          Loss: [Train: 2.25733], [Test: 2.27905],\n",
      "          Accuracy: [prop score:  0.94259], [q1: 0.92733], [q0: 0.99383],\n",
      "          Effect: [ate-q], [train: 0.21778], [test: 0.21796]\n",
      "********************************************************************************\n",
      "epoch: 141 / 500, time cost: 48.61 sec, \n",
      "          Loss: [Train: 2.25619], [Test: 2.27906],\n",
      "          Accuracy: [prop score:  0.94257], [q1: 0.92715], [q0: 0.99372],\n",
      "          Effect: [ate-q], [train: 0.21804], [test: 0.21822]\n",
      "********************************************************************************\n",
      "epoch: 142 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.25647], [Test: 2.27868],\n",
      "          Accuracy: [prop score:  0.94251], [q1: 0.92591], [q0: 0.99290],\n",
      "          Effect: [ate-q], [train: 0.21389], [test: 0.21408]\n",
      "********************************************************************************\n",
      "epoch: 143 / 500, time cost: 48.28 sec, \n",
      "          Loss: [Train: 2.25606], [Test: 2.27807],\n",
      "          Accuracy: [prop score:  0.94255], [q1: 0.92580], [q0: 0.99385],\n",
      "          Effect: [ate-q], [train: 0.21841], [test: 0.21859]\n",
      "********************************************************************************\n",
      "epoch: 144 / 500, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.25577], [Test: 2.27835],\n",
      "          Accuracy: [prop score:  0.94270], [q1: 0.92714], [q0: 0.99311],\n",
      "          Effect: [ate-q], [train: 0.21903], [test: 0.21920]\n",
      "********************************************************************************\n",
      "epoch: 145 / 500, time cost: 47.69 sec, \n",
      "          Loss: [Train: 2.25505], [Test: 2.27870],\n",
      "          Accuracy: [prop score:  0.94269], [q1: 0.92485], [q0: 0.99176],\n",
      "          Effect: [ate-q], [train: 0.21063], [test: 0.21082]\n",
      "********************************************************************************\n",
      "epoch: 146 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.25506], [Test: 2.27855],\n",
      "          Accuracy: [prop score:  0.94266], [q1: 0.92572], [q0: 0.99249],\n",
      "          Effect: [ate-q], [train: 0.21631], [test: 0.21650]\n",
      "********************************************************************************\n",
      "epoch: 147 / 500, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.25467], [Test: 2.27808],\n",
      "          Accuracy: [prop score:  0.94286], [q1: 0.92437], [q0: 0.99286],\n",
      "          Effect: [ate-q], [train: 0.21541], [test: 0.21559]\n",
      "********************************************************************************\n",
      "epoch: 148 / 500, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.25446], [Test: 2.27775],\n",
      "          Accuracy: [prop score:  0.94290], [q1: 0.92422], [q0: 0.99251],\n",
      "          Effect: [ate-q], [train: 0.21527], [test: 0.21544]\n",
      "********************************************************************************\n",
      "epoch: 149 / 500, time cost: 48.03 sec, \n",
      "          Loss: [Train: 2.25367], [Test: 2.27766],\n",
      "          Accuracy: [prop score:  0.94296], [q1: 0.92509], [q0: 0.99253],\n",
      "          Effect: [ate-q], [train: 0.21773], [test: 0.21792]\n",
      "********************************************************************************\n",
      "epoch: 150 / 500, time cost: 45.60 sec, \n",
      "          Loss: [Train: 2.25405], [Test: 2.27756],\n",
      "          Accuracy: [prop score:  0.94314], [q1: 0.92597], [q0: 0.99304],\n",
      "          Effect: [ate-q], [train: 0.22196], [test: 0.22216]\n",
      "********************************************************************************\n",
      "epoch: 151 / 500, time cost: 48.05 sec, \n",
      "          Loss: [Train: 2.25337], [Test: 2.27708],\n",
      "          Accuracy: [prop score:  0.94310], [q1: 0.92191], [q0: 0.99283],\n",
      "          Effect: [ate-q], [train: 0.21563], [test: 0.21584]\n",
      "********************************************************************************\n",
      "epoch: 152 / 500, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.25287], [Test: 2.27717],\n",
      "          Accuracy: [prop score:  0.94318], [q1: 0.92214], [q0: 0.99205],\n",
      "          Effect: [ate-q], [train: 0.21373], [test: 0.21393]\n",
      "********************************************************************************\n",
      "epoch: 153 / 500, time cost: 48.14 sec, \n",
      "          Loss: [Train: 2.25245], [Test: 2.27682],\n",
      "          Accuracy: [prop score:  0.94317], [q1: 0.92270], [q0: 0.99077],\n",
      "          Effect: [ate-q], [train: 0.21127], [test: 0.21147]\n",
      "********************************************************************************\n",
      "epoch: 154 / 500, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.25239], [Test: 2.27681],\n",
      "          Accuracy: [prop score:  0.94317], [q1: 0.92373], [q0: 0.99186],\n",
      "          Effect: [ate-q], [train: 0.21757], [test: 0.21778]\n",
      "********************************************************************************\n",
      "epoch: 155 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.25202], [Test: 2.27672],\n",
      "          Accuracy: [prop score:  0.94329], [q1: 0.92242], [q0: 0.99196],\n",
      "          Effect: [ate-q], [train: 0.21636], [test: 0.21654]\n",
      "********************************************************************************\n",
      "epoch: 156 / 500, time cost: 45.98 sec, \n",
      "          Loss: [Train: 2.25203], [Test: 2.27645],\n",
      "          Accuracy: [prop score:  0.94341], [q1: 0.92012], [q0: 0.99085],\n",
      "          Effect: [ate-q], [train: 0.21008], [test: 0.21027]\n",
      "********************************************************************************\n",
      "epoch: 157 / 500, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.25113], [Test: 2.27605],\n",
      "          Accuracy: [prop score:  0.94341], [q1: 0.91998], [q0: 0.99124],\n",
      "          Effect: [ate-q], [train: 0.21237], [test: 0.21257]\n",
      "********************************************************************************\n",
      "epoch: 158 / 500, time cost: 46.05 sec, \n",
      "          Loss: [Train: 2.25069], [Test: 2.27616],\n",
      "          Accuracy: [prop score:  0.94345], [q1: 0.91934], [q0: 0.99110],\n",
      "          Effect: [ate-q], [train: 0.21210], [test: 0.21233]\n",
      "********************************************************************************\n",
      "epoch: 159 / 500, time cost: 47.90 sec, \n",
      "          Loss: [Train: 2.25070], [Test: 2.27654],\n",
      "          Accuracy: [prop score:  0.94350], [q1: 0.92043], [q0: 0.99103],\n",
      "          Effect: [ate-q], [train: 0.21411], [test: 0.21431]\n",
      "********************************************************************************\n",
      "epoch: 160 / 500, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.25087], [Test: 2.27616],\n",
      "          Accuracy: [prop score:  0.94348], [q1: 0.91894], [q0: 0.99046],\n",
      "          Effect: [ate-q], [train: 0.21103], [test: 0.21124]\n",
      "********************************************************************************\n",
      "epoch: 161 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.25014], [Test: 2.27578],\n",
      "          Accuracy: [prop score:  0.94346], [q1: 0.91962], [q0: 0.99112],\n",
      "          Effect: [ate-q], [train: 0.21521], [test: 0.21543]\n",
      "********************************************************************************\n",
      "epoch: 162 / 500, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.24972], [Test: 2.27578],\n",
      "          Accuracy: [prop score:  0.94357], [q1: 0.92005], [q0: 0.99082],\n",
      "          Effect: [ate-q], [train: 0.21563], [test: 0.21583]\n",
      "********************************************************************************\n",
      "epoch: 163 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.24965], [Test: 2.27582],\n",
      "          Accuracy: [prop score:  0.94365], [q1: 0.91708], [q0: 0.99060],\n",
      "          Effect: [ate-q], [train: 0.21152], [test: 0.21175]\n",
      "********************************************************************************\n",
      "epoch: 164 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.24890], [Test: 2.27492],\n",
      "          Accuracy: [prop score:  0.94380], [q1: 0.91868], [q0: 0.99126],\n",
      "          Effect: [ate-q], [train: 0.21715], [test: 0.21736]\n",
      "********************************************************************************\n",
      "epoch: 165 / 500, time cost: 48.29 sec, \n",
      "          Loss: [Train: 2.24868], [Test: 2.27543],\n",
      "          Accuracy: [prop score:  0.94380], [q1: 0.91859], [q0: 0.99060],\n",
      "          Effect: [ate-q], [train: 0.21496], [test: 0.21517]\n",
      "********************************************************************************\n",
      "epoch: 166 / 500, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.24868], [Test: 2.27518],\n",
      "          Accuracy: [prop score:  0.94388], [q1: 0.91706], [q0: 0.99051],\n",
      "          Effect: [ate-q], [train: 0.21348], [test: 0.21371]\n",
      "********************************************************************************\n",
      "epoch: 167 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.24820], [Test: 2.27454],\n",
      "          Accuracy: [prop score:  0.94389], [q1: 0.91736], [q0: 0.99088],\n",
      "          Effect: [ate-q], [train: 0.21622], [test: 0.21646]\n",
      "********************************************************************************\n",
      "epoch: 168 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.24733], [Test: 2.27490],\n",
      "          Accuracy: [prop score:  0.94395], [q1: 0.91457], [q0: 0.99051],\n",
      "          Effect: [ate-q], [train: 0.21214], [test: 0.21237]\n",
      "********************************************************************************\n",
      "epoch: 169 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.24785], [Test: 2.27468],\n",
      "          Accuracy: [prop score:  0.94398], [q1: 0.91547], [q0: 0.99030],\n",
      "          Effect: [ate-q], [train: 0.21304], [test: 0.21326]\n",
      "********************************************************************************\n",
      "epoch: 170 / 500, time cost: 46.47 sec, \n",
      "          Loss: [Train: 2.24698], [Test: 2.27447],\n",
      "          Accuracy: [prop score:  0.94403], [q1: 0.91315], [q0: 0.99016],\n",
      "          Effect: [ate-q], [train: 0.21075], [test: 0.21101]\n",
      "********************************************************************************\n",
      "epoch: 171 / 500, time cost: 48.77 sec, \n",
      "          Loss: [Train: 2.24709], [Test: 2.27424],\n",
      "          Accuracy: [prop score:  0.94412], [q1: 0.91497], [q0: 0.99058],\n",
      "          Effect: [ate-q], [train: 0.21515], [test: 0.21538]\n",
      "********************************************************************************\n",
      "epoch: 172 / 500, time cost: 46.38 sec, \n",
      "          Loss: [Train: 2.24654], [Test: 2.27384],\n",
      "          Accuracy: [prop score:  0.94409], [q1: 0.91119], [q0: 0.98919],\n",
      "          Effect: [ate-q], [train: 0.20739], [test: 0.20764]\n",
      "********************************************************************************\n",
      "epoch: 173 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.24600], [Test: 2.27399],\n",
      "          Accuracy: [prop score:  0.94416], [q1: 0.91639], [q0: 0.99010],\n",
      "          Effect: [ate-q], [train: 0.21696], [test: 0.21718]\n",
      "********************************************************************************\n",
      "epoch: 174 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.24583], [Test: 2.27373],\n",
      "          Accuracy: [prop score:  0.94415], [q1: 0.91211], [q0: 0.98933],\n",
      "          Effect: [ate-q], [train: 0.20989], [test: 0.21015]\n",
      "********************************************************************************\n",
      "epoch: 175 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.24565], [Test: 2.27372],\n",
      "          Accuracy: [prop score:  0.94425], [q1: 0.91197], [q0: 0.98977],\n",
      "          Effect: [ate-q], [train: 0.21115], [test: 0.21141]\n",
      "********************************************************************************\n",
      "epoch: 176 / 500, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.24511], [Test: 2.27387],\n",
      "          Accuracy: [prop score:  0.94430], [q1: 0.91127], [q0: 0.98949],\n",
      "          Effect: [ate-q], [train: 0.21078], [test: 0.21104]\n",
      "********************************************************************************\n",
      "epoch: 177 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.24478], [Test: 2.27360],\n",
      "          Accuracy: [prop score:  0.94445], [q1: 0.91353], [q0: 0.98962],\n",
      "          Effect: [ate-q], [train: 0.21446], [test: 0.21472]\n",
      "********************************************************************************\n",
      "epoch: 178 / 500, time cost: 45.84 sec, \n",
      "          Loss: [Train: 2.24433], [Test: 2.27296],\n",
      "          Accuracy: [prop score:  0.94436], [q1: 0.90628], [q0: 0.98890],\n",
      "          Effect: [ate-q], [train: 0.20544], [test: 0.20572]\n",
      "********************************************************************************\n",
      "epoch: 179 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.24424], [Test: 2.27333],\n",
      "          Accuracy: [prop score:  0.94464], [q1: 0.91176], [q0: 0.98996],\n",
      "          Effect: [ate-q], [train: 0.21492], [test: 0.21517]\n",
      "********************************************************************************\n",
      "epoch: 180 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.24437], [Test: 2.27325],\n",
      "          Accuracy: [prop score:  0.94456], [q1: 0.90812], [q0: 0.98996],\n",
      "          Effect: [ate-q], [train: 0.21151], [test: 0.21177]\n",
      "********************************************************************************\n",
      "epoch: 181 / 500, time cost: 48.12 sec, \n",
      "          Loss: [Train: 2.24407], [Test: 2.27251],\n",
      "          Accuracy: [prop score:  0.94458], [q1: 0.90858], [q0: 0.98905],\n",
      "          Effect: [ate-q], [train: 0.20992], [test: 0.21019]\n",
      "********************************************************************************\n",
      "epoch: 182 / 500, time cost: 45.28 sec, \n",
      "          Loss: [Train: 2.24397], [Test: 2.27307],\n",
      "          Accuracy: [prop score:  0.94465], [q1: 0.90788], [q0: 0.98923],\n",
      "          Effect: [ate-q], [train: 0.21023], [test: 0.21052]\n",
      "********************************************************************************\n",
      "epoch: 183 / 500, time cost: 48.29 sec, \n",
      "          Loss: [Train: 2.24314], [Test: 2.27295],\n",
      "          Accuracy: [prop score:  0.94479], [q1: 0.90896], [q0: 0.98963],\n",
      "          Effect: [ate-q], [train: 0.21347], [test: 0.21376]\n",
      "********************************************************************************\n",
      "epoch: 184 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.24243], [Test: 2.27268],\n",
      "          Accuracy: [prop score:  0.94469], [q1: 0.90841], [q0: 0.98981],\n",
      "          Effect: [ate-q], [train: 0.21423], [test: 0.21451]\n",
      "********************************************************************************\n",
      "epoch: 185 / 500, time cost: 47.78 sec, \n",
      "          Loss: [Train: 2.24218], [Test: 2.27259],\n",
      "          Accuracy: [prop score:  0.94492], [q1: 0.90739], [q0: 0.98891],\n",
      "          Effect: [ate-q], [train: 0.21075], [test: 0.21103]\n",
      "********************************************************************************\n",
      "epoch: 186 / 500, time cost: 45.82 sec, \n",
      "          Loss: [Train: 2.24234], [Test: 2.27186],\n",
      "          Accuracy: [prop score:  0.94503], [q1: 0.90387], [q0: 0.98953],\n",
      "          Effect: [ate-q], [train: 0.20935], [test: 0.20965]\n",
      "********************************************************************************\n",
      "epoch: 187 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.24167], [Test: 2.27195],\n",
      "          Accuracy: [prop score:  0.94508], [q1: 0.90721], [q0: 0.98848],\n",
      "          Effect: [ate-q], [train: 0.21042], [test: 0.21071]\n",
      "********************************************************************************\n",
      "epoch: 188 / 500, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.24097], [Test: 2.27243],\n",
      "          Accuracy: [prop score:  0.94517], [q1: 0.90676], [q0: 0.98769],\n",
      "          Effect: [ate-q], [train: 0.20862], [test: 0.20891]\n",
      "********************************************************************************\n",
      "epoch: 189 / 500, time cost: 47.89 sec, \n",
      "          Loss: [Train: 2.24085], [Test: 2.27213],\n",
      "          Accuracy: [prop score:  0.94515], [q1: 0.90250], [q0: 0.98861],\n",
      "          Effect: [ate-q], [train: 0.20749], [test: 0.20779]\n",
      "********************************************************************************\n",
      "epoch: 190 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.24049], [Test: 2.27183],\n",
      "          Accuracy: [prop score:  0.94532], [q1: 0.90071], [q0: 0.98894],\n",
      "          Effect: [ate-q], [train: 0.20712], [test: 0.20743]\n",
      "********************************************************************************\n",
      "epoch: 191 / 500, time cost: 47.74 sec, \n",
      "          Loss: [Train: 2.24077], [Test: 2.27199],\n",
      "          Accuracy: [prop score:  0.94524], [q1: 0.90936], [q0: 0.98834],\n",
      "          Effect: [ate-q], [train: 0.21469], [test: 0.21499]\n",
      "********************************************************************************\n",
      "epoch: 192 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.23997], [Test: 2.27135],\n",
      "          Accuracy: [prop score:  0.94537], [q1: 0.90501], [q0: 0.98961],\n",
      "          Effect: [ate-q], [train: 0.21453], [test: 0.21485]\n",
      "********************************************************************************\n",
      "epoch: 193 / 500, time cost: 47.71 sec, \n",
      "          Loss: [Train: 2.23953], [Test: 2.27139],\n",
      "          Accuracy: [prop score:  0.94542], [q1: 0.90504], [q0: 0.98821],\n",
      "          Effect: [ate-q], [train: 0.21083], [test: 0.21114]\n",
      "********************************************************************************\n",
      "epoch: 194 / 500, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.23965], [Test: 2.27139],\n",
      "          Accuracy: [prop score:  0.94554], [q1: 0.90762], [q0: 0.98810],\n",
      "          Effect: [ate-q], [train: 0.21376], [test: 0.21408]\n",
      "********************************************************************************\n",
      "epoch: 195 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.23855], [Test: 2.27146],\n",
      "          Accuracy: [prop score:  0.94557], [q1: 0.90144], [q0: 0.98720],\n",
      "          Effect: [ate-q], [train: 0.20602], [test: 0.20634]\n",
      "********************************************************************************\n",
      "epoch: 196 / 500, time cost: 45.66 sec, \n",
      "          Loss: [Train: 2.23890], [Test: 2.27141],\n",
      "          Accuracy: [prop score:  0.94562], [q1: 0.90460], [q0: 0.98847],\n",
      "          Effect: [ate-q], [train: 0.21288], [test: 0.21319]\n",
      "********************************************************************************\n",
      "epoch: 197 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.23821], [Test: 2.27157],\n",
      "          Accuracy: [prop score:  0.94581], [q1: 0.90245], [q0: 0.98800],\n",
      "          Effect: [ate-q], [train: 0.20993], [test: 0.21024]\n",
      "********************************************************************************\n",
      "epoch: 198 / 500, time cost: 45.70 sec, \n",
      "          Loss: [Train: 2.23851], [Test: 2.27135],\n",
      "          Accuracy: [prop score:  0.94580], [q1: 0.90201], [q0: 0.98770],\n",
      "          Effect: [ate-q], [train: 0.20947], [test: 0.20979]\n",
      "********************************************************************************\n",
      "epoch: 199 / 500, time cost: 47.90 sec, \n",
      "          Loss: [Train: 2.23756], [Test: 2.27076],\n",
      "          Accuracy: [prop score:  0.94583], [q1: 0.89747], [q0: 0.98690],\n",
      "          Effect: [ate-q], [train: 0.20428], [test: 0.20463]\n",
      "********************************************************************************\n",
      "epoch: 200 / 500, time cost: 45.84 sec, \n",
      "          Loss: [Train: 2.23770], [Test: 2.27044],\n",
      "          Accuracy: [prop score:  0.94583], [q1: 0.89657], [q0: 0.98786],\n",
      "          Effect: [ate-q], [train: 0.20632], [test: 0.20667]\n",
      "********************************************************************************\n",
      "epoch: 201 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.23779], [Test: 2.27106],\n",
      "          Accuracy: [prop score:  0.94583], [q1: 0.89917], [q0: 0.98704],\n",
      "          Effect: [ate-q], [train: 0.20761], [test: 0.20795]\n",
      "********************************************************************************\n",
      "epoch: 202 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.23690], [Test: 2.27087],\n",
      "          Accuracy: [prop score:  0.94594], [q1: 0.89856], [q0: 0.98697],\n",
      "          Effect: [ate-q], [train: 0.20694], [test: 0.20727]\n",
      "********************************************************************************\n",
      "epoch: 203 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.23668], [Test: 2.27063],\n",
      "          Accuracy: [prop score:  0.94609], [q1: 0.90116], [q0: 0.98786],\n",
      "          Effect: [ate-q], [train: 0.21192], [test: 0.21228]\n",
      "********************************************************************************\n",
      "epoch: 204 / 500, time cost: 45.24 sec, \n",
      "          Loss: [Train: 2.23619], [Test: 2.27026],\n",
      "          Accuracy: [prop score:  0.94613], [q1: 0.89904], [q0: 0.98701],\n",
      "          Effect: [ate-q], [train: 0.20820], [test: 0.20856]\n",
      "********************************************************************************\n",
      "epoch: 205 / 500, time cost: 47.34 sec, \n",
      "          Loss: [Train: 2.23616], [Test: 2.27030],\n",
      "          Accuracy: [prop score:  0.94616], [q1: 0.89828], [q0: 0.98646],\n",
      "          Effect: [ate-q], [train: 0.20700], [test: 0.20734]\n",
      "********************************************************************************\n",
      "epoch: 206 / 500, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.23562], [Test: 2.27017],\n",
      "          Accuracy: [prop score:  0.94623], [q1: 0.89477], [q0: 0.98635],\n",
      "          Effect: [ate-q], [train: 0.20366], [test: 0.20400]\n",
      "********************************************************************************\n",
      "epoch: 207 / 500, time cost: 47.75 sec, \n",
      "          Loss: [Train: 2.23537], [Test: 2.26988],\n",
      "          Accuracy: [prop score:  0.94629], [q1: 0.89476], [q0: 0.98627],\n",
      "          Effect: [ate-q], [train: 0.20403], [test: 0.20439]\n",
      "********************************************************************************\n",
      "epoch: 208 / 500, time cost: 45.56 sec, \n",
      "          Loss: [Train: 2.23500], [Test: 2.27044],\n",
      "          Accuracy: [prop score:  0.94637], [q1: 0.89078], [q0: 0.98657],\n",
      "          Effect: [ate-q], [train: 0.20166], [test: 0.20204]\n",
      "********************************************************************************\n",
      "epoch: 209 / 500, time cost: 47.88 sec, \n",
      "          Loss: [Train: 2.23455], [Test: 2.27014],\n",
      "          Accuracy: [prop score:  0.94627], [q1: 0.90077], [q0: 0.98719],\n",
      "          Effect: [ate-q], [train: 0.21346], [test: 0.21382]\n",
      "********************************************************************************\n",
      "epoch: 210 / 500, time cost: 45.41 sec, \n",
      "          Loss: [Train: 2.23467], [Test: 2.26982],\n",
      "          Accuracy: [prop score:  0.94642], [q1: 0.89486], [q0: 0.98599],\n",
      "          Effect: [ate-q], [train: 0.20529], [test: 0.20567]\n",
      "********************************************************************************\n",
      "epoch: 211 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.23417], [Test: 2.27019],\n",
      "          Accuracy: [prop score:  0.94647], [q1: 0.89569], [q0: 0.98704],\n",
      "          Effect: [ate-q], [train: 0.20909], [test: 0.20946]\n",
      "********************************************************************************\n",
      "epoch: 212 / 500, time cost: 45.82 sec, \n",
      "          Loss: [Train: 2.23357], [Test: 2.26987],\n",
      "          Accuracy: [prop score:  0.94653], [q1: 0.89183], [q0: 0.98734],\n",
      "          Effect: [ate-q], [train: 0.20725], [test: 0.20764]\n",
      "********************************************************************************\n",
      "epoch: 213 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.23377], [Test: 2.27001],\n",
      "          Accuracy: [prop score:  0.94662], [q1: 0.89192], [q0: 0.98670],\n",
      "          Effect: [ate-q], [train: 0.20569], [test: 0.20609]\n",
      "********************************************************************************\n",
      "epoch: 214 / 500, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.23307], [Test: 2.26956],\n",
      "          Accuracy: [prop score:  0.94673], [q1: 0.89895], [q0: 0.98699],\n",
      "          Effect: [ate-q], [train: 0.21363], [test: 0.21401]\n",
      "********************************************************************************\n",
      "epoch: 215 / 500, time cost: 47.56 sec, \n",
      "          Loss: [Train: 2.23270], [Test: 2.26944],\n",
      "          Accuracy: [prop score:  0.94671], [q1: 0.89223], [q0: 0.98698],\n",
      "          Effect: [ate-q], [train: 0.20817], [test: 0.20855]\n",
      "********************************************************************************\n",
      "epoch: 216 / 500, time cost: 46.04 sec, \n",
      "          Loss: [Train: 2.23265], [Test: 2.26966],\n",
      "          Accuracy: [prop score:  0.94679], [q1: 0.89490], [q0: 0.98583],\n",
      "          Effect: [ate-q], [train: 0.20822], [test: 0.20862]\n",
      "********************************************************************************\n",
      "epoch: 217 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.23244], [Test: 2.26973],\n",
      "          Accuracy: [prop score:  0.94677], [q1: 0.89147], [q0: 0.98611],\n",
      "          Effect: [ate-q], [train: 0.20624], [test: 0.20664]\n",
      "********************************************************************************\n",
      "epoch: 218 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.23176], [Test: 2.26957],\n",
      "          Accuracy: [prop score:  0.94699], [q1: 0.89501], [q0: 0.98658],\n",
      "          Effect: [ate-q], [train: 0.21104], [test: 0.21144]\n",
      "********************************************************************************\n",
      "epoch: 219 / 500, time cost: 47.61 sec, \n",
      "          Loss: [Train: 2.23135], [Test: 2.26908],\n",
      "          Accuracy: [prop score:  0.94692], [q1: 0.89068], [q0: 0.98588],\n",
      "          Effect: [ate-q], [train: 0.20599], [test: 0.20638]\n",
      "********************************************************************************\n",
      "epoch: 220 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.23085], [Test: 2.26934],\n",
      "          Accuracy: [prop score:  0.94700], [q1: 0.89297], [q0: 0.98543],\n",
      "          Effect: [ate-q], [train: 0.20703], [test: 0.20744]\n",
      "********************************************************************************\n",
      "epoch: 221 / 500, time cost: 47.62 sec, \n",
      "          Loss: [Train: 2.23051], [Test: 2.26917],\n",
      "          Accuracy: [prop score:  0.94709], [q1: 0.89212], [q0: 0.98588],\n",
      "          Effect: [ate-q], [train: 0.20797], [test: 0.20838]\n",
      "********************************************************************************\n",
      "epoch: 222 / 500, time cost: 45.87 sec, \n",
      "          Loss: [Train: 2.23059], [Test: 2.26936],\n",
      "          Accuracy: [prop score:  0.94723], [q1: 0.89153], [q0: 0.98711],\n",
      "          Effect: [ate-q], [train: 0.21098], [test: 0.21137]\n",
      "********************************************************************************\n",
      "epoch: 223 / 500, time cost: 48.13 sec, \n",
      "          Loss: [Train: 2.23011], [Test: 2.26814],\n",
      "          Accuracy: [prop score:  0.94729], [q1: 0.88480], [q0: 0.98692],\n",
      "          Effect: [ate-q], [train: 0.20525], [test: 0.20568]\n",
      "********************************************************************************\n",
      "epoch: 224 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.22963], [Test: 2.26860],\n",
      "          Accuracy: [prop score:  0.94725], [q1: 0.88615], [q0: 0.98458],\n",
      "          Effect: [ate-q], [train: 0.20123], [test: 0.20166]\n",
      "********************************************************************************\n",
      "epoch: 225 / 500, time cost: 47.79 sec, \n",
      "          Loss: [Train: 2.22956], [Test: 2.26923],\n",
      "          Accuracy: [prop score:  0.94722], [q1: 0.88388], [q0: 0.98717],\n",
      "          Effect: [ate-q], [train: 0.20668], [test: 0.20711]\n",
      "********************************************************************************\n",
      "epoch: 226 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.22905], [Test: 2.26847],\n",
      "          Accuracy: [prop score:  0.94733], [q1: 0.88724], [q0: 0.98562],\n",
      "          Effect: [ate-q], [train: 0.20577], [test: 0.20621]\n",
      "********************************************************************************\n",
      "epoch: 227 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.22869], [Test: 2.26893],\n",
      "          Accuracy: [prop score:  0.94747], [q1: 0.88676], [q0: 0.98577],\n",
      "          Effect: [ate-q], [train: 0.20625], [test: 0.20669]\n",
      "********************************************************************************\n",
      "epoch: 228 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 2.22835], [Test: 2.26863],\n",
      "          Accuracy: [prop score:  0.94746], [q1: 0.87942], [q0: 0.98498],\n",
      "          Effect: [ate-q], [train: 0.19924], [test: 0.19971]\n",
      "********************************************************************************\n",
      "epoch: 229 / 500, time cost: 48.67 sec, \n",
      "          Loss: [Train: 2.22748], [Test: 2.26890],\n",
      "          Accuracy: [prop score:  0.94751], [q1: 0.89324], [q0: 0.98678],\n",
      "          Effect: [ate-q], [train: 0.21642], [test: 0.21690]\n",
      "********************************************************************************\n",
      "epoch: 230 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.22722], [Test: 2.26817],\n",
      "          Accuracy: [prop score:  0.94760], [q1: 0.88253], [q0: 0.98693],\n",
      "          Effect: [ate-q], [train: 0.20781], [test: 0.20827]\n",
      "********************************************************************************\n",
      "epoch: 231 / 500, time cost: 48.40 sec, \n",
      "          Loss: [Train: 2.22742], [Test: 2.26846],\n",
      "          Accuracy: [prop score:  0.94765], [q1: 0.88902], [q0: 0.98580],\n",
      "          Effect: [ate-q], [train: 0.21037], [test: 0.21082]\n",
      "********************************************************************************\n",
      "epoch: 232 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.22715], [Test: 2.26834],\n",
      "          Accuracy: [prop score:  0.94772], [q1: 0.87904], [q0: 0.98541],\n",
      "          Effect: [ate-q], [train: 0.20174], [test: 0.20221]\n",
      "********************************************************************************\n",
      "epoch: 233 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.22708], [Test: 2.26829],\n",
      "          Accuracy: [prop score:  0.94774], [q1: 0.88033], [q0: 0.98681],\n",
      "          Effect: [ate-q], [train: 0.20716], [test: 0.20761]\n",
      "********************************************************************************\n",
      "epoch: 234 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.22650], [Test: 2.26791],\n",
      "          Accuracy: [prop score:  0.94780], [q1: 0.87997], [q0: 0.98558],\n",
      "          Effect: [ate-q], [train: 0.20368], [test: 0.20415]\n",
      "********************************************************************************\n",
      "epoch: 235 / 500, time cost: 48.83 sec, \n",
      "          Loss: [Train: 2.22578], [Test: 2.26825],\n",
      "          Accuracy: [prop score:  0.94793], [q1: 0.88861], [q0: 0.98607],\n",
      "          Effect: [ate-q], [train: 0.21292], [test: 0.21340]\n",
      "********************************************************************************\n",
      "epoch: 236 / 500, time cost: 47.01 sec, \n",
      "          Loss: [Train: 2.22545], [Test: 2.26828],\n",
      "          Accuracy: [prop score:  0.94796], [q1: 0.87875], [q0: 0.98500],\n",
      "          Effect: [ate-q], [train: 0.20246], [test: 0.20295]\n",
      "********************************************************************************\n",
      "epoch: 237 / 500, time cost: 49.55 sec, \n",
      "          Loss: [Train: 2.22532], [Test: 2.26812],\n",
      "          Accuracy: [prop score:  0.94802], [q1: 0.87712], [q0: 0.98571],\n",
      "          Effect: [ate-q], [train: 0.20372], [test: 0.20422]\n",
      "********************************************************************************\n",
      "epoch: 238 / 500, time cost: 45.38 sec, \n",
      "          Loss: [Train: 2.22506], [Test: 2.26800],\n",
      "          Accuracy: [prop score:  0.94810], [q1: 0.86862], [q0: 0.98568],\n",
      "          Effect: [ate-q], [train: 0.19748], [test: 0.19798]\n",
      "********************************************************************************\n",
      "epoch: 239 / 500, time cost: 48.33 sec, \n",
      "          Loss: [Train: 2.22437], [Test: 2.26754],\n",
      "          Accuracy: [prop score:  0.94812], [q1: 0.88397], [q0: 0.98600],\n",
      "          Effect: [ate-q], [train: 0.21044], [test: 0.21093]\n",
      "********************************************************************************\n",
      "epoch: 240 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.22371], [Test: 2.26751],\n",
      "          Accuracy: [prop score:  0.94822], [q1: 0.88097], [q0: 0.98550],\n",
      "          Effect: [ate-q], [train: 0.20732], [test: 0.20783]\n",
      "********************************************************************************\n",
      "epoch: 241 / 500, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.22386], [Test: 2.26792],\n",
      "          Accuracy: [prop score:  0.94822], [q1: 0.87302], [q0: 0.98342],\n",
      "          Effect: [ate-q], [train: 0.19637], [test: 0.19689]\n",
      "********************************************************************************\n",
      "epoch: 242 / 500, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.22352], [Test: 2.26814],\n",
      "          Accuracy: [prop score:  0.94834], [q1: 0.88120], [q0: 0.98521],\n",
      "          Effect: [ate-q], [train: 0.20772], [test: 0.20822]\n",
      "********************************************************************************\n",
      "epoch: 243 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.22296], [Test: 2.26759],\n",
      "          Accuracy: [prop score:  0.94840], [q1: 0.87551], [q0: 0.98462],\n",
      "          Effect: [ate-q], [train: 0.20265], [test: 0.20318]\n",
      "********************************************************************************\n",
      "epoch: 244 / 500, time cost: 45.60 sec, \n",
      "          Loss: [Train: 2.22313], [Test: 2.26770],\n",
      "          Accuracy: [prop score:  0.94842], [q1: 0.87531], [q0: 0.98585],\n",
      "          Effect: [ate-q], [train: 0.20591], [test: 0.20644]\n",
      "********************************************************************************\n",
      "epoch: 245 / 500, time cost: 47.40 sec, \n",
      "          Loss: [Train: 2.22280], [Test: 2.26810],\n",
      "          Accuracy: [prop score:  0.94843], [q1: 0.87872], [q0: 0.98567],\n",
      "          Effect: [ate-q], [train: 0.20908], [test: 0.20961]\n",
      "********************************************************************************\n",
      "epoch: 246 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.22259], [Test: 2.26784],\n",
      "          Accuracy: [prop score:  0.94855], [q1: 0.86478], [q0: 0.98417],\n",
      "          Effect: [ate-q], [train: 0.19411], [test: 0.19465]\n",
      "********************************************************************************\n",
      "epoch: 247 / 500, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.22202], [Test: 2.26753],\n",
      "          Accuracy: [prop score:  0.94864], [q1: 0.87701], [q0: 0.98481],\n",
      "          Effect: [ate-q], [train: 0.20575], [test: 0.20629]\n",
      "********************************************************************************\n",
      "epoch: 248 / 500, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.22185], [Test: 2.26728],\n",
      "          Accuracy: [prop score:  0.94864], [q1: 0.87376], [q0: 0.98546],\n",
      "          Effect: [ate-q], [train: 0.20540], [test: 0.20596]\n",
      "********************************************************************************\n",
      "epoch: 249 / 500, time cost: 47.89 sec, \n",
      "          Loss: [Train: 2.22046], [Test: 2.26730],\n",
      "          Accuracy: [prop score:  0.94872], [q1: 0.87705], [q0: 0.98589],\n",
      "          Effect: [ate-q], [train: 0.21012], [test: 0.21066]\n",
      "********************************************************************************\n",
      "epoch: 250 / 500, time cost: 45.62 sec, \n",
      "          Loss: [Train: 2.22077], [Test: 2.26722],\n",
      "          Accuracy: [prop score:  0.94880], [q1: 0.86954], [q0: 0.98414],\n",
      "          Effect: [ate-q], [train: 0.19960], [test: 0.20017]\n",
      "********************************************************************************\n",
      "epoch: 251 / 500, time cost: 47.60 sec, \n",
      "          Loss: [Train: 2.22081], [Test: 2.26666],\n",
      "          Accuracy: [prop score:  0.94893], [q1: 0.87288], [q0: 0.98534],\n",
      "          Effect: [ate-q], [train: 0.20565], [test: 0.20622]\n",
      "********************************************************************************\n",
      "epoch: 252 / 500, time cost: 45.74 sec, \n",
      "          Loss: [Train: 2.22025], [Test: 2.26736],\n",
      "          Accuracy: [prop score:  0.94892], [q1: 0.87264], [q0: 0.98506],\n",
      "          Effect: [ate-q], [train: 0.20543], [test: 0.20599]\n",
      "********************************************************************************\n",
      "epoch: 253 / 500, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.21976], [Test: 2.26720],\n",
      "          Accuracy: [prop score:  0.94893], [q1: 0.86343], [q0: 0.98646],\n",
      "          Effect: [ate-q], [train: 0.20235], [test: 0.20290]\n",
      "********************************************************************************\n",
      "epoch: 254 / 500, time cost: 46.19 sec, \n",
      "          Loss: [Train: 2.21937], [Test: 2.26727],\n",
      "          Accuracy: [prop score:  0.94906], [q1: 0.88058], [q0: 0.98452],\n",
      "          Effect: [ate-q], [train: 0.21178], [test: 0.21235]\n",
      "********************************************************************************\n",
      "epoch: 255 / 500, time cost: 48.40 sec, \n",
      "          Loss: [Train: 2.21926], [Test: 2.26712],\n",
      "          Accuracy: [prop score:  0.94914], [q1: 0.86656], [q0: 0.98391],\n",
      "          Effect: [ate-q], [train: 0.19849], [test: 0.19908]\n",
      "********************************************************************************\n",
      "epoch: 256 / 500, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.21832], [Test: 2.26725],\n",
      "          Accuracy: [prop score:  0.94916], [q1: 0.86233], [q0: 0.98407],\n",
      "          Effect: [ate-q], [train: 0.19635], [test: 0.19694]\n",
      "********************************************************************************\n",
      "epoch: 257 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.21869], [Test: 2.26742],\n",
      "          Accuracy: [prop score:  0.94913], [q1: 0.87598], [q0: 0.98481],\n",
      "          Effect: [ate-q], [train: 0.20925], [test: 0.20984]\n",
      "********************************************************************************\n",
      "epoch: 258 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.21800], [Test: 2.26734],\n",
      "          Accuracy: [prop score:  0.94927], [q1: 0.85998], [q0: 0.98547],\n",
      "          Effect: [ate-q], [train: 0.19950], [test: 0.20010]\n",
      "********************************************************************************\n",
      "epoch: 259 / 500, time cost: 47.71 sec, \n",
      "          Loss: [Train: 2.21739], [Test: 2.26823],\n",
      "          Accuracy: [prop score:  0.94925], [q1: 0.87066], [q0: 0.98463],\n",
      "          Effect: [ate-q], [train: 0.20547], [test: 0.20605]\n",
      "********************************************************************************\n",
      "epoch: 260 / 500, time cost: 45.30 sec, \n",
      "          Loss: [Train: 2.21775], [Test: 2.26765],\n",
      "          Accuracy: [prop score:  0.94936], [q1: 0.86820], [q0: 0.98291],\n",
      "          Effect: [ate-q], [train: 0.19961], [test: 0.20022]\n",
      "********************************************************************************\n",
      "epoch: 261 / 500, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.21734], [Test: 2.26735],\n",
      "          Accuracy: [prop score:  0.94946], [q1: 0.86878], [q0: 0.98534],\n",
      "          Effect: [ate-q], [train: 0.20684], [test: 0.20745]\n",
      "********************************************************************************\n",
      "epoch: 262 / 500, time cost: 44.75 sec, \n",
      "          Loss: [Train: 2.21656], [Test: 2.26719],\n",
      "          Accuracy: [prop score:  0.94939], [q1: 0.86805], [q0: 0.98393],\n",
      "          Effect: [ate-q], [train: 0.20310], [test: 0.20372]\n",
      "********************************************************************************\n",
      "epoch: 263 / 500, time cost: 47.30 sec, \n",
      "          Loss: [Train: 2.21619], [Test: 2.26733],\n",
      "          Accuracy: [prop score:  0.94953], [q1: 0.85848], [q0: 0.98363],\n",
      "          Effect: [ate-q], [train: 0.19567], [test: 0.19631]\n",
      "********************************************************************************\n",
      "epoch: 264 / 500, time cost: 44.85 sec, \n",
      "          Loss: [Train: 2.21545], [Test: 2.26686],\n",
      "          Accuracy: [prop score:  0.94958], [q1: 0.85689], [q0: 0.98447],\n",
      "          Effect: [ate-q], [train: 0.19720], [test: 0.19782]\n",
      "********************************************************************************\n",
      "epoch: 265 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.21597], [Test: 2.26673],\n",
      "          Accuracy: [prop score:  0.94957], [q1: 0.86377], [q0: 0.98388],\n",
      "          Effect: [ate-q], [train: 0.20085], [test: 0.20148]\n",
      "********************************************************************************\n",
      "epoch: 266 / 500, time cost: 44.80 sec, \n",
      "          Loss: [Train: 2.21519], [Test: 2.26714],\n",
      "          Accuracy: [prop score:  0.94957], [q1: 0.85603], [q0: 0.98415],\n",
      "          Effect: [ate-q], [train: 0.19668], [test: 0.19731]\n",
      "********************************************************************************\n",
      "epoch: 267 / 500, time cost: 47.62 sec, \n",
      "          Loss: [Train: 2.21487], [Test: 2.26672],\n",
      "          Accuracy: [prop score:  0.94975], [q1: 0.85012], [q0: 0.98426],\n",
      "          Effect: [ate-q], [train: 0.19304], [test: 0.19369]\n",
      "********************************************************************************\n",
      "epoch: 268 / 500, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.21465], [Test: 2.26702],\n",
      "          Accuracy: [prop score:  0.94970], [q1: 0.87047], [q0: 0.98397],\n",
      "          Effect: [ate-q], [train: 0.20812], [test: 0.20878]\n",
      "********************************************************************************\n",
      "epoch: 269 / 500, time cost: 47.63 sec, \n",
      "          Loss: [Train: 2.21400], [Test: 2.26707],\n",
      "          Accuracy: [prop score:  0.94974], [q1: 0.85320], [q0: 0.98302],\n",
      "          Effect: [ate-q], [train: 0.19304], [test: 0.19373]\n",
      "********************************************************************************\n",
      "epoch: 270 / 500, time cost: 45.12 sec, \n",
      "          Loss: [Train: 2.21341], [Test: 2.26692],\n",
      "          Accuracy: [prop score:  0.94987], [q1: 0.86312], [q0: 0.98409],\n",
      "          Effect: [ate-q], [train: 0.20329], [test: 0.20394]\n",
      "********************************************************************************\n",
      "epoch: 271 / 500, time cost: 47.27 sec, \n",
      "          Loss: [Train: 2.21365], [Test: 2.26749],\n",
      "          Accuracy: [prop score:  0.94993], [q1: 0.86274], [q0: 0.98160],\n",
      "          Effect: [ate-q], [train: 0.19775], [test: 0.19841]\n",
      "********************************************************************************\n",
      "epoch: 272 / 500, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.21322], [Test: 2.26684],\n",
      "          Accuracy: [prop score:  0.95003], [q1: 0.85986], [q0: 0.98517],\n",
      "          Effect: [ate-q], [train: 0.20496], [test: 0.20559]\n",
      "********************************************************************************\n",
      "epoch: 273 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.21241], [Test: 2.26692],\n",
      "          Accuracy: [prop score:  0.95010], [q1: 0.86036], [q0: 0.98437],\n",
      "          Effect: [ate-q], [train: 0.20351], [test: 0.20416]\n",
      "********************************************************************************\n",
      "epoch: 274 / 500, time cost: 46.09 sec, \n",
      "          Loss: [Train: 2.21221], [Test: 2.26729],\n",
      "          Accuracy: [prop score:  0.95011], [q1: 0.87176], [q0: 0.98368],\n",
      "          Effect: [ate-q], [train: 0.21117], [test: 0.21185]\n",
      "********************************************************************************\n",
      "epoch: 275 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.21166], [Test: 2.26693],\n",
      "          Accuracy: [prop score:  0.95020], [q1: 0.86134], [q0: 0.98341],\n",
      "          Effect: [ate-q], [train: 0.20227], [test: 0.20295]\n",
      "********************************************************************************\n",
      "epoch: 276 / 500, time cost: 45.50 sec, \n",
      "          Loss: [Train: 2.21133], [Test: 2.26699],\n",
      "          Accuracy: [prop score:  0.95022], [q1: 0.85704], [q0: 0.98401],\n",
      "          Effect: [ate-q], [train: 0.20118], [test: 0.20187]\n",
      "********************************************************************************\n",
      "epoch: 277 / 500, time cost: 47.20 sec, \n",
      "          Loss: [Train: 2.21086], [Test: 2.26684],\n",
      "          Accuracy: [prop score:  0.95027], [q1: 0.85720], [q0: 0.98247],\n",
      "          Effect: [ate-q], [train: 0.19802], [test: 0.19872]\n",
      "********************************************************************************\n",
      "epoch: 278 / 500, time cost: 45.14 sec, \n",
      "          Loss: [Train: 2.21116], [Test: 2.26696],\n",
      "          Accuracy: [prop score:  0.95039], [q1: 0.86213], [q0: 0.98271],\n",
      "          Effect: [ate-q], [train: 0.20285], [test: 0.20356]\n",
      "********************************************************************************\n",
      "epoch: 279 / 500, time cost: 48.51 sec, \n",
      "          Loss: [Train: 2.21047], [Test: 2.26706],\n",
      "          Accuracy: [prop score:  0.95049], [q1: 0.84639], [q0: 0.98176],\n",
      "          Effect: [ate-q], [train: 0.18987], [test: 0.19055]\n",
      "********************************************************************************\n",
      "epoch: 280 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.20991], [Test: 2.26668],\n",
      "          Accuracy: [prop score:  0.95052], [q1: 0.85585], [q0: 0.98283],\n",
      "          Effect: [ate-q], [train: 0.19945], [test: 0.20015]\n",
      "********************************************************************************\n",
      "epoch: 281 / 500, time cost: 48.58 sec, \n",
      "          Loss: [Train: 2.20938], [Test: 2.26702],\n",
      "          Accuracy: [prop score:  0.95063], [q1: 0.84928], [q0: 0.98372],\n",
      "          Effect: [ate-q], [train: 0.19756], [test: 0.19826]\n",
      "********************************************************************************\n",
      "epoch: 282 / 500, time cost: 46.50 sec, \n",
      "          Loss: [Train: 2.20903], [Test: 2.26690],\n",
      "          Accuracy: [prop score:  0.95070], [q1: 0.86095], [q0: 0.98321],\n",
      "          Effect: [ate-q], [train: 0.20503], [test: 0.20574]\n",
      "********************************************************************************\n",
      "epoch: 283 / 500, time cost: 48.62 sec, \n",
      "          Loss: [Train: 2.20831], [Test: 2.26654],\n",
      "          Accuracy: [prop score:  0.95077], [q1: 0.85291], [q0: 0.98349],\n",
      "          Effect: [ate-q], [train: 0.20030], [test: 0.20104]\n",
      "********************************************************************************\n",
      "epoch: 284 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.20822], [Test: 2.26717],\n",
      "          Accuracy: [prop score:  0.95083], [q1: 0.84882], [q0: 0.98307],\n",
      "          Effect: [ate-q], [train: 0.19673], [test: 0.19743]\n",
      "********************************************************************************\n",
      "epoch: 285 / 500, time cost: 48.00 sec, \n",
      "          Loss: [Train: 2.20819], [Test: 2.26725],\n",
      "          Accuracy: [prop score:  0.95080], [q1: 0.85149], [q0: 0.98262],\n",
      "          Effect: [ate-q], [train: 0.19806], [test: 0.19880]\n",
      "********************************************************************************\n",
      "epoch: 286 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.20780], [Test: 2.26711],\n",
      "          Accuracy: [prop score:  0.95091], [q1: 0.84878], [q0: 0.98226],\n",
      "          Effect: [ate-q], [train: 0.19609], [test: 0.19684]\n",
      "********************************************************************************\n",
      "epoch: 287 / 500, time cost: 47.81 sec, \n",
      "          Loss: [Train: 2.20679], [Test: 2.26697],\n",
      "          Accuracy: [prop score:  0.95090], [q1: 0.85229], [q0: 0.98387],\n",
      "          Effect: [ate-q], [train: 0.20311], [test: 0.20388]\n",
      "********************************************************************************\n",
      "epoch: 288 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.20707], [Test: 2.26681],\n",
      "          Accuracy: [prop score:  0.95098], [q1: 0.84767], [q0: 0.98389],\n",
      "          Effect: [ate-q], [train: 0.20011], [test: 0.20085]\n",
      "********************************************************************************\n",
      "epoch: 289 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.20643], [Test: 2.26673],\n",
      "          Accuracy: [prop score:  0.95106], [q1: 0.84530], [q0: 0.98314],\n",
      "          Effect: [ate-q], [train: 0.19693], [test: 0.19773]\n",
      "********************************************************************************\n",
      "epoch: 290 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.20632], [Test: 2.26700],\n",
      "          Accuracy: [prop score:  0.95108], [q1: 0.84757], [q0: 0.98079],\n",
      "          Effect: [ate-q], [train: 0.19359], [test: 0.19436]\n",
      "********************************************************************************\n",
      "epoch: 291 / 500, time cost: 47.88 sec, \n",
      "          Loss: [Train: 2.20547], [Test: 2.26648],\n",
      "          Accuracy: [prop score:  0.95107], [q1: 0.84419], [q0: 0.98494],\n",
      "          Effect: [ate-q], [train: 0.20199], [test: 0.20278]\n",
      "********************************************************************************\n",
      "epoch: 292 / 500, time cost: 45.44 sec, \n",
      "          Loss: [Train: 2.20477], [Test: 2.26719],\n",
      "          Accuracy: [prop score:  0.95118], [q1: 0.84274], [q0: 0.98286],\n",
      "          Effect: [ate-q], [train: 0.19599], [test: 0.19675]\n",
      "********************************************************************************\n",
      "epoch: 293 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.20503], [Test: 2.26708],\n",
      "          Accuracy: [prop score:  0.95114], [q1: 0.84319], [q0: 0.98299],\n",
      "          Effect: [ate-q], [train: 0.19743], [test: 0.19821]\n",
      "********************************************************************************\n",
      "epoch: 294 / 500, time cost: 45.76 sec, \n",
      "          Loss: [Train: 2.20426], [Test: 2.26719],\n",
      "          Accuracy: [prop score:  0.95117], [q1: 0.83892], [q0: 0.98111],\n",
      "          Effect: [ate-q], [train: 0.19092], [test: 0.19168]\n",
      "********************************************************************************\n",
      "epoch: 295 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.20414], [Test: 2.26736],\n",
      "          Accuracy: [prop score:  0.95128], [q1: 0.84832], [q0: 0.98256],\n",
      "          Effect: [ate-q], [train: 0.20121], [test: 0.20198]\n",
      "********************************************************************************\n",
      "epoch: 296 / 500, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.20355], [Test: 2.26704],\n",
      "          Accuracy: [prop score:  0.95137], [q1: 0.84793], [q0: 0.98373],\n",
      "          Effect: [ate-q], [train: 0.20398], [test: 0.20476]\n",
      "********************************************************************************\n",
      "epoch: 297 / 500, time cost: 48.34 sec, \n",
      "          Loss: [Train: 2.20307], [Test: 2.26739],\n",
      "          Accuracy: [prop score:  0.95141], [q1: 0.83062], [q0: 0.98290],\n",
      "          Effect: [ate-q], [train: 0.19048], [test: 0.19128]\n",
      "********************************************************************************\n",
      "epoch: 298 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.20311], [Test: 2.26725],\n",
      "          Accuracy: [prop score:  0.95147], [q1: 0.83206], [q0: 0.98017],\n",
      "          Effect: [ate-q], [train: 0.18550], [test: 0.18632]\n",
      "********************************************************************************\n",
      "epoch: 299 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.20254], [Test: 2.26734],\n",
      "          Accuracy: [prop score:  0.95146], [q1: 0.83502], [q0: 0.98131],\n",
      "          Effect: [ate-q], [train: 0.19057], [test: 0.19135]\n",
      "********************************************************************************\n",
      "epoch: 300 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.20266], [Test: 2.26696],\n",
      "          Accuracy: [prop score:  0.95152], [q1: 0.83534], [q0: 0.98179],\n",
      "          Effect: [ate-q], [train: 0.19183], [test: 0.19264]\n",
      "********************************************************************************\n",
      "epoch: 301 / 500, time cost: 47.73 sec, \n",
      "          Loss: [Train: 2.20223], [Test: 2.26706],\n",
      "          Accuracy: [prop score:  0.95158], [q1: 0.84184], [q0: 0.98212],\n",
      "          Effect: [ate-q], [train: 0.19704], [test: 0.19784]\n",
      "********************************************************************************\n",
      "epoch: 302 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.20174], [Test: 2.26712],\n",
      "          Accuracy: [prop score:  0.95162], [q1: 0.84097], [q0: 0.98241],\n",
      "          Effect: [ate-q], [train: 0.19810], [test: 0.19893]\n",
      "********************************************************************************\n",
      "epoch: 303 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.20121], [Test: 2.26721],\n",
      "          Accuracy: [prop score:  0.95165], [q1: 0.83990], [q0: 0.98244],\n",
      "          Effect: [ate-q], [train: 0.19785], [test: 0.19868]\n",
      "********************************************************************************\n",
      "epoch: 304 / 500, time cost: 46.04 sec, \n",
      "          Loss: [Train: 2.20111], [Test: 2.26740],\n",
      "          Accuracy: [prop score:  0.95175], [q1: 0.83437], [q0: 0.98286],\n",
      "          Effect: [ate-q], [train: 0.19526], [test: 0.19609]\n",
      "********************************************************************************\n",
      "epoch: 305 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.20005], [Test: 2.26740],\n",
      "          Accuracy: [prop score:  0.95177], [q1: 0.84046], [q0: 0.98129],\n",
      "          Effect: [ate-q], [train: 0.19643], [test: 0.19727]\n",
      "********************************************************************************\n",
      "epoch: 306 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.19970], [Test: 2.26733],\n",
      "          Accuracy: [prop score:  0.95183], [q1: 0.83810], [q0: 0.98066],\n",
      "          Effect: [ate-q], [train: 0.19376], [test: 0.19460]\n",
      "********************************************************************************\n",
      "epoch: 307 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.19947], [Test: 2.26706],\n",
      "          Accuracy: [prop score:  0.95192], [q1: 0.84062], [q0: 0.98038],\n",
      "          Effect: [ate-q], [train: 0.19516], [test: 0.19603]\n",
      "********************************************************************************\n",
      "epoch: 308 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.19848], [Test: 2.26737],\n",
      "          Accuracy: [prop score:  0.95189], [q1: 0.82940], [q0: 0.98210],\n",
      "          Effect: [ate-q], [train: 0.19259], [test: 0.19347]\n",
      "********************************************************************************\n",
      "epoch: 309 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.19889], [Test: 2.26726],\n",
      "          Accuracy: [prop score:  0.95205], [q1: 0.82476], [q0: 0.98326],\n",
      "          Effect: [ate-q], [train: 0.19257], [test: 0.19340]\n",
      "********************************************************************************\n",
      "epoch: 310 / 500, time cost: 46.59 sec, \n",
      "          Loss: [Train: 2.19820], [Test: 2.26747],\n",
      "          Accuracy: [prop score:  0.95202], [q1: 0.84185], [q0: 0.98094],\n",
      "          Effect: [ate-q], [train: 0.19901], [test: 0.19987]\n",
      "********************************************************************************\n",
      "epoch: 311 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.19752], [Test: 2.26744],\n",
      "          Accuracy: [prop score:  0.95194], [q1: 0.83933], [q0: 0.98206],\n",
      "          Effect: [ate-q], [train: 0.19975], [test: 0.20062]\n",
      "********************************************************************************\n",
      "epoch: 312 / 500, time cost: 45.46 sec, \n",
      "          Loss: [Train: 2.19725], [Test: 2.26806],\n",
      "          Accuracy: [prop score:  0.95200], [q1: 0.83248], [q0: 0.98107],\n",
      "          Effect: [ate-q], [train: 0.19362], [test: 0.19449]\n",
      "********************************************************************************\n",
      "epoch: 313 / 500, time cost: 47.36 sec, \n",
      "          Loss: [Train: 2.19709], [Test: 2.26750],\n",
      "          Accuracy: [prop score:  0.95212], [q1: 0.84395], [q0: 0.98310],\n",
      "          Effect: [ate-q], [train: 0.20664], [test: 0.20752]\n",
      "********************************************************************************\n",
      "epoch: 314 / 500, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.19630], [Test: 2.26703],\n",
      "          Accuracy: [prop score:  0.95215], [q1: 0.84489], [q0: 0.98135],\n",
      "          Effect: [ate-q], [train: 0.20385], [test: 0.20474]\n",
      "********************************************************************************\n",
      "epoch: 315 / 500, time cost: 47.40 sec, \n",
      "          Loss: [Train: 2.19672], [Test: 2.26774],\n",
      "          Accuracy: [prop score:  0.95222], [q1: 0.83414], [q0: 0.98047],\n",
      "          Effect: [ate-q], [train: 0.19474], [test: 0.19565]\n",
      "********************************************************************************\n",
      "epoch: 316 / 500, time cost: 45.57 sec, \n",
      "          Loss: [Train: 2.19541], [Test: 2.26761],\n",
      "          Accuracy: [prop score:  0.95228], [q1: 0.83114], [q0: 0.98251],\n",
      "          Effect: [ate-q], [train: 0.19796], [test: 0.19886]\n",
      "********************************************************************************\n",
      "epoch: 317 / 500, time cost: 47.71 sec, \n",
      "          Loss: [Train: 2.19537], [Test: 2.26809],\n",
      "          Accuracy: [prop score:  0.95235], [q1: 0.82319], [q0: 0.97853],\n",
      "          Effect: [ate-q], [train: 0.18436], [test: 0.18523]\n",
      "********************************************************************************\n",
      "epoch: 318 / 500, time cost: 45.46 sec, \n",
      "          Loss: [Train: 2.19522], [Test: 2.26774],\n",
      "          Accuracy: [prop score:  0.95235], [q1: 0.83465], [q0: 0.98099],\n",
      "          Effect: [ate-q], [train: 0.19781], [test: 0.19872]\n",
      "********************************************************************************\n",
      "epoch: 319 / 500, time cost: 47.61 sec, \n",
      "          Loss: [Train: 2.19488], [Test: 2.26814],\n",
      "          Accuracy: [prop score:  0.95232], [q1: 0.83785], [q0: 0.98197],\n",
      "          Effect: [ate-q], [train: 0.20269], [test: 0.20359]\n",
      "********************************************************************************\n",
      "epoch: 320 / 500, time cost: 45.53 sec, \n",
      "          Loss: [Train: 2.19420], [Test: 2.26811],\n",
      "          Accuracy: [prop score:  0.95241], [q1: 0.83445], [q0: 0.97951],\n",
      "          Effect: [ate-q], [train: 0.19486], [test: 0.19576]\n",
      "********************************************************************************\n",
      "epoch: 321 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.19378], [Test: 2.26818],\n",
      "          Accuracy: [prop score:  0.95226], [q1: 0.82609], [q0: 0.97981],\n",
      "          Effect: [ate-q], [train: 0.19023], [test: 0.19119]\n",
      "********************************************************************************\n",
      "epoch: 322 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.19359], [Test: 2.26773],\n",
      "          Accuracy: [prop score:  0.95241], [q1: 0.82910], [q0: 0.98154],\n",
      "          Effect: [ate-q], [train: 0.19678], [test: 0.19773]\n",
      "********************************************************************************\n",
      "epoch: 323 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.19245], [Test: 2.26827],\n",
      "          Accuracy: [prop score:  0.95248], [q1: 0.82987], [q0: 0.98110],\n",
      "          Effect: [ate-q], [train: 0.19663], [test: 0.19755]\n",
      "********************************************************************************\n",
      "epoch: 324 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.19262], [Test: 2.26771],\n",
      "          Accuracy: [prop score:  0.95262], [q1: 0.82979], [q0: 0.98009],\n",
      "          Effect: [ate-q], [train: 0.19469], [test: 0.19559]\n",
      "********************************************************************************\n",
      "epoch: 325 / 500, time cost: 47.79 sec, \n",
      "          Loss: [Train: 2.19244], [Test: 2.26833],\n",
      "          Accuracy: [prop score:  0.95266], [q1: 0.82664], [q0: 0.98286],\n",
      "          Effect: [ate-q], [train: 0.20022], [test: 0.20119]\n",
      "********************************************************************************\n",
      "epoch: 326 / 500, time cost: 45.55 sec, \n",
      "          Loss: [Train: 2.19147], [Test: 2.26853],\n",
      "          Accuracy: [prop score:  0.95272], [q1: 0.82277], [q0: 0.98140],\n",
      "          Effect: [ate-q], [train: 0.19413], [test: 0.19508]\n",
      "********************************************************************************\n",
      "epoch: 327 / 500, time cost: 47.77 sec, \n",
      "          Loss: [Train: 2.19129], [Test: 2.26852],\n",
      "          Accuracy: [prop score:  0.95270], [q1: 0.82330], [q0: 0.97918],\n",
      "          Effect: [ate-q], [train: 0.19005], [test: 0.19099]\n",
      "********************************************************************************\n",
      "epoch: 328 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.19050], [Test: 2.26844],\n",
      "          Accuracy: [prop score:  0.95284], [q1: 0.83551], [q0: 0.98227],\n",
      "          Effect: [ate-q], [train: 0.20612], [test: 0.20708]\n",
      "********************************************************************************\n",
      "epoch: 329 / 500, time cost: 47.54 sec, \n",
      "          Loss: [Train: 2.19071], [Test: 2.26891],\n",
      "          Accuracy: [prop score:  0.95301], [q1: 0.81598], [q0: 0.97915],\n",
      "          Effect: [ate-q], [train: 0.18570], [test: 0.18667]\n",
      "********************************************************************************\n",
      "epoch: 330 / 500, time cost: 45.45 sec, \n",
      "          Loss: [Train: 2.19012], [Test: 2.26819],\n",
      "          Accuracy: [prop score:  0.95299], [q1: 0.80989], [q0: 0.97948],\n",
      "          Effect: [ate-q], [train: 0.18370], [test: 0.18467]\n",
      "********************************************************************************\n",
      "epoch: 331 / 500, time cost: 48.25 sec, \n",
      "          Loss: [Train: 2.18960], [Test: 2.26827],\n",
      "          Accuracy: [prop score:  0.95299], [q1: 0.82867], [q0: 0.98117],\n",
      "          Effect: [ate-q], [train: 0.19989], [test: 0.20086]\n",
      "********************************************************************************\n",
      "epoch: 332 / 500, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.18923], [Test: 2.26812],\n",
      "          Accuracy: [prop score:  0.95309], [q1: 0.83076], [q0: 0.98005],\n",
      "          Effect: [ate-q], [train: 0.19910], [test: 0.20007]\n",
      "********************************************************************************\n",
      "epoch: 333 / 500, time cost: 48.28 sec, \n",
      "          Loss: [Train: 2.18880], [Test: 2.26845],\n",
      "          Accuracy: [prop score:  0.95318], [q1: 0.82394], [q0: 0.97972],\n",
      "          Effect: [ate-q], [train: 0.19396], [test: 0.19493]\n",
      "********************************************************************************\n",
      "epoch: 334 / 500, time cost: 46.41 sec, \n",
      "          Loss: [Train: 2.18870], [Test: 2.26863],\n",
      "          Accuracy: [prop score:  0.95310], [q1: 0.82111], [q0: 0.97780],\n",
      "          Effect: [ate-q], [train: 0.18847], [test: 0.18945]\n",
      "********************************************************************************\n",
      "epoch: 335 / 500, time cost: 49.87 sec, \n",
      "          Loss: [Train: 2.18803], [Test: 2.26876],\n",
      "          Accuracy: [prop score:  0.95322], [q1: 0.82182], [q0: 0.97919],\n",
      "          Effect: [ate-q], [train: 0.19180], [test: 0.19280]\n",
      "********************************************************************************\n",
      "epoch: 336 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.18744], [Test: 2.26865],\n",
      "          Accuracy: [prop score:  0.95323], [q1: 0.81727], [q0: 0.98026],\n",
      "          Effect: [ate-q], [train: 0.19219], [test: 0.19318]\n",
      "********************************************************************************\n",
      "epoch: 337 / 500, time cost: 49.10 sec, \n",
      "          Loss: [Train: 2.18683], [Test: 2.26881],\n",
      "          Accuracy: [prop score:  0.95324], [q1: 0.82438], [q0: 0.98043],\n",
      "          Effect: [ate-q], [train: 0.19749], [test: 0.19848]\n",
      "********************************************************************************\n",
      "epoch: 338 / 500, time cost: 46.83 sec, \n",
      "          Loss: [Train: 2.18636], [Test: 2.26881],\n",
      "          Accuracy: [prop score:  0.95333], [q1: 0.81228], [q0: 0.98145],\n",
      "          Effect: [ate-q], [train: 0.19289], [test: 0.19393]\n",
      "********************************************************************************\n",
      "epoch: 339 / 500, time cost: 48.75 sec, \n",
      "          Loss: [Train: 2.18632], [Test: 2.26894],\n",
      "          Accuracy: [prop score:  0.95342], [q1: 0.80415], [q0: 0.97733],\n",
      "          Effect: [ate-q], [train: 0.17996], [test: 0.18100]\n",
      "********************************************************************************\n",
      "epoch: 340 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.18563], [Test: 2.26888],\n",
      "          Accuracy: [prop score:  0.95352], [q1: 0.82573], [q0: 0.97851],\n",
      "          Effect: [ate-q], [train: 0.19563], [test: 0.19665]\n",
      "********************************************************************************\n",
      "epoch: 341 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.18607], [Test: 2.26882],\n",
      "          Accuracy: [prop score:  0.95353], [q1: 0.80797], [q0: 0.98165],\n",
      "          Effect: [ate-q], [train: 0.19227], [test: 0.19329]\n",
      "********************************************************************************\n",
      "epoch: 342 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.18544], [Test: 2.26923],\n",
      "          Accuracy: [prop score:  0.95349], [q1: 0.80680], [q0: 0.97883],\n",
      "          Effect: [ate-q], [train: 0.18536], [test: 0.18644]\n",
      "********************************************************************************\n",
      "epoch: 343 / 500, time cost: 48.77 sec, \n",
      "          Loss: [Train: 2.18424], [Test: 2.26958],\n",
      "          Accuracy: [prop score:  0.95354], [q1: 0.80291], [q0: 0.97844],\n",
      "          Effect: [ate-q], [train: 0.18277], [test: 0.18379]\n",
      "********************************************************************************\n",
      "epoch: 344 / 500, time cost: 46.37 sec, \n",
      "          Loss: [Train: 2.18395], [Test: 2.26922],\n",
      "          Accuracy: [prop score:  0.95362], [q1: 0.81025], [q0: 0.98109],\n",
      "          Effect: [ate-q], [train: 0.19332], [test: 0.19437]\n",
      "********************************************************************************\n",
      "epoch: 345 / 500, time cost: 48.31 sec, \n",
      "          Loss: [Train: 2.18377], [Test: 2.26870],\n",
      "          Accuracy: [prop score:  0.95356], [q1: 0.80774], [q0: 0.97970],\n",
      "          Effect: [ate-q], [train: 0.18876], [test: 0.18981]\n",
      "********************************************************************************\n",
      "epoch: 346 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.18306], [Test: 2.26950],\n",
      "          Accuracy: [prop score:  0.95370], [q1: 0.81474], [q0: 0.97999],\n",
      "          Effect: [ate-q], [train: 0.19420], [test: 0.19526]\n",
      "********************************************************************************\n",
      "epoch: 347 / 500, time cost: 48.11 sec, \n",
      "          Loss: [Train: 2.18268], [Test: 2.26901],\n",
      "          Accuracy: [prop score:  0.95364], [q1: 0.81038], [q0: 0.97734],\n",
      "          Effect: [ate-q], [train: 0.18699], [test: 0.18803]\n",
      "********************************************************************************\n",
      "epoch: 348 / 500, time cost: 45.66 sec, \n",
      "          Loss: [Train: 2.18300], [Test: 2.26943],\n",
      "          Accuracy: [prop score:  0.95373], [q1: 0.81400], [q0: 0.97853],\n",
      "          Effect: [ate-q], [train: 0.19142], [test: 0.19248]\n",
      "********************************************************************************\n",
      "epoch: 349 / 500, time cost: 47.88 sec, \n",
      "          Loss: [Train: 2.18251], [Test: 2.26990],\n",
      "          Accuracy: [prop score:  0.95380], [q1: 0.82507], [q0: 0.98054],\n",
      "          Effect: [ate-q], [train: 0.20345], [test: 0.20450]\n",
      "********************************************************************************\n",
      "epoch: 350 / 500, time cost: 45.48 sec, \n",
      "          Loss: [Train: 2.18236], [Test: 2.26957],\n",
      "          Accuracy: [prop score:  0.95383], [q1: 0.80785], [q0: 0.97853],\n",
      "          Effect: [ate-q], [train: 0.18852], [test: 0.18958]\n",
      "********************************************************************************\n",
      "epoch: 351 / 500, time cost: 48.03 sec, \n",
      "          Loss: [Train: 2.18108], [Test: 2.26983],\n",
      "          Accuracy: [prop score:  0.95382], [q1: 0.80082], [q0: 0.97998],\n",
      "          Effect: [ate-q], [train: 0.18794], [test: 0.18901]\n",
      "********************************************************************************\n",
      "epoch: 352 / 500, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.18045], [Test: 2.27037],\n",
      "          Accuracy: [prop score:  0.95395], [q1: 0.80827], [q0: 0.97950],\n",
      "          Effect: [ate-q], [train: 0.19188], [test: 0.19298]\n",
      "********************************************************************************\n",
      "epoch: 353 / 500, time cost: 47.68 sec, \n",
      "          Loss: [Train: 2.18005], [Test: 2.27023],\n",
      "          Accuracy: [prop score:  0.95389], [q1: 0.80614], [q0: 0.97845],\n",
      "          Effect: [ate-q], [train: 0.18885], [test: 0.18998]\n",
      "********************************************************************************\n",
      "epoch: 354 / 500, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.17990], [Test: 2.27006],\n",
      "          Accuracy: [prop score:  0.95395], [q1: 0.80147], [q0: 0.97764],\n",
      "          Effect: [ate-q], [train: 0.18519], [test: 0.18631]\n",
      "********************************************************************************\n",
      "epoch: 355 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.17917], [Test: 2.27008],\n",
      "          Accuracy: [prop score:  0.95399], [q1: 0.81069], [q0: 0.97679],\n",
      "          Effect: [ate-q], [train: 0.18965], [test: 0.19077]\n",
      "********************************************************************************\n",
      "epoch: 356 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.17868], [Test: 2.27105],\n",
      "          Accuracy: [prop score:  0.95400], [q1: 0.81190], [q0: 0.97431],\n",
      "          Effect: [ate-q], [train: 0.18703], [test: 0.18816]\n",
      "********************************************************************************\n",
      "epoch: 357 / 500, time cost: 47.74 sec, \n",
      "          Loss: [Train: 2.17823], [Test: 2.27028],\n",
      "          Accuracy: [prop score:  0.95405], [q1: 0.80160], [q0: 0.97723],\n",
      "          Effect: [ate-q], [train: 0.18577], [test: 0.18691]\n",
      "********************************************************************************\n",
      "epoch: 358 / 500, time cost: 45.99 sec, \n",
      "          Loss: [Train: 2.17766], [Test: 2.27076],\n",
      "          Accuracy: [prop score:  0.95411], [q1: 0.80962], [q0: 0.97805],\n",
      "          Effect: [ate-q], [train: 0.19228], [test: 0.19341]\n",
      "********************************************************************************\n",
      "epoch: 359 / 500, time cost: 47.41 sec, \n",
      "          Loss: [Train: 2.17756], [Test: 2.27068],\n",
      "          Accuracy: [prop score:  0.95416], [q1: 0.81572], [q0: 0.97857],\n",
      "          Effect: [ate-q], [train: 0.19754], [test: 0.19868]\n",
      "********************************************************************************\n",
      "epoch: 360 / 500, time cost: 45.54 sec, \n",
      "          Loss: [Train: 2.17703], [Test: 2.27078],\n",
      "          Accuracy: [prop score:  0.95423], [q1: 0.79330], [q0: 0.97453],\n",
      "          Effect: [ate-q], [train: 0.17761], [test: 0.17874]\n",
      "********************************************************************************\n",
      "epoch: 361 / 500, time cost: 48.09 sec, \n",
      "          Loss: [Train: 2.17645], [Test: 2.27089],\n",
      "          Accuracy: [prop score:  0.95414], [q1: 0.80343], [q0: 0.97613],\n",
      "          Effect: [ate-q], [train: 0.18671], [test: 0.18788]\n",
      "********************************************************************************\n",
      "epoch: 362 / 500, time cost: 45.56 sec, \n",
      "          Loss: [Train: 2.17616], [Test: 2.27110],\n",
      "          Accuracy: [prop score:  0.95427], [q1: 0.80522], [q0: 0.97816],\n",
      "          Effect: [ate-q], [train: 0.19180], [test: 0.19296]\n",
      "********************************************************************************\n",
      "epoch: 363 / 500, time cost: 48.70 sec, \n",
      "          Loss: [Train: 2.17581], [Test: 2.27081],\n",
      "          Accuracy: [prop score:  0.95434], [q1: 0.80311], [q0: 0.97856],\n",
      "          Effect: [ate-q], [train: 0.19157], [test: 0.19273]\n",
      "********************************************************************************\n",
      "epoch: 364 / 500, time cost: 46.67 sec, \n",
      "          Loss: [Train: 2.17561], [Test: 2.27133],\n",
      "          Accuracy: [prop score:  0.95427], [q1: 0.80156], [q0: 0.97531],\n",
      "          Effect: [ate-q], [train: 0.18517], [test: 0.18632]\n",
      "********************************************************************************\n",
      "epoch: 365 / 500, time cost: 48.82 sec, \n",
      "          Loss: [Train: 2.17456], [Test: 2.27075],\n",
      "          Accuracy: [prop score:  0.95438], [q1: 0.79801], [q0: 0.97738],\n",
      "          Effect: [ate-q], [train: 0.18695], [test: 0.18813]\n",
      "********************************************************************************\n",
      "epoch: 366 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.17455], [Test: 2.27127],\n",
      "          Accuracy: [prop score:  0.95432], [q1: 0.79858], [q0: 0.97730],\n",
      "          Effect: [ate-q], [train: 0.18743], [test: 0.18861]\n",
      "********************************************************************************\n",
      "epoch: 367 / 500, time cost: 48.70 sec, \n",
      "          Loss: [Train: 2.17386], [Test: 2.27178],\n",
      "          Accuracy: [prop score:  0.95442], [q1: 0.80318], [q0: 0.97859],\n",
      "          Effect: [ate-q], [train: 0.19368], [test: 0.19489]\n",
      "********************************************************************************\n",
      "epoch: 368 / 500, time cost: 46.80 sec, \n",
      "          Loss: [Train: 2.17375], [Test: 2.27203],\n",
      "          Accuracy: [prop score:  0.95446], [q1: 0.80418], [q0: 0.97707],\n",
      "          Effect: [ate-q], [train: 0.19176], [test: 0.19297]\n",
      "********************************************************************************\n",
      "epoch: 369 / 500, time cost: 48.66 sec, \n",
      "          Loss: [Train: 2.17268], [Test: 2.27130],\n",
      "          Accuracy: [prop score:  0.95455], [q1: 0.79656], [q0: 0.97550],\n",
      "          Effect: [ate-q], [train: 0.18556], [test: 0.18672]\n",
      "********************************************************************************\n",
      "epoch: 370 / 500, time cost: 46.56 sec, \n",
      "          Loss: [Train: 2.17248], [Test: 2.27218],\n",
      "          Accuracy: [prop score:  0.95455], [q1: 0.81396], [q0: 0.97388],\n",
      "          Effect: [ate-q], [train: 0.19343], [test: 0.19462]\n",
      "********************************************************************************\n",
      "epoch: 371 / 500, time cost: 48.31 sec, \n",
      "          Loss: [Train: 2.17230], [Test: 2.27163],\n",
      "          Accuracy: [prop score:  0.95461], [q1: 0.80115], [q0: 0.98020],\n",
      "          Effect: [ate-q], [train: 0.19740], [test: 0.19860]\n",
      "********************************************************************************\n",
      "epoch: 372 / 500, time cost: 45.88 sec, \n",
      "          Loss: [Train: 2.17153], [Test: 2.27189],\n",
      "          Accuracy: [prop score:  0.95463], [q1: 0.81435], [q0: 0.97412],\n",
      "          Effect: [ate-q], [train: 0.19505], [test: 0.19623]\n",
      "********************************************************************************\n",
      "epoch: 373 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.17118], [Test: 2.27242],\n",
      "          Accuracy: [prop score:  0.95472], [q1: 0.76984], [q0: 0.97584],\n",
      "          Effect: [ate-q], [train: 0.17366], [test: 0.17488]\n",
      "********************************************************************************\n",
      "epoch: 374 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.17126], [Test: 2.27270],\n",
      "          Accuracy: [prop score:  0.95481], [q1: 0.79094], [q0: 0.97985],\n",
      "          Effect: [ate-q], [train: 0.19207], [test: 0.19330]\n",
      "********************************************************************************\n",
      "epoch: 375 / 500, time cost: 47.72 sec, \n",
      "          Loss: [Train: 2.17036], [Test: 2.27208],\n",
      "          Accuracy: [prop score:  0.95481], [q1: 0.80441], [q0: 0.97723],\n",
      "          Effect: [ate-q], [train: 0.19541], [test: 0.19667]\n",
      "********************************************************************************\n",
      "epoch: 376 / 500, time cost: 45.72 sec, \n",
      "          Loss: [Train: 2.16972], [Test: 2.27286],\n",
      "          Accuracy: [prop score:  0.95486], [q1: 0.80655], [q0: 0.97778],\n",
      "          Effect: [ate-q], [train: 0.19806], [test: 0.19930]\n",
      "********************************************************************************\n",
      "epoch: 377 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.16953], [Test: 2.27305],\n",
      "          Accuracy: [prop score:  0.95492], [q1: 0.81549], [q0: 0.97788],\n",
      "          Effect: [ate-q], [train: 0.20471], [test: 0.20596]\n",
      "********************************************************************************\n",
      "epoch: 378 / 500, time cost: 46.28 sec, \n",
      "          Loss: [Train: 2.16912], [Test: 2.27248],\n",
      "          Accuracy: [prop score:  0.95492], [q1: 0.78948], [q0: 0.97572],\n",
      "          Effect: [ate-q], [train: 0.18550], [test: 0.18675]\n",
      "********************************************************************************\n",
      "epoch: 379 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.16821], [Test: 2.27286],\n",
      "          Accuracy: [prop score:  0.95500], [q1: 0.79974], [q0: 0.97891],\n",
      "          Effect: [ate-q], [train: 0.19735], [test: 0.19861]\n",
      "********************************************************************************\n",
      "epoch: 380 / 500, time cost: 46.19 sec, \n",
      "          Loss: [Train: 2.16805], [Test: 2.27363],\n",
      "          Accuracy: [prop score:  0.95508], [q1: 0.80225], [q0: 0.97427],\n",
      "          Effect: [ate-q], [train: 0.19141], [test: 0.19268]\n",
      "********************************************************************************\n",
      "epoch: 381 / 500, time cost: 48.66 sec, \n",
      "          Loss: [Train: 2.16813], [Test: 2.27322],\n",
      "          Accuracy: [prop score:  0.95514], [q1: 0.79989], [q0: 0.97547],\n",
      "          Effect: [ate-q], [train: 0.19219], [test: 0.19345]\n",
      "********************************************************************************\n",
      "epoch: 382 / 500, time cost: 46.51 sec, \n",
      "          Loss: [Train: 2.16722], [Test: 2.27358],\n",
      "          Accuracy: [prop score:  0.95513], [q1: 0.80590], [q0: 0.97422],\n",
      "          Effect: [ate-q], [train: 0.19437], [test: 0.19564]\n",
      "********************************************************************************\n",
      "epoch: 383 / 500, time cost: 48.74 sec, \n",
      "          Loss: [Train: 2.16645], [Test: 2.27294],\n",
      "          Accuracy: [prop score:  0.95519], [q1: 0.79113], [q0: 0.97498],\n",
      "          Effect: [ate-q], [train: 0.18731], [test: 0.18858]\n",
      "********************************************************************************\n",
      "epoch: 384 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 2.16630], [Test: 2.27377],\n",
      "          Accuracy: [prop score:  0.95521], [q1: 0.80085], [q0: 0.97425],\n",
      "          Effect: [ate-q], [train: 0.19228], [test: 0.19357]\n",
      "********************************************************************************\n",
      "epoch: 385 / 500, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.16588], [Test: 2.27350],\n",
      "          Accuracy: [prop score:  0.95527], [q1: 0.78057], [q0: 0.97357],\n",
      "          Effect: [ate-q], [train: 0.17964], [test: 0.18094]\n",
      "********************************************************************************\n",
      "epoch: 386 / 500, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.16591], [Test: 2.27386],\n",
      "          Accuracy: [prop score:  0.95544], [q1: 0.77702], [q0: 0.97369],\n",
      "          Effect: [ate-q], [train: 0.17860], [test: 0.17992]\n",
      "********************************************************************************\n",
      "epoch: 387 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.16472], [Test: 2.27377],\n",
      "          Accuracy: [prop score:  0.95539], [q1: 0.78693], [q0: 0.97568],\n",
      "          Effect: [ate-q], [train: 0.18795], [test: 0.18931]\n",
      "********************************************************************************\n",
      "epoch: 388 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.16470], [Test: 2.27402],\n",
      "          Accuracy: [prop score:  0.95550], [q1: 0.79071], [q0: 0.97692],\n",
      "          Effect: [ate-q], [train: 0.19260], [test: 0.19389]\n",
      "********************************************************************************\n",
      "epoch: 389 / 500, time cost: 48.68 sec, \n",
      "          Loss: [Train: 2.16446], [Test: 2.27453],\n",
      "          Accuracy: [prop score:  0.95550], [q1: 0.79991], [q0: 0.97204],\n",
      "          Effect: [ate-q], [train: 0.19043], [test: 0.19178]\n",
      "********************************************************************************\n",
      "epoch: 390 / 500, time cost: 46.64 sec, \n",
      "          Loss: [Train: 2.16335], [Test: 2.27487],\n",
      "          Accuracy: [prop score:  0.95562], [q1: 0.77044], [q0: 0.97345],\n",
      "          Effect: [ate-q], [train: 0.17663], [test: 0.17795]\n",
      "********************************************************************************\n",
      "epoch: 391 / 500, time cost: 48.86 sec, \n",
      "          Loss: [Train: 2.16329], [Test: 2.27474],\n",
      "          Accuracy: [prop score:  0.95557], [q1: 0.78572], [q0: 0.97718],\n",
      "          Effect: [ate-q], [train: 0.19161], [test: 0.19296]\n",
      "********************************************************************************\n",
      "epoch: 392 / 500, time cost: 46.70 sec, \n",
      "          Loss: [Train: 2.16285], [Test: 2.27496],\n",
      "          Accuracy: [prop score:  0.95563], [q1: 0.78965], [q0: 0.97282],\n",
      "          Effect: [ate-q], [train: 0.18678], [test: 0.18812]\n",
      "********************************************************************************\n",
      "epoch: 393 / 500, time cost: 48.50 sec, \n",
      "          Loss: [Train: 2.16179], [Test: 2.27484],\n",
      "          Accuracy: [prop score:  0.95572], [q1: 0.80532], [q0: 0.97331],\n",
      "          Effect: [ate-q], [train: 0.19760], [test: 0.19893]\n",
      "********************************************************************************\n",
      "epoch: 394 / 500, time cost: 46.56 sec, \n",
      "          Loss: [Train: 2.16151], [Test: 2.27468],\n",
      "          Accuracy: [prop score:  0.95578], [q1: 0.78764], [q0: 0.97563],\n",
      "          Effect: [ate-q], [train: 0.19139], [test: 0.19275]\n",
      "********************************************************************************\n",
      "epoch: 395 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.16113], [Test: 2.27494],\n",
      "          Accuracy: [prop score:  0.95577], [q1: 0.79697], [q0: 0.97421],\n",
      "          Effect: [ate-q], [train: 0.19476], [test: 0.19609]\n",
      "********************************************************************************\n",
      "epoch: 396 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.16054], [Test: 2.27564],\n",
      "          Accuracy: [prop score:  0.95582], [q1: 0.80118], [q0: 0.97264],\n",
      "          Effect: [ate-q], [train: 0.19510], [test: 0.19645]\n",
      "********************************************************************************\n",
      "epoch: 397 / 500, time cost: 48.71 sec, \n",
      "          Loss: [Train: 2.16041], [Test: 2.27546],\n",
      "          Accuracy: [prop score:  0.95586], [q1: 0.79198], [q0: 0.97180],\n",
      "          Effect: [ate-q], [train: 0.18850], [test: 0.18990]\n",
      "********************************************************************************\n",
      "epoch: 398 / 500, time cost: 46.74 sec, \n",
      "          Loss: [Train: 2.15955], [Test: 2.27603],\n",
      "          Accuracy: [prop score:  0.95589], [q1: 0.76879], [q0: 0.97543],\n",
      "          Effect: [ate-q], [train: 0.18218], [test: 0.18358]\n",
      "********************************************************************************\n",
      "epoch: 399 / 500, time cost: 49.00 sec, \n",
      "          Loss: [Train: 2.15859], [Test: 2.27559],\n",
      "          Accuracy: [prop score:  0.95588], [q1: 0.77752], [q0: 0.97187],\n",
      "          Effect: [ate-q], [train: 0.18173], [test: 0.18311]\n",
      "********************************************************************************\n",
      "epoch: 400 / 500, time cost: 46.98 sec, \n",
      "          Loss: [Train: 2.15870], [Test: 2.27619],\n",
      "          Accuracy: [prop score:  0.95587], [q1: 0.78521], [q0: 0.97291],\n",
      "          Effect: [ate-q], [train: 0.18792], [test: 0.18930]\n",
      "********************************************************************************\n",
      "epoch: 401 / 500, time cost: 49.11 sec, \n",
      "          Loss: [Train: 2.15808], [Test: 2.27592],\n",
      "          Accuracy: [prop score:  0.95598], [q1: 0.76344], [q0: 0.96967],\n",
      "          Effect: [ate-q], [train: 0.17208], [test: 0.17352]\n",
      "********************************************************************************\n",
      "epoch: 402 / 500, time cost: 47.21 sec, \n",
      "          Loss: [Train: 2.15728], [Test: 2.27606],\n",
      "          Accuracy: [prop score:  0.95596], [q1: 0.78370], [q0: 0.97255],\n",
      "          Effect: [ate-q], [train: 0.18748], [test: 0.18887]\n",
      "********************************************************************************\n",
      "epoch: 403 / 500, time cost: 48.79 sec, \n",
      "          Loss: [Train: 2.15733], [Test: 2.27675],\n",
      "          Accuracy: [prop score:  0.95594], [q1: 0.77867], [q0: 0.96984],\n",
      "          Effect: [ate-q], [train: 0.18132], [test: 0.18274]\n",
      "********************************************************************************\n",
      "epoch: 404 / 500, time cost: 46.49 sec, \n",
      "          Loss: [Train: 2.15675], [Test: 2.27650],\n",
      "          Accuracy: [prop score:  0.95603], [q1: 0.78170], [q0: 0.97435],\n",
      "          Effect: [ate-q], [train: 0.18958], [test: 0.19102]\n",
      "********************************************************************************\n",
      "epoch: 405 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.15680], [Test: 2.27624],\n",
      "          Accuracy: [prop score:  0.95601], [q1: 0.77744], [q0: 0.97295],\n",
      "          Effect: [ate-q], [train: 0.18572], [test: 0.18710]\n",
      "********************************************************************************\n",
      "epoch: 406 / 500, time cost: 45.61 sec, \n",
      "          Loss: [Train: 2.15596], [Test: 2.27668],\n",
      "          Accuracy: [prop score:  0.95614], [q1: 0.78334], [q0: 0.97228],\n",
      "          Effect: [ate-q], [train: 0.18838], [test: 0.18982]\n",
      "********************************************************************************\n",
      "epoch: 407 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.15582], [Test: 2.27684],\n",
      "          Accuracy: [prop score:  0.95613], [q1: 0.78307], [q0: 0.96861],\n",
      "          Effect: [ate-q], [train: 0.18377], [test: 0.18520]\n",
      "********************************************************************************\n",
      "epoch: 408 / 500, time cost: 45.59 sec, \n",
      "          Loss: [Train: 2.15464], [Test: 2.27741],\n",
      "          Accuracy: [prop score:  0.95619], [q1: 0.78969], [q0: 0.97005],\n",
      "          Effect: [ate-q], [train: 0.18986], [test: 0.19135]\n",
      "********************************************************************************\n",
      "epoch: 409 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.15492], [Test: 2.27781],\n",
      "          Accuracy: [prop score:  0.95623], [q1: 0.78470], [q0: 0.96744],\n",
      "          Effect: [ate-q], [train: 0.18449], [test: 0.18593]\n",
      "********************************************************************************\n",
      "epoch: 410 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.15363], [Test: 2.27741],\n",
      "          Accuracy: [prop score:  0.95625], [q1: 0.77292], [q0: 0.96749],\n",
      "          Effect: [ate-q], [train: 0.17832], [test: 0.17981]\n",
      "********************************************************************************\n",
      "epoch: 411 / 500, time cost: 48.67 sec, \n",
      "          Loss: [Train: 2.15355], [Test: 2.27784],\n",
      "          Accuracy: [prop score:  0.95625], [q1: 0.78466], [q0: 0.97099],\n",
      "          Effect: [ate-q], [train: 0.18971], [test: 0.19116]\n",
      "********************************************************************************\n",
      "epoch: 412 / 500, time cost: 45.17 sec, \n",
      "          Loss: [Train: 2.15325], [Test: 2.27795],\n",
      "          Accuracy: [prop score:  0.95633], [q1: 0.75199], [q0: 0.97102],\n",
      "          Effect: [ate-q], [train: 0.17271], [test: 0.17423]\n",
      "********************************************************************************\n",
      "epoch: 413 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.15219], [Test: 2.27772],\n",
      "          Accuracy: [prop score:  0.95633], [q1: 0.77814], [q0: 0.96912],\n",
      "          Effect: [ate-q], [train: 0.18439], [test: 0.18590]\n",
      "********************************************************************************\n",
      "epoch: 414 / 500, time cost: 46.39 sec, \n",
      "          Loss: [Train: 2.15185], [Test: 2.27797],\n",
      "          Accuracy: [prop score:  0.95633], [q1: 0.76837], [q0: 0.97354],\n",
      "          Effect: [ate-q], [train: 0.18555], [test: 0.18704]\n",
      "********************************************************************************\n",
      "epoch: 415 / 500, time cost: 48.81 sec, \n",
      "          Loss: [Train: 2.15176], [Test: 2.27814],\n",
      "          Accuracy: [prop score:  0.95630], [q1: 0.78433], [q0: 0.97327],\n",
      "          Effect: [ate-q], [train: 0.19503], [test: 0.19653]\n",
      "********************************************************************************\n",
      "epoch: 416 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 2.15077], [Test: 2.27848],\n",
      "          Accuracy: [prop score:  0.95640], [q1: 0.78500], [q0: 0.96514],\n",
      "          Effect: [ate-q], [train: 0.18551], [test: 0.18704]\n",
      "********************************************************************************\n",
      "epoch: 417 / 500, time cost: 48.84 sec, \n",
      "          Loss: [Train: 2.15067], [Test: 2.27873],\n",
      "          Accuracy: [prop score:  0.95645], [q1: 0.77722], [q0: 0.96975],\n",
      "          Effect: [ate-q], [train: 0.18649], [test: 0.18800]\n",
      "********************************************************************************\n",
      "epoch: 418 / 500, time cost: 46.90 sec, \n",
      "          Loss: [Train: 2.15008], [Test: 2.27866],\n",
      "          Accuracy: [prop score:  0.95644], [q1: 0.77786], [q0: 0.97094],\n",
      "          Effect: [ate-q], [train: 0.18911], [test: 0.19062]\n",
      "********************************************************************************\n",
      "epoch: 419 / 500, time cost: 49.02 sec, \n",
      "          Loss: [Train: 2.14990], [Test: 2.27950],\n",
      "          Accuracy: [prop score:  0.95656], [q1: 0.75354], [q0: 0.96569],\n",
      "          Effect: [ate-q], [train: 0.17016], [test: 0.17169]\n",
      "********************************************************************************\n",
      "epoch: 420 / 500, time cost: 47.06 sec, \n",
      "          Loss: [Train: 2.14945], [Test: 2.27872],\n",
      "          Accuracy: [prop score:  0.95641], [q1: 0.78368], [q0: 0.97002],\n",
      "          Effect: [ate-q], [train: 0.19191], [test: 0.19345]\n",
      "********************************************************************************\n",
      "epoch: 421 / 500, time cost: 48.33 sec, \n",
      "          Loss: [Train: 2.14834], [Test: 2.27877],\n",
      "          Accuracy: [prop score:  0.95656], [q1: 0.76377], [q0: 0.96898],\n",
      "          Effect: [ate-q], [train: 0.18010], [test: 0.18161]\n",
      "********************************************************************************\n",
      "epoch: 422 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.14836], [Test: 2.27994],\n",
      "          Accuracy: [prop score:  0.95662], [q1: 0.78509], [q0: 0.96433],\n",
      "          Effect: [ate-q], [train: 0.18734], [test: 0.18888]\n",
      "********************************************************************************\n",
      "epoch: 423 / 500, time cost: 48.78 sec, \n",
      "          Loss: [Train: 2.14800], [Test: 2.28062],\n",
      "          Accuracy: [prop score:  0.95656], [q1: 0.77368], [q0: 0.96744],\n",
      "          Effect: [ate-q], [train: 0.18452], [test: 0.18607]\n",
      "********************************************************************************\n",
      "epoch: 424 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 2.14703], [Test: 2.27918],\n",
      "          Accuracy: [prop score:  0.95661], [q1: 0.77708], [q0: 0.96829],\n",
      "          Effect: [ate-q], [train: 0.18792], [test: 0.18949]\n",
      "********************************************************************************\n",
      "epoch: 425 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.14634], [Test: 2.28040],\n",
      "          Accuracy: [prop score:  0.95674], [q1: 0.78437], [q0: 0.96721],\n",
      "          Effect: [ate-q], [train: 0.19130], [test: 0.19285]\n",
      "********************************************************************************\n",
      "epoch: 426 / 500, time cost: 46.43 sec, \n",
      "          Loss: [Train: 2.14645], [Test: 2.28053],\n",
      "          Accuracy: [prop score:  0.95671], [q1: 0.77917], [q0: 0.96936],\n",
      "          Effect: [ate-q], [train: 0.19157], [test: 0.19311]\n",
      "********************************************************************************\n",
      "epoch: 427 / 500, time cost: 47.72 sec, \n",
      "          Loss: [Train: 2.14495], [Test: 2.27991],\n",
      "          Accuracy: [prop score:  0.95674], [q1: 0.76237], [q0: 0.97018],\n",
      "          Effect: [ate-q], [train: 0.18385], [test: 0.18543]\n",
      "********************************************************************************\n",
      "epoch: 428 / 500, time cost: 46.50 sec, \n",
      "          Loss: [Train: 2.14514], [Test: 2.28078],\n",
      "          Accuracy: [prop score:  0.95674], [q1: 0.77633], [q0: 0.96215],\n",
      "          Effect: [ate-q], [train: 0.18264], [test: 0.18422]\n",
      "********************************************************************************\n",
      "epoch: 429 / 500, time cost: 48.47 sec, \n",
      "          Loss: [Train: 2.14460], [Test: 2.28121],\n",
      "          Accuracy: [prop score:  0.95676], [q1: 0.75717], [q0: 0.96961],\n",
      "          Effect: [ate-q], [train: 0.18094], [test: 0.18253]\n",
      "********************************************************************************\n",
      "epoch: 430 / 500, time cost: 46.70 sec, \n",
      "          Loss: [Train: 2.14356], [Test: 2.28156],\n",
      "          Accuracy: [prop score:  0.95686], [q1: 0.74413], [q0: 0.96639],\n",
      "          Effect: [ate-q], [train: 0.17120], [test: 0.17282]\n",
      "********************************************************************************\n",
      "epoch: 431 / 500, time cost: 48.68 sec, \n",
      "          Loss: [Train: 2.14327], [Test: 2.28066],\n",
      "          Accuracy: [prop score:  0.95685], [q1: 0.77283], [q0: 0.97260],\n",
      "          Effect: [ate-q], [train: 0.19479], [test: 0.19643]\n",
      "********************************************************************************\n",
      "epoch: 432 / 500, time cost: 46.71 sec, \n",
      "          Loss: [Train: 2.14301], [Test: 2.28182],\n",
      "          Accuracy: [prop score:  0.95692], [q1: 0.77824], [q0: 0.97151],\n",
      "          Effect: [ate-q], [train: 0.19646], [test: 0.19809]\n",
      "********************************************************************************\n",
      "epoch: 433 / 500, time cost: 48.90 sec, \n",
      "          Loss: [Train: 2.14228], [Test: 2.28193],\n",
      "          Accuracy: [prop score:  0.95694], [q1: 0.78457], [q0: 0.95976],\n",
      "          Effect: [ate-q], [train: 0.18780], [test: 0.18939]\n",
      "********************************************************************************\n",
      "epoch: 434 / 500, time cost: 46.70 sec, \n",
      "          Loss: [Train: 2.14194], [Test: 2.28116],\n",
      "          Accuracy: [prop score:  0.95693], [q1: 0.76721], [q0: 0.96508],\n",
      "          Effect: [ate-q], [train: 0.18327], [test: 0.18489]\n",
      "********************************************************************************\n",
      "epoch: 435 / 500, time cost: 49.09 sec, \n",
      "          Loss: [Train: 2.14144], [Test: 2.28168],\n",
      "          Accuracy: [prop score:  0.95693], [q1: 0.75351], [q0: 0.95961],\n",
      "          Effect: [ate-q], [train: 0.17105], [test: 0.17269]\n",
      "********************************************************************************\n",
      "epoch: 436 / 500, time cost: 46.90 sec, \n",
      "          Loss: [Train: 2.14116], [Test: 2.28191],\n",
      "          Accuracy: [prop score:  0.95705], [q1: 0.76155], [q0: 0.96194],\n",
      "          Effect: [ate-q], [train: 0.17762], [test: 0.17927]\n",
      "********************************************************************************\n",
      "epoch: 437 / 500, time cost: 49.08 sec, \n",
      "          Loss: [Train: 2.14048], [Test: 2.28194],\n",
      "          Accuracy: [prop score:  0.95711], [q1: 0.76892], [q0: 0.95940],\n",
      "          Effect: [ate-q], [train: 0.17953], [test: 0.18116]\n",
      "********************************************************************************\n",
      "epoch: 438 / 500, time cost: 46.83 sec, \n",
      "          Loss: [Train: 2.13981], [Test: 2.28196],\n",
      "          Accuracy: [prop score:  0.95713], [q1: 0.76225], [q0: 0.96831],\n",
      "          Effect: [ate-q], [train: 0.18622], [test: 0.18787]\n",
      "********************************************************************************\n",
      "epoch: 439 / 500, time cost: 48.44 sec, \n",
      "          Loss: [Train: 2.13929], [Test: 2.28271],\n",
      "          Accuracy: [prop score:  0.95713], [q1: 0.76113], [q0: 0.96190],\n",
      "          Effect: [ate-q], [train: 0.17847], [test: 0.18016]\n",
      "********************************************************************************\n",
      "epoch: 440 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.13867], [Test: 2.28275],\n",
      "          Accuracy: [prop score:  0.95717], [q1: 0.76977], [q0: 0.96666],\n",
      "          Effect: [ate-q], [train: 0.18947], [test: 0.19113]\n",
      "********************************************************************************\n",
      "epoch: 441 / 500, time cost: 48.75 sec, \n",
      "          Loss: [Train: 2.13865], [Test: 2.28317],\n",
      "          Accuracy: [prop score:  0.95714], [q1: 0.77189], [q0: 0.96260],\n",
      "          Effect: [ate-q], [train: 0.18670], [test: 0.18837]\n",
      "********************************************************************************\n",
      "epoch: 442 / 500, time cost: 45.88 sec, \n",
      "          Loss: [Train: 2.13797], [Test: 2.28345],\n",
      "          Accuracy: [prop score:  0.95725], [q1: 0.76861], [q0: 0.96429],\n",
      "          Effect: [ate-q], [train: 0.18723], [test: 0.18890]\n",
      "********************************************************************************\n",
      "epoch: 443 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.13724], [Test: 2.28309],\n",
      "          Accuracy: [prop score:  0.95729], [q1: 0.75595], [q0: 0.96346],\n",
      "          Effect: [ate-q], [train: 0.17947], [test: 0.18115]\n",
      "********************************************************************************\n",
      "epoch: 444 / 500, time cost: 46.47 sec, \n",
      "          Loss: [Train: 2.13674], [Test: 2.28441],\n",
      "          Accuracy: [prop score:  0.95725], [q1: 0.73455], [q0: 0.96110],\n",
      "          Effect: [ate-q], [train: 0.16743], [test: 0.16910]\n",
      "********************************************************************************\n",
      "epoch: 445 / 500, time cost: 48.39 sec, \n",
      "          Loss: [Train: 2.13687], [Test: 2.28456],\n",
      "          Accuracy: [prop score:  0.95737], [q1: 0.73849], [q0: 0.95925],\n",
      "          Effect: [ate-q], [train: 0.16791], [test: 0.16963]\n",
      "********************************************************************************\n",
      "epoch: 446 / 500, time cost: 46.37 sec, \n",
      "          Loss: [Train: 2.13571], [Test: 2.28425],\n",
      "          Accuracy: [prop score:  0.95730], [q1: 0.77720], [q0: 0.96478],\n",
      "          Effect: [ate-q], [train: 0.19491], [test: 0.19664]\n",
      "********************************************************************************\n",
      "epoch: 447 / 500, time cost: 48.42 sec, \n",
      "          Loss: [Train: 2.13514], [Test: 2.28485],\n",
      "          Accuracy: [prop score:  0.95747], [q1: 0.76793], [q0: 0.96393],\n",
      "          Effect: [ate-q], [train: 0.18863], [test: 0.19037]\n",
      "********************************************************************************\n",
      "epoch: 448 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.13444], [Test: 2.28494],\n",
      "          Accuracy: [prop score:  0.95746], [q1: 0.76468], [q0: 0.96444],\n",
      "          Effect: [ate-q], [train: 0.18777], [test: 0.18954]\n",
      "********************************************************************************\n",
      "epoch: 449 / 500, time cost: 48.47 sec, \n",
      "          Loss: [Train: 2.13363], [Test: 2.28470],\n",
      "          Accuracy: [prop score:  0.95736], [q1: 0.76626], [q0: 0.95400],\n",
      "          Effect: [ate-q], [train: 0.17890], [test: 0.18063]\n",
      "********************************************************************************\n",
      "epoch: 450 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.13354], [Test: 2.28490],\n",
      "          Accuracy: [prop score:  0.95747], [q1: 0.76393], [q0: 0.96032],\n",
      "          Effect: [ate-q], [train: 0.18397], [test: 0.18573]\n",
      "********************************************************************************\n",
      "epoch: 451 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.13306], [Test: 2.28524],\n",
      "          Accuracy: [prop score:  0.95736], [q1: 0.75813], [q0: 0.96678],\n",
      "          Effect: [ate-q], [train: 0.18836], [test: 0.19011]\n",
      "********************************************************************************\n",
      "epoch: 452 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.13236], [Test: 2.28578],\n",
      "          Accuracy: [prop score:  0.95747], [q1: 0.74642], [q0: 0.95444],\n",
      "          Effect: [ate-q], [train: 0.17036], [test: 0.17209]\n",
      "********************************************************************************\n",
      "epoch: 453 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.13237], [Test: 2.28590],\n",
      "          Accuracy: [prop score:  0.95758], [q1: 0.75869], [q0: 0.96736],\n",
      "          Effect: [ate-q], [train: 0.19034], [test: 0.19216]\n",
      "********************************************************************************\n",
      "epoch: 454 / 500, time cost: 46.68 sec, \n",
      "          Loss: [Train: 2.13163], [Test: 2.28617],\n",
      "          Accuracy: [prop score:  0.95749], [q1: 0.74515], [q0: 0.95571],\n",
      "          Effect: [ate-q], [train: 0.17258], [test: 0.17436]\n",
      "********************************************************************************\n",
      "epoch: 455 / 500, time cost: 49.03 sec, \n",
      "          Loss: [Train: 2.13056], [Test: 2.28616],\n",
      "          Accuracy: [prop score:  0.95758], [q1: 0.73953], [q0: 0.95608],\n",
      "          Effect: [ate-q], [train: 0.17013], [test: 0.17190]\n",
      "********************************************************************************\n",
      "epoch: 456 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 2.13058], [Test: 2.28619],\n",
      "          Accuracy: [prop score:  0.95759], [q1: 0.75123], [q0: 0.95365],\n",
      "          Effect: [ate-q], [train: 0.17369], [test: 0.17546]\n",
      "********************************************************************************\n",
      "epoch: 457 / 500, time cost: 49.11 sec, \n",
      "          Loss: [Train: 2.13017], [Test: 2.28646],\n",
      "          Accuracy: [prop score:  0.95757], [q1: 0.74192], [q0: 0.95957],\n",
      "          Effect: [ate-q], [train: 0.17516], [test: 0.17691]\n",
      "********************************************************************************\n",
      "epoch: 458 / 500, time cost: 47.11 sec, \n",
      "          Loss: [Train: 2.12959], [Test: 2.28700],\n",
      "          Accuracy: [prop score:  0.95766], [q1: 0.75615], [q0: 0.95600],\n",
      "          Effect: [ate-q], [train: 0.17933], [test: 0.18111]\n",
      "********************************************************************************\n",
      "epoch: 459 / 500, time cost: 49.18 sec, \n",
      "          Loss: [Train: 2.12794], [Test: 2.28744],\n",
      "          Accuracy: [prop score:  0.95758], [q1: 0.76329], [q0: 0.96030],\n",
      "          Effect: [ate-q], [train: 0.18843], [test: 0.19024]\n",
      "********************************************************************************\n",
      "epoch: 460 / 500, time cost: 46.94 sec, \n",
      "          Loss: [Train: 2.12848], [Test: 2.28714],\n",
      "          Accuracy: [prop score:  0.95772], [q1: 0.76116], [q0: 0.96005],\n",
      "          Effect: [ate-q], [train: 0.18748], [test: 0.18931]\n",
      "********************************************************************************\n",
      "epoch: 461 / 500, time cost: 49.18 sec, \n",
      "          Loss: [Train: 2.12753], [Test: 2.28796],\n",
      "          Accuracy: [prop score:  0.95775], [q1: 0.75130], [q0: 0.95290],\n",
      "          Effect: [ate-q], [train: 0.17525], [test: 0.17705]\n",
      "********************************************************************************\n",
      "epoch: 462 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 2.12728], [Test: 2.28807],\n",
      "          Accuracy: [prop score:  0.95771], [q1: 0.76209], [q0: 0.95228],\n",
      "          Effect: [ate-q], [train: 0.18106], [test: 0.18288]\n",
      "********************************************************************************\n",
      "epoch: 463 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.12598], [Test: 2.28788],\n",
      "          Accuracy: [prop score:  0.95774], [q1: 0.75054], [q0: 0.95360],\n",
      "          Effect: [ate-q], [train: 0.17655], [test: 0.17839]\n",
      "********************************************************************************\n",
      "epoch: 464 / 500, time cost: 46.83 sec, \n",
      "          Loss: [Train: 2.12613], [Test: 2.28861],\n",
      "          Accuracy: [prop score:  0.95769], [q1: 0.75332], [q0: 0.95449],\n",
      "          Effect: [ate-q], [train: 0.17954], [test: 0.18137]\n",
      "********************************************************************************\n",
      "epoch: 465 / 500, time cost: 48.66 sec, \n",
      "          Loss: [Train: 2.12553], [Test: 2.28832],\n",
      "          Accuracy: [prop score:  0.95780], [q1: 0.74321], [q0: 0.96091],\n",
      "          Effect: [ate-q], [train: 0.18169], [test: 0.18351]\n",
      "********************************************************************************\n",
      "epoch: 466 / 500, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.12432], [Test: 2.28859],\n",
      "          Accuracy: [prop score:  0.95780], [q1: 0.76180], [q0: 0.96117],\n",
      "          Effect: [ate-q], [train: 0.19214], [test: 0.19400]\n",
      "********************************************************************************\n",
      "epoch: 467 / 500, time cost: 48.77 sec, \n",
      "          Loss: [Train: 2.12441], [Test: 2.28892],\n",
      "          Accuracy: [prop score:  0.95791], [q1: 0.75899], [q0: 0.95619],\n",
      "          Effect: [ate-q], [train: 0.18564], [test: 0.18745]\n",
      "********************************************************************************\n",
      "epoch: 468 / 500, time cost: 46.61 sec, \n",
      "          Loss: [Train: 2.12413], [Test: 2.28927],\n",
      "          Accuracy: [prop score:  0.95790], [q1: 0.75174], [q0: 0.95075],\n",
      "          Effect: [ate-q], [train: 0.17733], [test: 0.17920]\n",
      "********************************************************************************\n",
      "epoch: 469 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.12295], [Test: 2.28969],\n",
      "          Accuracy: [prop score:  0.95798], [q1: 0.72921], [q0: 0.95432],\n",
      "          Effect: [ate-q], [train: 0.16903], [test: 0.17091]\n",
      "********************************************************************************\n",
      "epoch: 470 / 500, time cost: 45.70 sec, \n",
      "          Loss: [Train: 2.12311], [Test: 2.28987],\n",
      "          Accuracy: [prop score:  0.95796], [q1: 0.75102], [q0: 0.94953],\n",
      "          Effect: [ate-q], [train: 0.17642], [test: 0.17830]\n",
      "********************************************************************************\n",
      "epoch: 471 / 500, time cost: 48.38 sec, \n",
      "          Loss: [Train: 2.12233], [Test: 2.29036],\n",
      "          Accuracy: [prop score:  0.95804], [q1: 0.75653], [q0: 0.95636],\n",
      "          Effect: [ate-q], [train: 0.18679], [test: 0.18867]\n",
      "********************************************************************************\n",
      "epoch: 472 / 500, time cost: 46.71 sec, \n",
      "          Loss: [Train: 2.12192], [Test: 2.29108],\n",
      "          Accuracy: [prop score:  0.95814], [q1: 0.73549], [q0: 0.94643],\n",
      "          Effect: [ate-q], [train: 0.16757], [test: 0.16948]\n",
      "********************************************************************************\n",
      "epoch: 473 / 500, time cost: 48.70 sec, \n",
      "          Loss: [Train: 2.12139], [Test: 2.29104],\n",
      "          Accuracy: [prop score:  0.95817], [q1: 0.73685], [q0: 0.96028],\n",
      "          Effect: [ate-q], [train: 0.18129], [test: 0.18321]\n",
      "********************************************************************************\n",
      "epoch: 474 / 500, time cost: 46.59 sec, \n",
      "          Loss: [Train: 2.12085], [Test: 2.29080],\n",
      "          Accuracy: [prop score:  0.95818], [q1: 0.74321], [q0: 0.95382],\n",
      "          Effect: [ate-q], [train: 0.17830], [test: 0.18021]\n",
      "********************************************************************************\n",
      "epoch: 475 / 500, time cost: 48.82 sec, \n",
      "          Loss: [Train: 2.12065], [Test: 2.29113],\n",
      "          Accuracy: [prop score:  0.95824], [q1: 0.75931], [q0: 0.94956],\n",
      "          Effect: [ate-q], [train: 0.18360], [test: 0.18545]\n",
      "********************************************************************************\n",
      "epoch: 476 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 2.11939], [Test: 2.29194],\n",
      "          Accuracy: [prop score:  0.95827], [q1: 0.71273], [q0: 0.94773],\n",
      "          Effect: [ate-q], [train: 0.15984], [test: 0.16177]\n",
      "********************************************************************************\n",
      "epoch: 477 / 500, time cost: 48.78 sec, \n",
      "          Loss: [Train: 2.11877], [Test: 2.29290],\n",
      "          Accuracy: [prop score:  0.95817], [q1: 0.78107], [q0: 0.94833],\n",
      "          Effect: [ate-q], [train: 0.19695], [test: 0.19892]\n",
      "********************************************************************************\n",
      "epoch: 478 / 500, time cost: 46.38 sec, \n",
      "          Loss: [Train: 2.11799], [Test: 2.29270],\n",
      "          Accuracy: [prop score:  0.95820], [q1: 0.76024], [q0: 0.93601],\n",
      "          Effect: [ate-q], [train: 0.17614], [test: 0.17807]\n",
      "********************************************************************************\n",
      "epoch: 479 / 500, time cost: 48.62 sec, \n",
      "          Loss: [Train: 2.11821], [Test: 2.29247],\n",
      "          Accuracy: [prop score:  0.95821], [q1: 0.74474], [q0: 0.95442],\n",
      "          Effect: [ate-q], [train: 0.18275], [test: 0.18472]\n",
      "********************************************************************************\n",
      "epoch: 480 / 500, time cost: 45.55 sec, \n",
      "          Loss: [Train: 2.11744], [Test: 2.29263],\n",
      "          Accuracy: [prop score:  0.95832], [q1: 0.74124], [q0: 0.95502],\n",
      "          Effect: [ate-q], [train: 0.18180], [test: 0.18379]\n",
      "********************************************************************************\n",
      "epoch: 481 / 500, time cost: 48.35 sec, \n",
      "          Loss: [Train: 2.11699], [Test: 2.29368],\n",
      "          Accuracy: [prop score:  0.95829], [q1: 0.76852], [q0: 0.92973],\n",
      "          Effect: [ate-q], [train: 0.17839], [test: 0.18034]\n",
      "********************************************************************************\n",
      "epoch: 482 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.11605], [Test: 2.29455],\n",
      "          Accuracy: [prop score:  0.95823], [q1: 0.76990], [q0: 0.92768],\n",
      "          Effect: [ate-q], [train: 0.17866], [test: 0.18063]\n",
      "********************************************************************************\n",
      "epoch: 483 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.11581], [Test: 2.29353],\n",
      "          Accuracy: [prop score:  0.95834], [q1: 0.74476], [q0: 0.93942],\n",
      "          Effect: [ate-q], [train: 0.17194], [test: 0.17388]\n",
      "********************************************************************************\n",
      "epoch: 484 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.11522], [Test: 2.29377],\n",
      "          Accuracy: [prop score:  0.95828], [q1: 0.74772], [q0: 0.94382],\n",
      "          Effect: [ate-q], [train: 0.17770], [test: 0.17965]\n",
      "********************************************************************************\n",
      "epoch: 485 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.11486], [Test: 2.29459],\n",
      "          Accuracy: [prop score:  0.95839], [q1: 0.76226], [q0: 0.94866],\n",
      "          Effect: [ate-q], [train: 0.18973], [test: 0.19168]\n",
      "********************************************************************************\n",
      "epoch: 486 / 500, time cost: 46.41 sec, \n",
      "          Loss: [Train: 2.11384], [Test: 2.29389],\n",
      "          Accuracy: [prop score:  0.95843], [q1: 0.74844], [q0: 0.94147],\n",
      "          Effect: [ate-q], [train: 0.17688], [test: 0.17883]\n",
      "********************************************************************************\n",
      "epoch: 487 / 500, time cost: 48.49 sec, \n",
      "          Loss: [Train: 2.11360], [Test: 2.29545],\n",
      "          Accuracy: [prop score:  0.95844], [q1: 0.75120], [q0: 0.93850],\n",
      "          Effect: [ate-q], [train: 0.17652], [test: 0.17847]\n",
      "********************************************************************************\n",
      "epoch: 488 / 500, time cost: 46.52 sec, \n",
      "          Loss: [Train: 2.11326], [Test: 2.29495],\n",
      "          Accuracy: [prop score:  0.95850], [q1: 0.73146], [q0: 0.94087],\n",
      "          Effect: [ate-q], [train: 0.16964], [test: 0.17166]\n",
      "********************************************************************************\n",
      "epoch: 489 / 500, time cost: 48.97 sec, \n",
      "          Loss: [Train: 2.11260], [Test: 2.29527],\n",
      "          Accuracy: [prop score:  0.95853], [q1: 0.74131], [q0: 0.94378],\n",
      "          Effect: [ate-q], [train: 0.17686], [test: 0.17889]\n",
      "********************************************************************************\n",
      "epoch: 490 / 500, time cost: 46.95 sec, \n",
      "          Loss: [Train: 2.11197], [Test: 2.29577],\n",
      "          Accuracy: [prop score:  0.95847], [q1: 0.72311], [q0: 0.95127],\n",
      "          Effect: [ate-q], [train: 0.17473], [test: 0.17671]\n",
      "********************************************************************************\n",
      "epoch: 491 / 500, time cost: 49.84 sec, \n",
      "          Loss: [Train: 2.11116], [Test: 2.29589],\n",
      "          Accuracy: [prop score:  0.95847], [q1: 0.72864], [q0: 0.94609],\n",
      "          Effect: [ate-q], [train: 0.17255], [test: 0.17453]\n",
      "********************************************************************************\n",
      "epoch: 492 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 2.11109], [Test: 2.29556],\n",
      "          Accuracy: [prop score:  0.95851], [q1: 0.74433], [q0: 0.94066],\n",
      "          Effect: [ate-q], [train: 0.17660], [test: 0.17861]\n",
      "********************************************************************************\n",
      "epoch: 493 / 500, time cost: 48.78 sec, \n",
      "          Loss: [Train: 2.11039], [Test: 2.29653],\n",
      "          Accuracy: [prop score:  0.95849], [q1: 0.74676], [q0: 0.94652],\n",
      "          Effect: [ate-q], [train: 0.18317], [test: 0.18522]\n",
      "********************************************************************************\n",
      "epoch: 494 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.10919], [Test: 2.29655],\n",
      "          Accuracy: [prop score:  0.95854], [q1: 0.74395], [q0: 0.94842],\n",
      "          Effect: [ate-q], [train: 0.18388], [test: 0.18592]\n",
      "********************************************************************************\n",
      "epoch: 495 / 500, time cost: 48.61 sec, \n",
      "          Loss: [Train: 2.10932], [Test: 2.29700],\n",
      "          Accuracy: [prop score:  0.95866], [q1: 0.74089], [q0: 0.93690],\n",
      "          Effect: [ate-q], [train: 0.17379], [test: 0.17581]\n",
      "********************************************************************************\n",
      "epoch: 496 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.10854], [Test: 2.29717],\n",
      "          Accuracy: [prop score:  0.95863], [q1: 0.73702], [q0: 0.93392],\n",
      "          Effect: [ate-q], [train: 0.17013], [test: 0.17220]\n",
      "********************************************************************************\n",
      "epoch: 497 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.10846], [Test: 2.29771],\n",
      "          Accuracy: [prop score:  0.95873], [q1: 0.74193], [q0: 0.93702],\n",
      "          Effect: [ate-q], [train: 0.17504], [test: 0.17711]\n",
      "********************************************************************************\n",
      "epoch: 498 / 500, time cost: 46.74 sec, \n",
      "          Loss: [Train: 2.10763], [Test: 2.29819],\n",
      "          Accuracy: [prop score:  0.95877], [q1: 0.74985], [q0: 0.94480],\n",
      "          Effect: [ate-q], [train: 0.18561], [test: 0.18768]\n",
      "********************************************************************************\n",
      "epoch: 499 / 500, time cost: 47.37 sec, \n",
      "          Loss: [Train: 2.10768], [Test: 2.29832],\n",
      "          Accuracy: [prop score:  0.95871], [q1: 0.72598], [q0: 0.94130],\n",
      "          Effect: [ate-q], [train: 0.17091], [test: 0.17300]\n",
      "********************************************************************************\n",
      "epoch: 500 / 500, time cost: 45.27 sec, \n",
      "          Loss: [Train: 2.10669], [Test: 2.29914],\n",
      "          Accuracy: [prop score:  0.95874], [q1: 0.76176], [q0: 0.93604],\n",
      "          Effect: [ate-q], [train: 0.18670], [test: 0.18880]\n",
      "********************************************************************************\n",
      "Finish training...\n"
     ]
    }
   ],
   "source": [
    "train_loss_hist, test_loss_hist, est_effect, prop_score_hist = [], [], [], []\n",
    "for e in range(1, epoch + 1):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        run_loss += loss.item()\n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    prop_score_hist.append(g_loss_test)\n",
    "    est_effect.append(test_effect)\n",
    "    \n",
    "    print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW in 500 epochs，lr=5e-6, no scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW in 800 epochs，lr=5e-6, no scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.10610], [Test: 2.29910],\n",
      "          Accuracy: [prop score:  0.95876], [q1: 0.74626], [q0: 0.94587],\n",
      "          Effect: [ate-q], [train: 0.18584], [test: 0.18794]\n",
      "********************************************************************************\n",
      "epoch: 501 / 500, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.10535], [Test: 2.29915],\n",
      "          Accuracy: [prop score:  0.95874], [q1: 0.73663], [q0: 0.92729],\n",
      "          Effect: [ate-q], [train: 0.16788], [test: 0.17001]\n",
      "********************************************************************************\n",
      "epoch: 502 / 500, time cost: 45.39 sec, \n",
      "          Loss: [Train: 2.10477], [Test: 2.29986],\n",
      "          Accuracy: [prop score:  0.95883], [q1: 0.73569], [q0: 0.93839],\n",
      "          Effect: [ate-q], [train: 0.17569], [test: 0.17777]\n",
      "********************************************************************************\n",
      "epoch: 503 / 500, time cost: 47.70 sec, \n",
      "          Loss: [Train: 2.10448], [Test: 2.29947],\n",
      "          Accuracy: [prop score:  0.95880], [q1: 0.73112], [q0: 0.92990],\n",
      "          Effect: [ate-q], [train: 0.16809], [test: 0.17019]\n",
      "********************************************************************************\n",
      "epoch: 504 / 500, time cost: 45.62 sec, \n",
      "          Loss: [Train: 2.10371], [Test: 2.30006],\n",
      "          Accuracy: [prop score:  0.95893], [q1: 0.75217], [q0: 0.93891],\n",
      "          Effect: [ate-q], [train: 0.18531], [test: 0.18742]\n",
      "********************************************************************************\n",
      "epoch: 505 / 500, time cost: 47.52 sec, \n",
      "          Loss: [Train: 2.10314], [Test: 2.30094],\n",
      "          Accuracy: [prop score:  0.95884], [q1: 0.73573], [q0: 0.92868],\n",
      "          Effect: [ate-q], [train: 0.17082], [test: 0.17293]\n",
      "********************************************************************************\n",
      "epoch: 506 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.10243], [Test: 2.30073],\n",
      "          Accuracy: [prop score:  0.95878], [q1: 0.74010], [q0: 0.93796],\n",
      "          Effect: [ate-q], [train: 0.17986], [test: 0.18200]\n",
      "********************************************************************************\n",
      "epoch: 507 / 500, time cost: 45.72 sec, \n",
      "          Loss: [Train: 2.10249], [Test: 2.30158],\n",
      "          Accuracy: [prop score:  0.95895], [q1: 0.73874], [q0: 0.93334],\n",
      "          Effect: [ate-q], [train: 0.17607], [test: 0.17823]\n",
      "********************************************************************************\n",
      "epoch: 508 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.10131], [Test: 2.30209],\n",
      "          Accuracy: [prop score:  0.95884], [q1: 0.75304], [q0: 0.94201],\n",
      "          Effect: [ate-q], [train: 0.19174], [test: 0.19389]\n",
      "********************************************************************************\n",
      "epoch: 509 / 500, time cost: 47.01 sec, \n",
      "          Loss: [Train: 2.10144], [Test: 2.30198],\n",
      "          Accuracy: [prop score:  0.95887], [q1: 0.72508], [q0: 0.93489],\n",
      "          Effect: [ate-q], [train: 0.17027], [test: 0.17237]\n",
      "********************************************************************************\n",
      "epoch: 510 / 500, time cost: 48.95 sec, \n",
      "          Loss: [Train: 2.10075], [Test: 2.30239],\n",
      "          Accuracy: [prop score:  0.95880], [q1: 0.73761], [q0: 0.92314],\n",
      "          Effect: [ate-q], [train: 0.16987], [test: 0.17199]\n",
      "********************************************************************************\n",
      "epoch: 511 / 500, time cost: 46.83 sec, \n",
      "          Loss: [Train: 2.09983], [Test: 2.30314],\n",
      "          Accuracy: [prop score:  0.95891], [q1: 0.74902], [q0: 0.92993],\n",
      "          Effect: [ate-q], [train: 0.18150], [test: 0.18368]\n",
      "********************************************************************************\n",
      "epoch: 512 / 500, time cost: 46.88 sec, \n",
      "          Loss: [Train: 2.09885], [Test: 2.30412],\n",
      "          Accuracy: [prop score:  0.95895], [q1: 0.76617], [q0: 0.93942],\n",
      "          Effect: [ate-q], [train: 0.19759], [test: 0.19979]\n",
      "********************************************************************************\n",
      "epoch: 513 / 500, time cost: 48.78 sec, \n",
      "          Loss: [Train: 2.09889], [Test: 2.30329],\n",
      "          Accuracy: [prop score:  0.95892], [q1: 0.73484], [q0: 0.92902],\n",
      "          Effect: [ate-q], [train: 0.17363], [test: 0.17578]\n",
      "********************************************************************************\n",
      "epoch: 514 / 500, time cost: 46.99 sec, \n",
      "          Loss: [Train: 2.09897], [Test: 2.30440],\n",
      "          Accuracy: [prop score:  0.95903], [q1: 0.73999], [q0: 0.92555],\n",
      "          Effect: [ate-q], [train: 0.17531], [test: 0.17747]\n",
      "********************************************************************************\n",
      "epoch: 515 / 500, time cost: 48.96 sec, \n",
      "          Loss: [Train: 2.09829], [Test: 2.30423],\n",
      "          Accuracy: [prop score:  0.95904], [q1: 0.71800], [q0: 0.92802],\n",
      "          Effect: [ate-q], [train: 0.16568], [test: 0.16790]\n",
      "********************************************************************************\n",
      "epoch: 516 / 500, time cost: 47.25 sec, \n",
      "          Loss: [Train: 2.09749], [Test: 2.30448],\n",
      "          Accuracy: [prop score:  0.95912], [q1: 0.71896], [q0: 0.92687],\n",
      "          Effect: [ate-q], [train: 0.16625], [test: 0.16840]\n",
      "********************************************************************************\n",
      "epoch: 517 / 500, time cost: 46.64 sec, \n",
      "          Loss: [Train: 2.09662], [Test: 2.30457],\n",
      "          Accuracy: [prop score:  0.95906], [q1: 0.73843], [q0: 0.92639],\n",
      "          Effect: [ate-q], [train: 0.17508], [test: 0.17726]\n",
      "********************************************************************************\n",
      "epoch: 518 / 500, time cost: 48.46 sec, \n",
      "          Loss: [Train: 2.09634], [Test: 2.30494],\n",
      "          Accuracy: [prop score:  0.95920], [q1: 0.73232], [q0: 0.93741],\n",
      "          Effect: [ate-q], [train: 0.18081], [test: 0.18297]\n",
      "********************************************************************************\n",
      "epoch: 519 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 2.09571], [Test: 2.30586],\n",
      "          Accuracy: [prop score:  0.95912], [q1: 0.73352], [q0: 0.92886],\n",
      "          Effect: [ate-q], [train: 0.17523], [test: 0.17742]\n",
      "********************************************************************************\n",
      "epoch: 520 / 500, time cost: 48.75 sec, \n",
      "          Loss: [Train: 2.09518], [Test: 2.30590],\n",
      "          Accuracy: [prop score:  0.95917], [q1: 0.73910], [q0: 0.93032],\n",
      "          Effect: [ate-q], [train: 0.17965], [test: 0.18189]\n",
      "********************************************************************************\n",
      "epoch: 521 / 500, time cost: 46.62 sec, \n",
      "          Loss: [Train: 2.09441], [Test: 2.30619],\n",
      "          Accuracy: [prop score:  0.95923], [q1: 0.73855], [q0: 0.93086],\n",
      "          Effect: [ate-q], [train: 0.18155], [test: 0.18380]\n",
      "********************************************************************************\n",
      "epoch: 522 / 500, time cost: 46.52 sec, \n",
      "          Loss: [Train: 2.09389], [Test: 2.30589],\n",
      "          Accuracy: [prop score:  0.95922], [q1: 0.71174], [q0: 0.93297],\n",
      "          Effect: [ate-q], [train: 0.16949], [test: 0.17169]\n",
      "********************************************************************************\n",
      "epoch: 523 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.09327], [Test: 2.30739],\n",
      "          Accuracy: [prop score:  0.95917], [q1: 0.74778], [q0: 0.91940],\n",
      "          Effect: [ate-q], [train: 0.17887], [test: 0.18105]\n",
      "********************************************************************************\n",
      "epoch: 524 / 500, time cost: 46.85 sec, \n",
      "          Loss: [Train: 2.09262], [Test: 2.30711],\n",
      "          Accuracy: [prop score:  0.95931], [q1: 0.72596], [q0: 0.91893],\n",
      "          Effect: [ate-q], [train: 0.16677], [test: 0.16901]\n",
      "********************************************************************************\n",
      "epoch: 525 / 500, time cost: 49.16 sec, \n",
      "          Loss: [Train: 2.09233], [Test: 2.30864],\n",
      "          Accuracy: [prop score:  0.95932], [q1: 0.75013], [q0: 0.91088],\n",
      "          Effect: [ate-q], [train: 0.17635], [test: 0.17860]\n",
      "********************************************************************************\n",
      "epoch: 526 / 500, time cost: 46.61 sec, \n",
      "          Loss: [Train: 2.09173], [Test: 2.30883],\n",
      "          Accuracy: [prop score:  0.95945], [q1: 0.71642], [q0: 0.91264],\n",
      "          Effect: [ate-q], [train: 0.15975], [test: 0.16199]\n",
      "********************************************************************************\n",
      "epoch: 527 / 500, time cost: 46.79 sec, \n",
      "          Loss: [Train: 2.09170], [Test: 2.30911],\n",
      "          Accuracy: [prop score:  0.95943], [q1: 0.73990], [q0: 0.91883],\n",
      "          Effect: [ate-q], [train: 0.17484], [test: 0.17705]\n",
      "********************************************************************************\n",
      "epoch: 528 / 500, time cost: 48.91 sec, \n",
      "          Loss: [Train: 2.09135], [Test: 2.30909],\n",
      "          Accuracy: [prop score:  0.95941], [q1: 0.72215], [q0: 0.92649],\n",
      "          Effect: [ate-q], [train: 0.17264], [test: 0.17487]\n",
      "********************************************************************************\n",
      "epoch: 529 / 500, time cost: 46.85 sec, \n",
      "          Loss: [Train: 2.09006], [Test: 2.30910],\n",
      "          Accuracy: [prop score:  0.95936], [q1: 0.72521], [q0: 0.93273],\n",
      "          Effect: [ate-q], [train: 0.17929], [test: 0.18160]\n",
      "********************************************************************************\n",
      "epoch: 530 / 500, time cost: 48.90 sec, \n",
      "          Loss: [Train: 2.09012], [Test: 2.30976],\n",
      "          Accuracy: [prop score:  0.95945], [q1: 0.73396], [q0: 0.91592],\n",
      "          Effect: [ate-q], [train: 0.17235], [test: 0.17457]\n",
      "********************************************************************************\n",
      "epoch: 531 / 500, time cost: 46.83 sec, \n",
      "          Loss: [Train: 2.08891], [Test: 2.31065],\n",
      "          Accuracy: [prop score:  0.95957], [q1: 0.73774], [q0: 0.92249],\n",
      "          Effect: [ate-q], [train: 0.17825], [test: 0.18050]\n",
      "********************************************************************************\n",
      "epoch: 532 / 500, time cost: 46.82 sec, \n",
      "          Loss: [Train: 2.08841], [Test: 2.31016],\n",
      "          Accuracy: [prop score:  0.95959], [q1: 0.72133], [q0: 0.92473],\n",
      "          Effect: [ate-q], [train: 0.17258], [test: 0.17480]\n",
      "********************************************************************************\n",
      "epoch: 533 / 500, time cost: 48.92 sec, \n",
      "          Loss: [Train: 2.08837], [Test: 2.31051],\n",
      "          Accuracy: [prop score:  0.95966], [q1: 0.74006], [q0: 0.92632],\n",
      "          Effect: [ate-q], [train: 0.18331], [test: 0.18560]\n",
      "********************************************************************************\n",
      "epoch: 534 / 500, time cost: 46.57 sec, \n",
      "          Loss: [Train: 2.08803], [Test: 2.31085],\n",
      "          Accuracy: [prop score:  0.95956], [q1: 0.72170], [q0: 0.92012],\n",
      "          Effect: [ate-q], [train: 0.17000], [test: 0.17228]\n",
      "********************************************************************************\n",
      "epoch: 535 / 500, time cost: 48.68 sec, \n",
      "          Loss: [Train: 2.08704], [Test: 2.31085],\n",
      "          Accuracy: [prop score:  0.95966], [q1: 0.72516], [q0: 0.92100],\n",
      "          Effect: [ate-q], [train: 0.17241], [test: 0.17470]\n",
      "********************************************************************************\n",
      "epoch: 536 / 500, time cost: 46.54 sec, \n",
      "          Loss: [Train: 2.08635], [Test: 2.31188],\n",
      "          Accuracy: [prop score:  0.95962], [q1: 0.72236], [q0: 0.91802],\n",
      "          Effect: [ate-q], [train: 0.16967], [test: 0.17197]\n",
      "********************************************************************************\n",
      "epoch: 537 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.08622], [Test: 2.31232],\n",
      "          Accuracy: [prop score:  0.95962], [q1: 0.70982], [q0: 0.90546],\n",
      "          Effect: [ate-q], [train: 0.15758], [test: 0.15987]\n",
      "********************************************************************************\n",
      "epoch: 538 / 500, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.08587], [Test: 2.31284],\n",
      "          Accuracy: [prop score:  0.95969], [q1: 0.75097], [q0: 0.91343],\n",
      "          Effect: [ate-q], [train: 0.18290], [test: 0.18519]\n",
      "********************************************************************************\n",
      "epoch: 539 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.08522], [Test: 2.31319],\n",
      "          Accuracy: [prop score:  0.95964], [q1: 0.72075], [q0: 0.91130],\n",
      "          Effect: [ate-q], [train: 0.16678], [test: 0.16911]\n",
      "********************************************************************************\n",
      "epoch: 540 / 500, time cost: 48.61 sec, \n",
      "          Loss: [Train: 2.08480], [Test: 2.31436],\n",
      "          Accuracy: [prop score:  0.95969], [q1: 0.73622], [q0: 0.92772],\n",
      "          Effect: [ate-q], [train: 0.18538], [test: 0.18765]\n",
      "********************************************************************************\n",
      "epoch: 541 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 2.08407], [Test: 2.31335],\n",
      "          Accuracy: [prop score:  0.95979], [q1: 0.71666], [q0: 0.91832],\n",
      "          Effect: [ate-q], [train: 0.16875], [test: 0.17111]\n",
      "********************************************************************************\n",
      "epoch: 542 / 500, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.08348], [Test: 2.31405],\n",
      "          Accuracy: [prop score:  0.95976], [q1: 0.72841], [q0: 0.90922],\n",
      "          Effect: [ate-q], [train: 0.17021], [test: 0.17254]\n",
      "********************************************************************************\n",
      "epoch: 543 / 500, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.08271], [Test: 2.31472],\n",
      "          Accuracy: [prop score:  0.95986], [q1: 0.71446], [q0: 0.91331],\n",
      "          Effect: [ate-q], [train: 0.16628], [test: 0.16861]\n",
      "********************************************************************************\n",
      "epoch: 544 / 500, time cost: 46.51 sec, \n",
      "          Loss: [Train: 2.08285], [Test: 2.31578],\n",
      "          Accuracy: [prop score:  0.95988], [q1: 0.73591], [q0: 0.91588],\n",
      "          Effect: [ate-q], [train: 0.17840], [test: 0.18070]\n",
      "********************************************************************************\n",
      "epoch: 545 / 500, time cost: 48.69 sec, \n",
      "          Loss: [Train: 2.08230], [Test: 2.31516],\n",
      "          Accuracy: [prop score:  0.95978], [q1: 0.72043], [q0: 0.92243],\n",
      "          Effect: [ate-q], [train: 0.17593], [test: 0.17832]\n",
      "********************************************************************************\n",
      "epoch: 546 / 500, time cost: 45.68 sec, \n",
      "          Loss: [Train: 2.08123], [Test: 2.31637],\n",
      "          Accuracy: [prop score:  0.95990], [q1: 0.73507], [q0: 0.92254],\n",
      "          Effect: [ate-q], [train: 0.18371], [test: 0.18603]\n",
      "********************************************************************************\n",
      "epoch: 547 / 500, time cost: 46.50 sec, \n",
      "          Loss: [Train: 2.08103], [Test: 2.31681],\n",
      "          Accuracy: [prop score:  0.95980], [q1: 0.72716], [q0: 0.90028],\n",
      "          Effect: [ate-q], [train: 0.16581], [test: 0.16820]\n",
      "********************************************************************************\n",
      "epoch: 548 / 500, time cost: 47.97 sec, \n",
      "          Loss: [Train: 2.08034], [Test: 2.31699],\n",
      "          Accuracy: [prop score:  0.95985], [q1: 0.71268], [q0: 0.90854],\n",
      "          Effect: [ate-q], [train: 0.16459], [test: 0.16694]\n",
      "********************************************************************************\n",
      "epoch: 549 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.07982], [Test: 2.31647],\n",
      "          Accuracy: [prop score:  0.95994], [q1: 0.70378], [q0: 0.92133],\n",
      "          Effect: [ate-q], [train: 0.16818], [test: 0.17055]\n",
      "********************************************************************************\n",
      "epoch: 550 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.07972], [Test: 2.31764],\n",
      "          Accuracy: [prop score:  0.95995], [q1: 0.72112], [q0: 0.90666],\n",
      "          Effect: [ate-q], [train: 0.16776], [test: 0.17005]\n",
      "********************************************************************************\n",
      "epoch: 551 / 500, time cost: 46.41 sec, \n",
      "          Loss: [Train: 2.07886], [Test: 2.31849],\n",
      "          Accuracy: [prop score:  0.96001], [q1: 0.73238], [q0: 0.92176],\n",
      "          Effect: [ate-q], [train: 0.18305], [test: 0.18546]\n",
      "********************************************************************************\n",
      "epoch: 552 / 500, time cost: 46.40 sec, \n",
      "          Loss: [Train: 2.07849], [Test: 2.31842],\n",
      "          Accuracy: [prop score:  0.95996], [q1: 0.70799], [q0: 0.92073],\n",
      "          Effect: [ate-q], [train: 0.17159], [test: 0.17402]\n",
      "********************************************************************************\n",
      "epoch: 553 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.07816], [Test: 2.31876],\n",
      "          Accuracy: [prop score:  0.96000], [q1: 0.72243], [q0: 0.91668],\n",
      "          Effect: [ate-q], [train: 0.17523], [test: 0.17759]\n",
      "********************************************************************************\n",
      "epoch: 554 / 500, time cost: 46.49 sec, \n",
      "          Loss: [Train: 2.07742], [Test: 2.31970],\n",
      "          Accuracy: [prop score:  0.96010], [q1: 0.72992], [q0: 0.89829],\n",
      "          Effect: [ate-q], [train: 0.16835], [test: 0.17074]\n",
      "********************************************************************************\n",
      "epoch: 555 / 500, time cost: 48.64 sec, \n",
      "          Loss: [Train: 2.07631], [Test: 2.32000],\n",
      "          Accuracy: [prop score:  0.96014], [q1: 0.70042], [q0: 0.91178],\n",
      "          Effect: [ate-q], [train: 0.16256], [test: 0.16495]\n",
      "********************************************************************************\n",
      "epoch: 556 / 500, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.07641], [Test: 2.32075],\n",
      "          Accuracy: [prop score:  0.96008], [q1: 0.69828], [q0: 0.90192],\n",
      "          Effect: [ate-q], [train: 0.15569], [test: 0.15812]\n",
      "********************************************************************************\n",
      "epoch: 557 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.07625], [Test: 2.32061],\n",
      "          Accuracy: [prop score:  0.96019], [q1: 0.71223], [q0: 0.91836],\n",
      "          Effect: [ate-q], [train: 0.17346], [test: 0.17584]\n",
      "********************************************************************************\n",
      "epoch: 558 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.07498], [Test: 2.32041],\n",
      "          Accuracy: [prop score:  0.96011], [q1: 0.70906], [q0: 0.89894],\n",
      "          Effect: [ate-q], [train: 0.16010], [test: 0.16246]\n",
      "********************************************************************************\n",
      "epoch: 559 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.07472], [Test: 2.32126],\n",
      "          Accuracy: [prop score:  0.96014], [q1: 0.71738], [q0: 0.90928],\n",
      "          Effect: [ate-q], [train: 0.16977], [test: 0.17215]\n",
      "********************************************************************************\n",
      "epoch: 560 / 500, time cost: 47.78 sec, \n",
      "          Loss: [Train: 2.07409], [Test: 2.32216],\n",
      "          Accuracy: [prop score:  0.96009], [q1: 0.70111], [q0: 0.92769],\n",
      "          Effect: [ate-q], [train: 0.17615], [test: 0.17859]\n",
      "********************************************************************************\n",
      "epoch: 561 / 500, time cost: 45.42 sec, \n",
      "          Loss: [Train: 2.07373], [Test: 2.32270],\n",
      "          Accuracy: [prop score:  0.96009], [q1: 0.71625], [q0: 0.89123],\n",
      "          Effect: [ate-q], [train: 0.16067], [test: 0.16309]\n",
      "********************************************************************************\n",
      "epoch: 562 / 500, time cost: 45.26 sec, \n",
      "          Loss: [Train: 2.07329], [Test: 2.32253],\n",
      "          Accuracy: [prop score:  0.96019], [q1: 0.73030], [q0: 0.90666],\n",
      "          Effect: [ate-q], [train: 0.17587], [test: 0.17823]\n",
      "********************************************************************************\n",
      "epoch: 563 / 500, time cost: 47.65 sec, \n",
      "          Loss: [Train: 2.07276], [Test: 2.32279],\n",
      "          Accuracy: [prop score:  0.96019], [q1: 0.72185], [q0: 0.90138],\n",
      "          Effect: [ate-q], [train: 0.16974], [test: 0.17215]\n",
      "********************************************************************************\n",
      "epoch: 564 / 500, time cost: 45.84 sec, \n",
      "          Loss: [Train: 2.07166], [Test: 2.32404],\n",
      "          Accuracy: [prop score:  0.96025], [q1: 0.71902], [q0: 0.91483],\n",
      "          Effect: [ate-q], [train: 0.17750], [test: 0.17996]\n",
      "********************************************************************************\n",
      "epoch: 565 / 500, time cost: 47.40 sec, \n",
      "          Loss: [Train: 2.07184], [Test: 2.32374],\n",
      "          Accuracy: [prop score:  0.96025], [q1: 0.72338], [q0: 0.89420],\n",
      "          Effect: [ate-q], [train: 0.16622], [test: 0.16862]\n",
      "********************************************************************************\n",
      "epoch: 566 / 500, time cost: 45.04 sec, \n",
      "          Loss: [Train: 2.07149], [Test: 2.32449],\n",
      "          Accuracy: [prop score:  0.96021], [q1: 0.73678], [q0: 0.90381],\n",
      "          Effect: [ate-q], [train: 0.17894], [test: 0.18132]\n",
      "********************************************************************************\n",
      "epoch: 567 / 500, time cost: 45.44 sec, \n",
      "          Loss: [Train: 2.07080], [Test: 2.32566],\n",
      "          Accuracy: [prop score:  0.96022], [q1: 0.74131], [q0: 0.90012],\n",
      "          Effect: [ate-q], [train: 0.18021], [test: 0.18262]\n",
      "********************************************************************************\n",
      "epoch: 568 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.07005], [Test: 2.32569],\n",
      "          Accuracy: [prop score:  0.96032], [q1: 0.73288], [q0: 0.91137],\n",
      "          Effect: [ate-q], [train: 0.18206], [test: 0.18455]\n",
      "********************************************************************************\n",
      "epoch: 569 / 500, time cost: 46.06 sec, \n",
      "          Loss: [Train: 2.06978], [Test: 2.32578],\n",
      "          Accuracy: [prop score:  0.96028], [q1: 0.73245], [q0: 0.90673],\n",
      "          Effect: [ate-q], [train: 0.17978], [test: 0.18222]\n",
      "********************************************************************************\n",
      "epoch: 570 / 500, time cost: 52.60 sec, \n",
      "          Loss: [Train: 2.06868], [Test: 2.32608],\n",
      "          Accuracy: [prop score:  0.96035], [q1: 0.72650], [q0: 0.90913],\n",
      "          Effect: [ate-q], [train: 0.17886], [test: 0.18129]\n",
      "********************************************************************************\n",
      "epoch: 571 / 500, time cost: 55.52 sec, \n",
      "          Loss: [Train: 2.06894], [Test: 2.32718],\n",
      "          Accuracy: [prop score:  0.96032], [q1: 0.73423], [q0: 0.91115],\n",
      "          Effect: [ate-q], [train: 0.18470], [test: 0.18715]\n",
      "********************************************************************************\n",
      "epoch: 572 / 500, time cost: 52.92 sec, \n",
      "          Loss: [Train: 2.06819], [Test: 2.32749],\n",
      "          Accuracy: [prop score:  0.96038], [q1: 0.72435], [q0: 0.91137],\n",
      "          Effect: [ate-q], [train: 0.18066], [test: 0.18308]\n",
      "********************************************************************************\n",
      "epoch: 573 / 500, time cost: 53.13 sec, \n",
      "          Loss: [Train: 2.06827], [Test: 2.32761],\n",
      "          Accuracy: [prop score:  0.96042], [q1: 0.70545], [q0: 0.91273],\n",
      "          Effect: [ate-q], [train: 0.17055], [test: 0.17301]\n",
      "********************************************************************************\n",
      "epoch: 574 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.06763], [Test: 2.32887],\n",
      "          Accuracy: [prop score:  0.96044], [q1: 0.68594], [q0: 0.90606],\n",
      "          Effect: [ate-q], [train: 0.15923], [test: 0.16167]\n",
      "********************************************************************************\n",
      "epoch: 575 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.06704], [Test: 2.32793],\n",
      "          Accuracy: [prop score:  0.96044], [q1: 0.71575], [q0: 0.90375],\n",
      "          Effect: [ate-q], [train: 0.17140], [test: 0.17379]\n",
      "********************************************************************************\n",
      "epoch: 576 / 500, time cost: 45.46 sec, \n",
      "          Loss: [Train: 2.06647], [Test: 2.32921],\n",
      "          Accuracy: [prop score:  0.96053], [q1: 0.73711], [q0: 0.90585],\n",
      "          Effect: [ate-q], [train: 0.18389], [test: 0.18637]\n",
      "********************************************************************************\n",
      "epoch: 577 / 500, time cost: 45.44 sec, \n",
      "          Loss: [Train: 2.06583], [Test: 2.32973],\n",
      "          Accuracy: [prop score:  0.96054], [q1: 0.73760], [q0: 0.89880],\n",
      "          Effect: [ate-q], [train: 0.18030], [test: 0.18278]\n",
      "********************************************************************************\n",
      "epoch: 578 / 500, time cost: 47.65 sec, \n",
      "          Loss: [Train: 2.06518], [Test: 2.33006],\n",
      "          Accuracy: [prop score:  0.96060], [q1: 0.72880], [q0: 0.89221],\n",
      "          Effect: [ate-q], [train: 0.17274], [test: 0.17522]\n",
      "********************************************************************************\n",
      "epoch: 579 / 500, time cost: 45.28 sec, \n",
      "          Loss: [Train: 2.06465], [Test: 2.33057],\n",
      "          Accuracy: [prop score:  0.96063], [q1: 0.72101], [q0: 0.89749],\n",
      "          Effect: [ate-q], [train: 0.17301], [test: 0.17552]\n",
      "********************************************************************************\n",
      "epoch: 580 / 500, time cost: 46.46 sec, \n",
      "          Loss: [Train: 2.06430], [Test: 2.33108],\n",
      "          Accuracy: [prop score:  0.96065], [q1: 0.72482], [q0: 0.90360],\n",
      "          Effect: [ate-q], [train: 0.17712], [test: 0.17963]\n",
      "********************************************************************************\n",
      "epoch: 581 / 500, time cost: 44.38 sec, \n",
      "          Loss: [Train: 2.06390], [Test: 2.33158],\n",
      "          Accuracy: [prop score:  0.96066], [q1: 0.67866], [q0: 0.90265],\n",
      "          Effect: [ate-q], [train: 0.15598], [test: 0.15845]\n",
      "********************************************************************************\n",
      "epoch: 582 / 500, time cost: 44.42 sec, \n",
      "          Loss: [Train: 2.06298], [Test: 2.33221],\n",
      "          Accuracy: [prop score:  0.96072], [q1: 0.71440], [q0: 0.88529],\n",
      "          Effect: [ate-q], [train: 0.16278], [test: 0.16522]\n",
      "********************************************************************************\n",
      "epoch: 583 / 500, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.06326], [Test: 2.33231],\n",
      "          Accuracy: [prop score:  0.96064], [q1: 0.71044], [q0: 0.89993],\n",
      "          Effect: [ate-q], [train: 0.16825], [test: 0.17084]\n",
      "********************************************************************************\n",
      "epoch: 584 / 500, time cost: 44.28 sec, \n",
      "          Loss: [Train: 2.06229], [Test: 2.33317],\n",
      "          Accuracy: [prop score:  0.96070], [q1: 0.73521], [q0: 0.89740],\n",
      "          Effect: [ate-q], [train: 0.17944], [test: 0.18188]\n",
      "********************************************************************************\n",
      "epoch: 585 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 2.06212], [Test: 2.33468],\n",
      "          Accuracy: [prop score:  0.96072], [q1: 0.72890], [q0: 0.89704],\n",
      "          Effect: [ate-q], [train: 0.17725], [test: 0.17970]\n",
      "********************************************************************************\n",
      "epoch: 586 / 500, time cost: 44.18 sec, \n",
      "          Loss: [Train: 2.06170], [Test: 2.33422],\n",
      "          Accuracy: [prop score:  0.96072], [q1: 0.69347], [q0: 0.89024],\n",
      "          Effect: [ate-q], [train: 0.15683], [test: 0.15937]\n",
      "********************************************************************************\n",
      "epoch: 587 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 2.06103], [Test: 2.33547],\n",
      "          Accuracy: [prop score:  0.96066], [q1: 0.72049], [q0: 0.88972],\n",
      "          Effect: [ate-q], [train: 0.16999], [test: 0.17253]\n",
      "********************************************************************************\n",
      "epoch: 588 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.06064], [Test: 2.33620],\n",
      "          Accuracy: [prop score:  0.96076], [q1: 0.73425], [q0: 0.88662],\n",
      "          Effect: [ate-q], [train: 0.17532], [test: 0.17787]\n",
      "********************************************************************************\n",
      "epoch: 589 / 500, time cost: 45.35 sec, \n",
      "          Loss: [Train: 2.05996], [Test: 2.33760],\n",
      "          Accuracy: [prop score:  0.96079], [q1: 0.71207], [q0: 0.89152],\n",
      "          Effect: [ate-q], [train: 0.16607], [test: 0.16862]\n",
      "********************************************************************************\n",
      "epoch: 590 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.05946], [Test: 2.33724],\n",
      "          Accuracy: [prop score:  0.96090], [q1: 0.70432], [q0: 0.87480],\n",
      "          Effect: [ate-q], [train: 0.15477], [test: 0.15726]\n",
      "********************************************************************************\n",
      "epoch: 591 / 500, time cost: 45.43 sec, \n",
      "          Loss: [Train: 2.05938], [Test: 2.33789],\n",
      "          Accuracy: [prop score:  0.96080], [q1: 0.70112], [q0: 0.88845],\n",
      "          Effect: [ate-q], [train: 0.16041], [test: 0.16288]\n",
      "********************************************************************************\n",
      "epoch: 592 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.05851], [Test: 2.33736],\n",
      "          Accuracy: [prop score:  0.96091], [q1: 0.70768], [q0: 0.90040],\n",
      "          Effect: [ate-q], [train: 0.17046], [test: 0.17301]\n",
      "********************************************************************************\n",
      "epoch: 593 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.05846], [Test: 2.33766],\n",
      "          Accuracy: [prop score:  0.96084], [q1: 0.70576], [q0: 0.88069],\n",
      "          Effect: [ate-q], [train: 0.15902], [test: 0.16156]\n",
      "********************************************************************************\n",
      "epoch: 594 / 500, time cost: 45.52 sec, \n",
      "          Loss: [Train: 2.05755], [Test: 2.33901],\n",
      "          Accuracy: [prop score:  0.96081], [q1: 0.70875], [q0: 0.88613],\n",
      "          Effect: [ate-q], [train: 0.16409], [test: 0.16662]\n",
      "********************************************************************************\n",
      "epoch: 595 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.05717], [Test: 2.34031],\n",
      "          Accuracy: [prop score:  0.96092], [q1: 0.72983], [q0: 0.87244],\n",
      "          Effect: [ate-q], [train: 0.16708], [test: 0.16964]\n",
      "********************************************************************************\n",
      "epoch: 596 / 500, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.05664], [Test: 2.34046],\n",
      "          Accuracy: [prop score:  0.96103], [q1: 0.70669], [q0: 0.89347],\n",
      "          Effect: [ate-q], [train: 0.16670], [test: 0.16921]\n",
      "********************************************************************************\n",
      "epoch: 597 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.05607], [Test: 2.34154],\n",
      "          Accuracy: [prop score:  0.96097], [q1: 0.71512], [q0: 0.86949],\n",
      "          Effect: [ate-q], [train: 0.15933], [test: 0.16180]\n",
      "********************************************************************************\n",
      "epoch: 598 / 500, time cost: 47.90 sec, \n",
      "          Loss: [Train: 2.05577], [Test: 2.34140],\n",
      "          Accuracy: [prop score:  0.96100], [q1: 0.70283], [q0: 0.88999],\n",
      "          Effect: [ate-q], [train: 0.16511], [test: 0.16768]\n",
      "********************************************************************************\n",
      "epoch: 599 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.05513], [Test: 2.34218],\n",
      "          Accuracy: [prop score:  0.96097], [q1: 0.69810], [q0: 0.89466],\n",
      "          Effect: [ate-q], [train: 0.16535], [test: 0.16796]\n",
      "********************************************************************************\n",
      "epoch: 600 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.05511], [Test: 2.34298],\n",
      "          Accuracy: [prop score:  0.96097], [q1: 0.72021], [q0: 0.88920],\n",
      "          Effect: [ate-q], [train: 0.17310], [test: 0.17558]\n",
      "********************************************************************************\n",
      "epoch: 601 / 500, time cost: 45.54 sec, \n",
      "          Loss: [Train: 2.05488], [Test: 2.34356],\n",
      "          Accuracy: [prop score:  0.96098], [q1: 0.71447], [q0: 0.86786],\n",
      "          Effect: [ate-q], [train: 0.15836], [test: 0.16094]\n",
      "********************************************************************************\n",
      "epoch: 602 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.05355], [Test: 2.34260],\n",
      "          Accuracy: [prop score:  0.96107], [q1: 0.71083], [q0: 0.88667],\n",
      "          Effect: [ate-q], [train: 0.16555], [test: 0.16813]\n",
      "********************************************************************************\n",
      "epoch: 603 / 500, time cost: 47.89 sec, \n",
      "          Loss: [Train: 2.05381], [Test: 2.34465],\n",
      "          Accuracy: [prop score:  0.96107], [q1: 0.73044], [q0: 0.89739],\n",
      "          Effect: [ate-q], [train: 0.18384], [test: 0.18637]\n",
      "********************************************************************************\n",
      "epoch: 604 / 500, time cost: 46.69 sec, \n",
      "          Loss: [Train: 2.05303], [Test: 2.34420],\n",
      "          Accuracy: [prop score:  0.96118], [q1: 0.71087], [q0: 0.87519],\n",
      "          Effect: [ate-q], [train: 0.16065], [test: 0.16318]\n",
      "********************************************************************************\n",
      "epoch: 605 / 500, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.05288], [Test: 2.34484],\n",
      "          Accuracy: [prop score:  0.96118], [q1: 0.72061], [q0: 0.88258],\n",
      "          Effect: [ate-q], [train: 0.17059], [test: 0.17313]\n",
      "********************************************************************************\n",
      "epoch: 606 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.05208], [Test: 2.34537],\n",
      "          Accuracy: [prop score:  0.96116], [q1: 0.70977], [q0: 0.88620],\n",
      "          Effect: [ate-q], [train: 0.16895], [test: 0.17149]\n",
      "********************************************************************************\n",
      "epoch: 607 / 500, time cost: 45.63 sec, \n",
      "          Loss: [Train: 2.05199], [Test: 2.34549],\n",
      "          Accuracy: [prop score:  0.96120], [q1: 0.71073], [q0: 0.89316],\n",
      "          Effect: [ate-q], [train: 0.17353], [test: 0.17607]\n",
      "********************************************************************************\n",
      "epoch: 608 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.05137], [Test: 2.34644],\n",
      "          Accuracy: [prop score:  0.96126], [q1: 0.72142], [q0: 0.89299],\n",
      "          Effect: [ate-q], [train: 0.17818], [test: 0.18069]\n",
      "********************************************************************************\n",
      "epoch: 609 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.05087], [Test: 2.34662],\n",
      "          Accuracy: [prop score:  0.96128], [q1: 0.70753], [q0: 0.87947],\n",
      "          Effect: [ate-q], [train: 0.16395], [test: 0.16651]\n",
      "********************************************************************************\n",
      "epoch: 610 / 500, time cost: 48.33 sec, \n",
      "          Loss: [Train: 2.05035], [Test: 2.34715],\n",
      "          Accuracy: [prop score:  0.96129], [q1: 0.73790], [q0: 0.88042],\n",
      "          Effect: [ate-q], [train: 0.17974], [test: 0.18231]\n",
      "********************************************************************************\n",
      "epoch: 611 / 500, time cost: 46.23 sec, \n",
      "          Loss: [Train: 2.05005], [Test: 2.34832],\n",
      "          Accuracy: [prop score:  0.96133], [q1: 0.73984], [q0: 0.89497],\n",
      "          Effect: [ate-q], [train: 0.19116], [test: 0.19373]\n",
      "********************************************************************************\n",
      "epoch: 612 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.04942], [Test: 2.34840],\n",
      "          Accuracy: [prop score:  0.96129], [q1: 0.68004], [q0: 0.88658],\n",
      "          Effect: [ate-q], [train: 0.15674], [test: 0.15939]\n",
      "********************************************************************************\n",
      "epoch: 613 / 500, time cost: 48.44 sec, \n",
      "          Loss: [Train: 2.04907], [Test: 2.34885],\n",
      "          Accuracy: [prop score:  0.96124], [q1: 0.68731], [q0: 0.89092],\n",
      "          Effect: [ate-q], [train: 0.16255], [test: 0.16517]\n",
      "********************************************************************************\n",
      "epoch: 614 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.04842], [Test: 2.34912],\n",
      "          Accuracy: [prop score:  0.96127], [q1: 0.70755], [q0: 0.88568],\n",
      "          Effect: [ate-q], [train: 0.16802], [test: 0.17058]\n",
      "********************************************************************************\n",
      "epoch: 615 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.04835], [Test: 2.35001],\n",
      "          Accuracy: [prop score:  0.96144], [q1: 0.73129], [q0: 0.87775],\n",
      "          Effect: [ate-q], [train: 0.17692], [test: 0.17953]\n",
      "********************************************************************************\n",
      "epoch: 616 / 500, time cost: 46.46 sec, \n",
      "          Loss: [Train: 2.04788], [Test: 2.34997],\n",
      "          Accuracy: [prop score:  0.96143], [q1: 0.71514], [q0: 0.89068],\n",
      "          Effect: [ate-q], [train: 0.17534], [test: 0.17796]\n",
      "********************************************************************************\n",
      "epoch: 617 / 500, time cost: 46.58 sec, \n",
      "          Loss: [Train: 2.04714], [Test: 2.35128],\n",
      "          Accuracy: [prop score:  0.96140], [q1: 0.70602], [q0: 0.87572],\n",
      "          Effect: [ate-q], [train: 0.16259], [test: 0.16520]\n",
      "********************************************************************************\n",
      "epoch: 618 / 500, time cost: 47.53 sec, \n",
      "          Loss: [Train: 2.04683], [Test: 2.35176],\n",
      "          Accuracy: [prop score:  0.96144], [q1: 0.71041], [q0: 0.88179],\n",
      "          Effect: [ate-q], [train: 0.16808], [test: 0.17074]\n",
      "********************************************************************************\n",
      "epoch: 619 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.04635], [Test: 2.35170],\n",
      "          Accuracy: [prop score:  0.96139], [q1: 0.71303], [q0: 0.87821],\n",
      "          Effect: [ate-q], [train: 0.16917], [test: 0.17188]\n",
      "********************************************************************************\n",
      "epoch: 620 / 500, time cost: 49.30 sec, \n",
      "          Loss: [Train: 2.04606], [Test: 2.35283],\n",
      "          Accuracy: [prop score:  0.96143], [q1: 0.67605], [q0: 0.87491],\n",
      "          Effect: [ate-q], [train: 0.14846], [test: 0.15103]\n",
      "********************************************************************************\n",
      "epoch: 621 / 500, time cost: 44.12 sec, \n",
      "          Loss: [Train: 2.04602], [Test: 2.35343],\n",
      "          Accuracy: [prop score:  0.96148], [q1: 0.69142], [q0: 0.88022],\n",
      "          Effect: [ate-q], [train: 0.15916], [test: 0.16178]\n",
      "********************************************************************************\n",
      "epoch: 622 / 500, time cost: 45.31 sec, \n",
      "          Loss: [Train: 2.04576], [Test: 2.35368],\n",
      "          Accuracy: [prop score:  0.96150], [q1: 0.72167], [q0: 0.86654],\n",
      "          Effect: [ate-q], [train: 0.16633], [test: 0.16890]\n",
      "********************************************************************************\n",
      "epoch: 623 / 500, time cost: 57.98 sec, \n",
      "          Loss: [Train: 2.04508], [Test: 2.35406],\n",
      "          Accuracy: [prop score:  0.96149], [q1: 0.68950], [q0: 0.87997],\n",
      "          Effect: [ate-q], [train: 0.15845], [test: 0.16108]\n",
      "********************************************************************************\n",
      "epoch: 624 / 500, time cost: 57.51 sec, \n",
      "          Loss: [Train: 2.04464], [Test: 2.35470],\n",
      "          Accuracy: [prop score:  0.96157], [q1: 0.71344], [q0: 0.87253],\n",
      "          Effect: [ate-q], [train: 0.16600], [test: 0.16862]\n",
      "********************************************************************************\n",
      "epoch: 625 / 500, time cost: 49.95 sec, \n",
      "          Loss: [Train: 2.04395], [Test: 2.35230],\n",
      "          Accuracy: [prop score:  0.96158], [q1: 0.71936], [q0: 0.87344],\n",
      "          Effect: [ate-q], [train: 0.17041], [test: 0.17296]\n",
      "********************************************************************************\n",
      "epoch: 626 / 500, time cost: 55.72 sec, \n",
      "          Loss: [Train: 2.04304], [Test: 2.35564],\n",
      "          Accuracy: [prop score:  0.96161], [q1: 0.70795], [q0: 0.85642],\n",
      "          Effect: [ate-q], [train: 0.15547], [test: 0.15811]\n",
      "********************************************************************************\n",
      "epoch: 627 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.04226], [Test: 2.35518],\n",
      "          Accuracy: [prop score:  0.96169], [q1: 0.72075], [q0: 0.87869],\n",
      "          Effect: [ate-q], [train: 0.17338], [test: 0.17602]\n",
      "********************************************************************************\n",
      "epoch: 628 / 500, time cost: 49.93 sec, \n",
      "          Loss: [Train: 2.04259], [Test: 2.35659],\n",
      "          Accuracy: [prop score:  0.96166], [q1: 0.71181], [q0: 0.90171],\n",
      "          Effect: [ate-q], [train: 0.18315], [test: 0.18586]\n",
      "********************************************************************************\n",
      "epoch: 629 / 500, time cost: 47.85 sec, \n",
      "          Loss: [Train: 2.04208], [Test: 2.35639],\n",
      "          Accuracy: [prop score:  0.96169], [q1: 0.72019], [q0: 0.88615],\n",
      "          Effect: [ate-q], [train: 0.17859], [test: 0.18119]\n",
      "********************************************************************************\n",
      "epoch: 630 / 500, time cost: 49.15 sec, \n",
      "          Loss: [Train: 2.04141], [Test: 2.35796],\n",
      "          Accuracy: [prop score:  0.96175], [q1: 0.71986], [q0: 0.86368],\n",
      "          Effect: [ate-q], [train: 0.16707], [test: 0.16971]\n",
      "********************************************************************************\n",
      "epoch: 631 / 500, time cost: 46.91 sec, \n",
      "          Loss: [Train: 2.04103], [Test: 2.35794],\n",
      "          Accuracy: [prop score:  0.96178], [q1: 0.70478], [q0: 0.89112],\n",
      "          Effect: [ate-q], [train: 0.17284], [test: 0.17554]\n",
      "********************************************************************************\n",
      "epoch: 632 / 500, time cost: 46.97 sec, \n",
      "          Loss: [Train: 2.04028], [Test: 2.35820],\n",
      "          Accuracy: [prop score:  0.96178], [q1: 0.73014], [q0: 0.88112],\n",
      "          Effect: [ate-q], [train: 0.18163], [test: 0.18429]\n",
      "********************************************************************************\n",
      "epoch: 633 / 500, time cost: 50.23 sec, \n",
      "          Loss: [Train: 2.04058], [Test: 2.35906],\n",
      "          Accuracy: [prop score:  0.96185], [q1: 0.71045], [q0: 0.85406],\n",
      "          Effect: [ate-q], [train: 0.15725], [test: 0.15986]\n",
      "********************************************************************************\n",
      "epoch: 634 / 500, time cost: 47.10 sec, \n",
      "          Loss: [Train: 2.04016], [Test: 2.35979],\n",
      "          Accuracy: [prop score:  0.96180], [q1: 0.68950], [q0: 0.87642],\n",
      "          Effect: [ate-q], [train: 0.15903], [test: 0.16169]\n",
      "********************************************************************************\n",
      "epoch: 635 / 500, time cost: 50.22 sec, \n",
      "          Loss: [Train: 2.03932], [Test: 2.35963],\n",
      "          Accuracy: [prop score:  0.96187], [q1: 0.71348], [q0: 0.88725],\n",
      "          Effect: [ate-q], [train: 0.17690], [test: 0.17955]\n",
      "********************************************************************************\n",
      "epoch: 636 / 500, time cost: 48.02 sec, \n",
      "          Loss: [Train: 2.03898], [Test: 2.36112],\n",
      "          Accuracy: [prop score:  0.96196], [q1: 0.67597], [q0: 0.88368],\n",
      "          Effect: [ate-q], [train: 0.15607], [test: 0.15876]\n",
      "********************************************************************************\n",
      "epoch: 637 / 500, time cost: 49.78 sec, \n",
      "          Loss: [Train: 2.03859], [Test: 2.36078],\n",
      "          Accuracy: [prop score:  0.96191], [q1: 0.71644], [q0: 0.87279],\n",
      "          Effect: [ate-q], [train: 0.17020], [test: 0.17284]\n",
      "********************************************************************************\n",
      "epoch: 638 / 500, time cost: 50.58 sec, \n",
      "          Loss: [Train: 2.03794], [Test: 2.36182],\n",
      "          Accuracy: [prop score:  0.96186], [q1: 0.71683], [q0: 0.87839],\n",
      "          Effect: [ate-q], [train: 0.17700], [test: 0.17967]\n",
      "********************************************************************************\n",
      "epoch: 639 / 500, time cost: 47.53 sec, \n",
      "          Loss: [Train: 2.03738], [Test: 2.36260],\n",
      "          Accuracy: [prop score:  0.96193], [q1: 0.71196], [q0: 0.85011],\n",
      "          Effect: [ate-q], [train: 0.15879], [test: 0.16143]\n",
      "********************************************************************************\n",
      "epoch: 640 / 500, time cost: 49.63 sec, \n",
      "          Loss: [Train: 2.03752], [Test: 2.36297],\n",
      "          Accuracy: [prop score:  0.96207], [q1: 0.66393], [q0: 0.88383],\n",
      "          Effect: [ate-q], [train: 0.15327], [test: 0.15601]\n",
      "********************************************************************************\n",
      "epoch: 641 / 500, time cost: 46.91 sec, \n",
      "          Loss: [Train: 2.03649], [Test: 2.36301],\n",
      "          Accuracy: [prop score:  0.96202], [q1: 0.69216], [q0: 0.87933],\n",
      "          Effect: [ate-q], [train: 0.16327], [test: 0.16595]\n",
      "********************************************************************************\n",
      "epoch: 642 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.03628], [Test: 2.36287],\n",
      "          Accuracy: [prop score:  0.96203], [q1: 0.70037], [q0: 0.87538],\n",
      "          Effect: [ate-q], [train: 0.16475], [test: 0.16742]\n",
      "********************************************************************************\n",
      "epoch: 643 / 500, time cost: 48.93 sec, \n",
      "          Loss: [Train: 2.03585], [Test: 2.36363],\n",
      "          Accuracy: [prop score:  0.96203], [q1: 0.71050], [q0: 0.86037],\n",
      "          Effect: [ate-q], [train: 0.16321], [test: 0.16579]\n",
      "********************************************************************************\n",
      "epoch: 644 / 500, time cost: 54.97 sec, \n",
      "          Loss: [Train: 2.03577], [Test: 2.36476],\n",
      "          Accuracy: [prop score:  0.96191], [q1: 0.71574], [q0: 0.87974],\n",
      "          Effect: [ate-q], [train: 0.17753], [test: 0.18024]\n",
      "********************************************************************************\n",
      "epoch: 645 / 500, time cost: 53.52 sec, \n",
      "          Loss: [Train: 2.03517], [Test: 2.36542],\n",
      "          Accuracy: [prop score:  0.96206], [q1: 0.72573], [q0: 0.88712],\n",
      "          Effect: [ate-q], [train: 0.18720], [test: 0.18991]\n",
      "********************************************************************************\n",
      "epoch: 646 / 500, time cost: 55.56 sec, \n",
      "          Loss: [Train: 2.03453], [Test: 2.36590],\n",
      "          Accuracy: [prop score:  0.96211], [q1: 0.70062], [q0: 0.89998],\n",
      "          Effect: [ate-q], [train: 0.18463], [test: 0.18736]\n",
      "********************************************************************************\n",
      "epoch: 647 / 500, time cost: 47.07 sec, \n",
      "          Loss: [Train: 2.03453], [Test: 2.36640],\n",
      "          Accuracy: [prop score:  0.96210], [q1: 0.72814], [q0: 0.87880],\n",
      "          Effect: [ate-q], [train: 0.18405], [test: 0.18669]\n",
      "********************************************************************************\n",
      "epoch: 648 / 500, time cost: 48.84 sec, \n",
      "          Loss: [Train: 2.03397], [Test: 2.36650],\n",
      "          Accuracy: [prop score:  0.96208], [q1: 0.70406], [q0: 0.86333],\n",
      "          Effect: [ate-q], [train: 0.16045], [test: 0.16312]\n",
      "********************************************************************************\n",
      "epoch: 649 / 500, time cost: 47.08 sec, \n",
      "          Loss: [Train: 2.03301], [Test: 2.36743],\n",
      "          Accuracy: [prop score:  0.96203], [q1: 0.69980], [q0: 0.86713],\n",
      "          Effect: [ate-q], [train: 0.16379], [test: 0.16656]\n",
      "********************************************************************************\n",
      "epoch: 650 / 500, time cost: 49.66 sec, \n",
      "          Loss: [Train: 2.03302], [Test: 2.36870],\n",
      "          Accuracy: [prop score:  0.96221], [q1: 0.66937], [q0: 0.86843],\n",
      "          Effect: [ate-q], [train: 0.14803], [test: 0.15070]\n",
      "********************************************************************************\n",
      "epoch: 651 / 500, time cost: 47.23 sec, \n",
      "          Loss: [Train: 2.03255], [Test: 2.36897],\n",
      "          Accuracy: [prop score:  0.96202], [q1: 0.69492], [q0: 0.87757],\n",
      "          Effect: [ate-q], [train: 0.16560], [test: 0.16841]\n",
      "********************************************************************************\n",
      "epoch: 652 / 500, time cost: 47.28 sec, \n",
      "          Loss: [Train: 2.03234], [Test: 2.36923],\n",
      "          Accuracy: [prop score:  0.96200], [q1: 0.70875], [q0: 0.88912],\n",
      "          Effect: [ate-q], [train: 0.18067], [test: 0.18343]\n",
      "********************************************************************************\n",
      "epoch: 653 / 500, time cost: 48.97 sec, \n",
      "          Loss: [Train: 2.03202], [Test: 2.36983],\n",
      "          Accuracy: [prop score:  0.96216], [q1: 0.70837], [q0: 0.85977],\n",
      "          Effect: [ate-q], [train: 0.16136], [test: 0.16402]\n",
      "********************************************************************************\n",
      "epoch: 654 / 500, time cost: 47.64 sec, \n",
      "          Loss: [Train: 2.03117], [Test: 2.37053],\n",
      "          Accuracy: [prop score:  0.96207], [q1: 0.71071], [q0: 0.86268],\n",
      "          Effect: [ate-q], [train: 0.16519], [test: 0.16789]\n",
      "********************************************************************************\n",
      "epoch: 655 / 500, time cost: 50.20 sec, \n",
      "          Loss: [Train: 2.03121], [Test: 2.37028],\n",
      "          Accuracy: [prop score:  0.96208], [q1: 0.69569], [q0: 0.87315],\n",
      "          Effect: [ate-q], [train: 0.16331], [test: 0.16597]\n",
      "********************************************************************************\n",
      "epoch: 656 / 500, time cost: 47.12 sec, \n",
      "          Loss: [Train: 2.03055], [Test: 2.37165],\n",
      "          Accuracy: [prop score:  0.96211], [q1: 0.70578], [q0: 0.86530],\n",
      "          Effect: [ate-q], [train: 0.16476], [test: 0.16745]\n",
      "********************************************************************************\n",
      "epoch: 657 / 500, time cost: 47.25 sec, \n",
      "          Loss: [Train: 2.02986], [Test: 2.37237],\n",
      "          Accuracy: [prop score:  0.96221], [q1: 0.68458], [q0: 0.87121],\n",
      "          Effect: [ate-q], [train: 0.15760], [test: 0.16027]\n",
      "********************************************************************************\n",
      "epoch: 658 / 500, time cost: 48.91 sec, \n",
      "          Loss: [Train: 2.02943], [Test: 2.37458],\n",
      "          Accuracy: [prop score:  0.96211], [q1: 0.72073], [q0: 0.86909],\n",
      "          Effect: [ate-q], [train: 0.17623], [test: 0.17891]\n",
      "********************************************************************************\n",
      "epoch: 659 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 2.02923], [Test: 2.37498],\n",
      "          Accuracy: [prop score:  0.96211], [q1: 0.70204], [q0: 0.86016],\n",
      "          Effect: [ate-q], [train: 0.16037], [test: 0.16313]\n",
      "********************************************************************************\n",
      "epoch: 660 / 500, time cost: 48.67 sec, \n",
      "          Loss: [Train: 2.02852], [Test: 2.37476],\n",
      "          Accuracy: [prop score:  0.96214], [q1: 0.70871], [q0: 0.85792],\n",
      "          Effect: [ate-q], [train: 0.16535], [test: 0.16806]\n",
      "********************************************************************************\n",
      "epoch: 661 / 500, time cost: 47.17 sec, \n",
      "          Loss: [Train: 2.02888], [Test: 2.37452],\n",
      "          Accuracy: [prop score:  0.96210], [q1: 0.69212], [q0: 0.87181],\n",
      "          Effect: [ate-q], [train: 0.16324], [test: 0.16601]\n",
      "********************************************************************************\n",
      "epoch: 662 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.02850], [Test: 2.37601],\n",
      "          Accuracy: [prop score:  0.96219], [q1: 0.70187], [q0: 0.87216],\n",
      "          Effect: [ate-q], [train: 0.16877], [test: 0.17150]\n",
      "********************************************************************************\n",
      "epoch: 663 / 500, time cost: 49.30 sec, \n",
      "          Loss: [Train: 2.02734], [Test: 2.37708],\n",
      "          Accuracy: [prop score:  0.96215], [q1: 0.73831], [q0: 0.87338],\n",
      "          Effect: [ate-q], [train: 0.18905], [test: 0.19175]\n",
      "********************************************************************************\n",
      "epoch: 664 / 500, time cost: 46.75 sec, \n",
      "          Loss: [Train: 2.02781], [Test: 2.37644],\n",
      "          Accuracy: [prop score:  0.96217], [q1: 0.69182], [q0: 0.87256],\n",
      "          Effect: [ate-q], [train: 0.16320], [test: 0.16592]\n",
      "********************************************************************************\n",
      "epoch: 665 / 500, time cost: 49.60 sec, \n",
      "          Loss: [Train: 2.02745], [Test: 2.37744],\n",
      "          Accuracy: [prop score:  0.96217], [q1: 0.68774], [q0: 0.87666],\n",
      "          Effect: [ate-q], [train: 0.16380], [test: 0.16657]\n",
      "********************************************************************************\n",
      "epoch: 666 / 500, time cost: 46.83 sec, \n",
      "          Loss: [Train: 2.02608], [Test: 2.37745],\n",
      "          Accuracy: [prop score:  0.96218], [q1: 0.69032], [q0: 0.86899],\n",
      "          Effect: [ate-q], [train: 0.16099], [test: 0.16366]\n",
      "********************************************************************************\n",
      "epoch: 667 / 500, time cost: 46.79 sec, \n",
      "          Loss: [Train: 2.02611], [Test: 2.37923],\n",
      "          Accuracy: [prop score:  0.96227], [q1: 0.69160], [q0: 0.87554],\n",
      "          Effect: [ate-q], [train: 0.16539], [test: 0.16820]\n",
      "********************************************************************************\n",
      "epoch: 668 / 500, time cost: 49.10 sec, \n",
      "          Loss: [Train: 2.02579], [Test: 2.37906],\n",
      "          Accuracy: [prop score:  0.96226], [q1: 0.71515], [q0: 0.86600],\n",
      "          Effect: [ate-q], [train: 0.17337], [test: 0.17606]\n",
      "********************************************************************************\n",
      "epoch: 669 / 500, time cost: 46.88 sec, \n",
      "          Loss: [Train: 2.02569], [Test: 2.38058],\n",
      "          Accuracy: [prop score:  0.96232], [q1: 0.72535], [q0: 0.85078],\n",
      "          Effect: [ate-q], [train: 0.17086], [test: 0.17363]\n",
      "********************************************************************************\n",
      "epoch: 670 / 500, time cost: 49.03 sec, \n",
      "          Loss: [Train: 2.02506], [Test: 2.38120],\n",
      "          Accuracy: [prop score:  0.96231], [q1: 0.71902], [q0: 0.85173],\n",
      "          Effect: [ate-q], [train: 0.16950], [test: 0.17224]\n",
      "********************************************************************************\n",
      "epoch: 671 / 500, time cost: 46.56 sec, \n",
      "          Loss: [Train: 2.02486], [Test: 2.38179],\n",
      "          Accuracy: [prop score:  0.96240], [q1: 0.67715], [q0: 0.84961],\n",
      "          Effect: [ate-q], [train: 0.14614], [test: 0.14891]\n",
      "********************************************************************************\n",
      "epoch: 672 / 500, time cost: 47.30 sec, \n",
      "          Loss: [Train: 2.02404], [Test: 2.38143],\n",
      "          Accuracy: [prop score:  0.96231], [q1: 0.69859], [q0: 0.86643],\n",
      "          Effect: [ate-q], [train: 0.16576], [test: 0.16859]\n",
      "********************************************************************************\n",
      "epoch: 673 / 500, time cost: 49.77 sec, \n",
      "          Loss: [Train: 2.02408], [Test: 2.38124],\n",
      "          Accuracy: [prop score:  0.96234], [q1: 0.70065], [q0: 0.86551],\n",
      "          Effect: [ate-q], [train: 0.16663], [test: 0.16941]\n",
      "********************************************************************************\n",
      "epoch: 674 / 500, time cost: 47.21 sec, \n",
      "          Loss: [Train: 2.02384], [Test: 2.38328],\n",
      "          Accuracy: [prop score:  0.96232], [q1: 0.67757], [q0: 0.88626],\n",
      "          Effect: [ate-q], [train: 0.16886], [test: 0.17164]\n",
      "********************************************************************************\n",
      "epoch: 675 / 500, time cost: 49.49 sec, \n",
      "          Loss: [Train: 2.02257], [Test: 2.38221],\n",
      "          Accuracy: [prop score:  0.96239], [q1: 0.70505], [q0: 0.84895],\n",
      "          Effect: [ate-q], [train: 0.16029], [test: 0.16298]\n",
      "********************************************************************************\n",
      "epoch: 676 / 500, time cost: 47.39 sec, \n",
      "          Loss: [Train: 2.02311], [Test: 2.38324],\n",
      "          Accuracy: [prop score:  0.96240], [q1: 0.70487], [q0: 0.85587],\n",
      "          Effect: [ate-q], [train: 0.16354], [test: 0.16626]\n",
      "********************************************************************************\n",
      "epoch: 677 / 500, time cost: 47.70 sec, \n",
      "          Loss: [Train: 2.02220], [Test: 2.38277],\n",
      "          Accuracy: [prop score:  0.96251], [q1: 0.68404], [q0: 0.85720],\n",
      "          Effect: [ate-q], [train: 0.15327], [test: 0.15610]\n",
      "********************************************************************************\n",
      "epoch: 678 / 500, time cost: 48.92 sec, \n",
      "          Loss: [Train: 2.02175], [Test: 2.38418],\n",
      "          Accuracy: [prop score:  0.96250], [q1: 0.69749], [q0: 0.86967],\n",
      "          Effect: [ate-q], [train: 0.16810], [test: 0.17079]\n",
      "********************************************************************************\n",
      "epoch: 679 / 500, time cost: 47.32 sec, \n",
      "          Loss: [Train: 2.02170], [Test: 2.38572],\n",
      "          Accuracy: [prop score:  0.96251], [q1: 0.68472], [q0: 0.85708],\n",
      "          Effect: [ate-q], [train: 0.15373], [test: 0.15649]\n",
      "********************************************************************************\n",
      "epoch: 680 / 500, time cost: 49.01 sec, \n",
      "          Loss: [Train: 2.02133], [Test: 2.38693],\n",
      "          Accuracy: [prop score:  0.96249], [q1: 0.72013], [q0: 0.84710],\n",
      "          Effect: [ate-q], [train: 0.16936], [test: 0.17204]\n",
      "********************************************************************************\n",
      "epoch: 681 / 500, time cost: 47.99 sec, \n",
      "          Loss: [Train: 2.02068], [Test: 2.38610],\n",
      "          Accuracy: [prop score:  0.96252], [q1: 0.68869], [q0: 0.85744],\n",
      "          Effect: [ate-q], [train: 0.15642], [test: 0.15913]\n",
      "********************************************************************************\n",
      "epoch: 682 / 500, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.02051], [Test: 2.38856],\n",
      "          Accuracy: [prop score:  0.96252], [q1: 0.70229], [q0: 0.85412],\n",
      "          Effect: [ate-q], [train: 0.16266], [test: 0.16542]\n",
      "********************************************************************************\n",
      "epoch: 683 / 500, time cost: 49.20 sec, \n",
      "          Loss: [Train: 2.02022], [Test: 2.38926],\n",
      "          Accuracy: [prop score:  0.96252], [q1: 0.70967], [q0: 0.87000],\n",
      "          Effect: [ate-q], [train: 0.17332], [test: 0.17608]\n",
      "********************************************************************************\n",
      "epoch: 684 / 500, time cost: 47.59 sec, \n",
      "          Loss: [Train: 2.01986], [Test: 2.38983],\n",
      "          Accuracy: [prop score:  0.96261], [q1: 0.70150], [q0: 0.85914],\n",
      "          Effect: [ate-q], [train: 0.16484], [test: 0.16760]\n",
      "********************************************************************************\n",
      "epoch: 685 / 500, time cost: 49.48 sec, \n",
      "          Loss: [Train: 2.01940], [Test: 2.39054],\n",
      "          Accuracy: [prop score:  0.96258], [q1: 0.70225], [q0: 0.83841],\n",
      "          Effect: [ate-q], [train: 0.15449], [test: 0.15725]\n",
      "********************************************************************************\n",
      "epoch: 686 / 500, time cost: 47.45 sec, \n",
      "          Loss: [Train: 2.01867], [Test: 2.38994],\n",
      "          Accuracy: [prop score:  0.96253], [q1: 0.70061], [q0: 0.87372],\n",
      "          Effect: [ate-q], [train: 0.17481], [test: 0.17767]\n",
      "********************************************************************************\n",
      "epoch: 687 / 500, time cost: 47.32 sec, \n",
      "          Loss: [Train: 2.01885], [Test: 2.39044],\n",
      "          Accuracy: [prop score:  0.96263], [q1: 0.68416], [q0: 0.86220],\n",
      "          Effect: [ate-q], [train: 0.15906], [test: 0.16179]\n",
      "********************************************************************************\n",
      "epoch: 688 / 500, time cost: 49.70 sec, \n",
      "          Loss: [Train: 2.01819], [Test: 2.39159],\n",
      "          Accuracy: [prop score:  0.96262], [q1: 0.72805], [q0: 0.86560],\n",
      "          Effect: [ate-q], [train: 0.18235], [test: 0.18515]\n",
      "********************************************************************************\n",
      "epoch: 689 / 500, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.01797], [Test: 2.39238],\n",
      "          Accuracy: [prop score:  0.96271], [q1: 0.71643], [q0: 0.85856],\n",
      "          Effect: [ate-q], [train: 0.17272], [test: 0.17552]\n",
      "********************************************************************************\n",
      "epoch: 690 / 500, time cost: 49.75 sec, \n",
      "          Loss: [Train: 2.01742], [Test: 2.39183],\n",
      "          Accuracy: [prop score:  0.96272], [q1: 0.70877], [q0: 0.86166],\n",
      "          Effect: [ate-q], [train: 0.17042], [test: 0.17327]\n",
      "********************************************************************************\n",
      "epoch: 691 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.01700], [Test: 2.39304],\n",
      "          Accuracy: [prop score:  0.96285], [q1: 0.70647], [q0: 0.84465],\n",
      "          Effect: [ate-q], [train: 0.16107], [test: 0.16385]\n",
      "********************************************************************************\n",
      "epoch: 692 / 500, time cost: 47.30 sec, \n",
      "          Loss: [Train: 2.01676], [Test: 2.39421],\n",
      "          Accuracy: [prop score:  0.96281], [q1: 0.71702], [q0: 0.85658],\n",
      "          Effect: [ate-q], [train: 0.17245], [test: 0.17526]\n",
      "********************************************************************************\n",
      "epoch: 693 / 500, time cost: 49.05 sec, \n",
      "          Loss: [Train: 2.01575], [Test: 2.39509],\n",
      "          Accuracy: [prop score:  0.96280], [q1: 0.69051], [q0: 0.85426],\n",
      "          Effect: [ate-q], [train: 0.15956], [test: 0.16238]\n",
      "********************************************************************************\n",
      "epoch: 694 / 500, time cost: 47.07 sec, \n",
      "          Loss: [Train: 2.01594], [Test: 2.39400],\n",
      "          Accuracy: [prop score:  0.96272], [q1: 0.67067], [q0: 0.85559],\n",
      "          Effect: [ate-q], [train: 0.14998], [test: 0.15277]\n",
      "********************************************************************************\n",
      "epoch: 695 / 500, time cost: 48.77 sec, \n",
      "          Loss: [Train: 2.01512], [Test: 2.39640],\n",
      "          Accuracy: [prop score:  0.96289], [q1: 0.72912], [q0: 0.84977],\n",
      "          Effect: [ate-q], [train: 0.17684], [test: 0.17958]\n",
      "********************************************************************************\n",
      "epoch: 696 / 500, time cost: 45.57 sec, \n",
      "          Loss: [Train: 2.01505], [Test: 2.39587],\n",
      "          Accuracy: [prop score:  0.96273], [q1: 0.70297], [q0: 0.87529],\n",
      "          Effect: [ate-q], [train: 0.17603], [test: 0.17885]\n",
      "********************************************************************************\n",
      "epoch: 697 / 500, time cost: 45.54 sec, \n",
      "          Loss: [Train: 2.01499], [Test: 2.39358],\n",
      "          Accuracy: [prop score:  0.96273], [q1: 0.70180], [q0: 0.84989],\n",
      "          Effect: [ate-q], [train: 0.16179], [test: 0.16459]\n",
      "********************************************************************************\n",
      "epoch: 698 / 500, time cost: 48.14 sec, \n",
      "          Loss: [Train: 2.01446], [Test: 2.39736],\n",
      "          Accuracy: [prop score:  0.96280], [q1: 0.70043], [q0: 0.83416],\n",
      "          Effect: [ate-q], [train: 0.15431], [test: 0.15703]\n",
      "********************************************************************************\n",
      "epoch: 699 / 500, time cost: 45.68 sec, \n",
      "          Loss: [Train: 2.01397], [Test: 2.39769],\n",
      "          Accuracy: [prop score:  0.96291], [q1: 0.67293], [q0: 0.86586],\n",
      "          Effect: [ate-q], [train: 0.15772], [test: 0.16051]\n",
      "********************************************************************************\n",
      "epoch: 700 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.01395], [Test: 2.39808],\n",
      "          Accuracy: [prop score:  0.96292], [q1: 0.67760], [q0: 0.86544],\n",
      "          Effect: [ate-q], [train: 0.15963], [test: 0.16250]\n",
      "********************************************************************************\n",
      "epoch: 701 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.01362], [Test: 2.40127],\n",
      "          Accuracy: [prop score:  0.96290], [q1: 0.71870], [q0: 0.87966],\n",
      "          Effect: [ate-q], [train: 0.19160], [test: 0.19439]\n",
      "********************************************************************************\n",
      "epoch: 702 / 500, time cost: 45.87 sec, \n",
      "          Loss: [Train: 2.01319], [Test: 2.39982],\n",
      "          Accuracy: [prop score:  0.96286], [q1: 0.71124], [q0: 0.85965],\n",
      "          Effect: [ate-q], [train: 0.17337], [test: 0.17618]\n",
      "********************************************************************************\n",
      "epoch: 703 / 500, time cost: 48.25 sec, \n",
      "          Loss: [Train: 2.01269], [Test: 2.40278],\n",
      "          Accuracy: [prop score:  0.96289], [q1: 0.71700], [q0: 0.82670],\n",
      "          Effect: [ate-q], [train: 0.15953], [test: 0.16228]\n",
      "********************************************************************************\n",
      "epoch: 704 / 500, time cost: 46.15 sec, \n",
      "          Loss: [Train: 2.01236], [Test: 2.40198],\n",
      "          Accuracy: [prop score:  0.96290], [q1: 0.69167], [q0: 0.85629],\n",
      "          Effect: [ate-q], [train: 0.16199], [test: 0.16482]\n",
      "********************************************************************************\n",
      "epoch: 705 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.01194], [Test: 2.40529],\n",
      "          Accuracy: [prop score:  0.96295], [q1: 0.72312], [q0: 0.87127],\n",
      "          Effect: [ate-q], [train: 0.18667], [test: 0.18952]\n",
      "********************************************************************************\n",
      "epoch: 706 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.01131], [Test: 2.40339],\n",
      "          Accuracy: [prop score:  0.96301], [q1: 0.67872], [q0: 0.85569],\n",
      "          Effect: [ate-q], [train: 0.15478], [test: 0.15760]\n",
      "********************************************************************************\n",
      "epoch: 707 / 500, time cost: 45.75 sec, \n",
      "          Loss: [Train: 2.01126], [Test: 2.40408],\n",
      "          Accuracy: [prop score:  0.96295], [q1: 0.70415], [q0: 0.85682],\n",
      "          Effect: [ate-q], [train: 0.16922], [test: 0.17205]\n",
      "********************************************************************************\n",
      "epoch: 708 / 500, time cost: 48.03 sec, \n",
      "          Loss: [Train: 2.01066], [Test: 2.40662],\n",
      "          Accuracy: [prop score:  0.96294], [q1: 0.66965], [q0: 0.84116],\n",
      "          Effect: [ate-q], [train: 0.14540], [test: 0.14821]\n",
      "********************************************************************************\n",
      "epoch: 709 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.01036], [Test: 2.40753],\n",
      "          Accuracy: [prop score:  0.96298], [q1: 0.72440], [q0: 0.86784],\n",
      "          Effect: [ate-q], [train: 0.18614], [test: 0.18897]\n",
      "********************************************************************************\n",
      "epoch: 710 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.00995], [Test: 2.40739],\n",
      "          Accuracy: [prop score:  0.96298], [q1: 0.72105], [q0: 0.86093],\n",
      "          Effect: [ate-q], [train: 0.17997], [test: 0.18273]\n",
      "********************************************************************************\n",
      "epoch: 711 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.00954], [Test: 2.40809],\n",
      "          Accuracy: [prop score:  0.96304], [q1: 0.70298], [q0: 0.84203],\n",
      "          Effect: [ate-q], [train: 0.16089], [test: 0.16375]\n",
      "********************************************************************************\n",
      "epoch: 712 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.00928], [Test: 2.40752],\n",
      "          Accuracy: [prop score:  0.96303], [q1: 0.69739], [q0: 0.85306],\n",
      "          Effect: [ate-q], [train: 0.16294], [test: 0.16574]\n",
      "********************************************************************************\n",
      "epoch: 713 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.00905], [Test: 2.40880],\n",
      "          Accuracy: [prop score:  0.96320], [q1: 0.70802], [q0: 0.86558],\n",
      "          Effect: [ate-q], [train: 0.17606], [test: 0.17890]\n",
      "********************************************************************************\n",
      "epoch: 714 / 500, time cost: 45.99 sec, \n",
      "          Loss: [Train: 2.00856], [Test: 2.40912],\n",
      "          Accuracy: [prop score:  0.96318], [q1: 0.67852], [q0: 0.84924],\n",
      "          Effect: [ate-q], [train: 0.15302], [test: 0.15579]\n",
      "********************************************************************************\n",
      "epoch: 715 / 500, time cost: 48.12 sec, \n",
      "          Loss: [Train: 2.00805], [Test: 2.40890],\n",
      "          Accuracy: [prop score:  0.96325], [q1: 0.67794], [q0: 0.84894],\n",
      "          Effect: [ate-q], [train: 0.15122], [test: 0.15404]\n",
      "********************************************************************************\n",
      "epoch: 716 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 2.00757], [Test: 2.41000],\n",
      "          Accuracy: [prop score:  0.96318], [q1: 0.70266], [q0: 0.83884],\n",
      "          Effect: [ate-q], [train: 0.15901], [test: 0.16181]\n",
      "********************************************************************************\n",
      "epoch: 717 / 500, time cost: 46.37 sec, \n",
      "          Loss: [Train: 2.00750], [Test: 2.41170],\n",
      "          Accuracy: [prop score:  0.96315], [q1: 0.70040], [q0: 0.85323],\n",
      "          Effect: [ate-q], [train: 0.16531], [test: 0.16820]\n",
      "********************************************************************************\n",
      "epoch: 718 / 500, time cost: 48.54 sec, \n",
      "          Loss: [Train: 2.00726], [Test: 2.41193],\n",
      "          Accuracy: [prop score:  0.96321], [q1: 0.68221], [q0: 0.83399],\n",
      "          Effect: [ate-q], [train: 0.14705], [test: 0.14987]\n",
      "********************************************************************************\n",
      "epoch: 719 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 2.00630], [Test: 2.41211],\n",
      "          Accuracy: [prop score:  0.96320], [q1: 0.69527], [q0: 0.82446],\n",
      "          Effect: [ate-q], [train: 0.15009], [test: 0.15283]\n",
      "********************************************************************************\n",
      "epoch: 720 / 500, time cost: 48.66 sec, \n",
      "          Loss: [Train: 2.00691], [Test: 2.41232],\n",
      "          Accuracy: [prop score:  0.96323], [q1: 0.71041], [q0: 0.84308],\n",
      "          Effect: [ate-q], [train: 0.16629], [test: 0.16913]\n",
      "********************************************************************************\n",
      "epoch: 721 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.00616], [Test: 2.41315],\n",
      "          Accuracy: [prop score:  0.96331], [q1: 0.70779], [q0: 0.86247],\n",
      "          Effect: [ate-q], [train: 0.17591], [test: 0.17874]\n",
      "********************************************************************************\n",
      "epoch: 722 / 500, time cost: 46.37 sec, \n",
      "          Loss: [Train: 2.00522], [Test: 2.41275],\n",
      "          Accuracy: [prop score:  0.96333], [q1: 0.71405], [q0: 0.84019],\n",
      "          Effect: [ate-q], [train: 0.16549], [test: 0.16841]\n",
      "********************************************************************************\n",
      "epoch: 723 / 500, time cost: 48.35 sec, \n",
      "          Loss: [Train: 2.00531], [Test: 2.41395],\n",
      "          Accuracy: [prop score:  0.96331], [q1: 0.70363], [q0: 0.84499],\n",
      "          Effect: [ate-q], [train: 0.16558], [test: 0.16846]\n",
      "********************************************************************************\n",
      "epoch: 724 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.00532], [Test: 2.41536],\n",
      "          Accuracy: [prop score:  0.96333], [q1: 0.70168], [q0: 0.81655],\n",
      "          Effect: [ate-q], [train: 0.14990], [test: 0.15277]\n",
      "********************************************************************************\n",
      "epoch: 725 / 500, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.00451], [Test: 2.41522],\n",
      "          Accuracy: [prop score:  0.96337], [q1: 0.70069], [q0: 0.84594],\n",
      "          Effect: [ate-q], [train: 0.16280], [test: 0.16566]\n",
      "********************************************************************************\n",
      "epoch: 726 / 500, time cost: 46.28 sec, \n",
      "          Loss: [Train: 2.00384], [Test: 2.41605],\n",
      "          Accuracy: [prop score:  0.96343], [q1: 0.67431], [q0: 0.84265],\n",
      "          Effect: [ate-q], [train: 0.14869], [test: 0.15163]\n",
      "********************************************************************************\n",
      "epoch: 727 / 500, time cost: 46.32 sec, \n",
      "          Loss: [Train: 2.00383], [Test: 2.41682],\n",
      "          Accuracy: [prop score:  0.96341], [q1: 0.69874], [q0: 0.87318],\n",
      "          Effect: [ate-q], [train: 0.17740], [test: 0.18033]\n",
      "********************************************************************************\n",
      "epoch: 728 / 500, time cost: 48.05 sec, \n",
      "          Loss: [Train: 2.00354], [Test: 2.41493],\n",
      "          Accuracy: [prop score:  0.96336], [q1: 0.71785], [q0: 0.85692],\n",
      "          Effect: [ate-q], [train: 0.17992], [test: 0.18285]\n",
      "********************************************************************************\n",
      "epoch: 729 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.00330], [Test: 2.41490],\n",
      "          Accuracy: [prop score:  0.96340], [q1: 0.68008], [q0: 0.85365],\n",
      "          Effect: [ate-q], [train: 0.15699], [test: 0.16001]\n",
      "********************************************************************************\n",
      "epoch: 730 / 500, time cost: 48.13 sec, \n",
      "          Loss: [Train: 2.00272], [Test: 2.41728],\n",
      "          Accuracy: [prop score:  0.96351], [q1: 0.67540], [q0: 0.83541],\n",
      "          Effect: [ate-q], [train: 0.14697], [test: 0.14975]\n",
      "********************************************************************************\n",
      "epoch: 731 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.00280], [Test: 2.41871],\n",
      "          Accuracy: [prop score:  0.96342], [q1: 0.69490], [q0: 0.84556],\n",
      "          Effect: [ate-q], [train: 0.16119], [test: 0.16408]\n",
      "********************************************************************************\n",
      "epoch: 732 / 500, time cost: 45.44 sec, \n",
      "          Loss: [Train: 2.00186], [Test: 2.41921],\n",
      "          Accuracy: [prop score:  0.96346], [q1: 0.72265], [q0: 0.84481],\n",
      "          Effect: [ate-q], [train: 0.17614], [test: 0.17899]\n",
      "********************************************************************************\n",
      "epoch: 733 / 500, time cost: 47.89 sec, \n",
      "          Loss: [Train: 2.00199], [Test: 2.41979],\n",
      "          Accuracy: [prop score:  0.96345], [q1: 0.69116], [q0: 0.83522],\n",
      "          Effect: [ate-q], [train: 0.15372], [test: 0.15653]\n",
      "********************************************************************************\n",
      "epoch: 734 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.00127], [Test: 2.42049],\n",
      "          Accuracy: [prop score:  0.96348], [q1: 0.71041], [q0: 0.84054],\n",
      "          Effect: [ate-q], [train: 0.16766], [test: 0.17049]\n",
      "********************************************************************************\n",
      "epoch: 735 / 500, time cost: 48.55 sec, \n",
      "          Loss: [Train: 2.00102], [Test: 2.42110],\n",
      "          Accuracy: [prop score:  0.96342], [q1: 0.70204], [q0: 0.85501],\n",
      "          Effect: [ate-q], [train: 0.17029], [test: 0.17318]\n",
      "********************************************************************************\n",
      "epoch: 736 / 500, time cost: 46.16 sec, \n",
      "          Loss: [Train: 2.00047], [Test: 2.42149],\n",
      "          Accuracy: [prop score:  0.96356], [q1: 0.70158], [q0: 0.86039],\n",
      "          Effect: [ate-q], [train: 0.17443], [test: 0.17734]\n",
      "********************************************************************************\n",
      "epoch: 737 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.00037], [Test: 2.42135],\n",
      "          Accuracy: [prop score:  0.96352], [q1: 0.68701], [q0: 0.83390],\n",
      "          Effect: [ate-q], [train: 0.15299], [test: 0.15588]\n",
      "********************************************************************************\n",
      "epoch: 738 / 500, time cost: 48.65 sec, \n",
      "          Loss: [Train: 1.99989], [Test: 2.42254],\n",
      "          Accuracy: [prop score:  0.96354], [q1: 0.70912], [q0: 0.83198],\n",
      "          Effect: [ate-q], [train: 0.16230], [test: 0.16522]\n",
      "********************************************************************************\n",
      "epoch: 739 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 2.00022], [Test: 2.42308],\n",
      "          Accuracy: [prop score:  0.96364], [q1: 0.71434], [q0: 0.83938],\n",
      "          Effect: [ate-q], [train: 0.16976], [test: 0.17256]\n",
      "********************************************************************************\n",
      "epoch: 740 / 500, time cost: 48.46 sec, \n",
      "          Loss: [Train: 1.99920], [Test: 2.42389],\n",
      "          Accuracy: [prop score:  0.96348], [q1: 0.70597], [q0: 0.84477],\n",
      "          Effect: [ate-q], [train: 0.16887], [test: 0.17178]\n",
      "********************************************************************************\n",
      "epoch: 741 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 1.99907], [Test: 2.42476],\n",
      "          Accuracy: [prop score:  0.96364], [q1: 0.68296], [q0: 0.83129],\n",
      "          Effect: [ate-q], [train: 0.14884], [test: 0.15176]\n",
      "********************************************************************************\n",
      "epoch: 742 / 500, time cost: 46.25 sec, \n",
      "          Loss: [Train: 1.99878], [Test: 2.42492],\n",
      "          Accuracy: [prop score:  0.96356], [q1: 0.72109], [q0: 0.83930],\n",
      "          Effect: [ate-q], [train: 0.17266], [test: 0.17554]\n",
      "********************************************************************************\n",
      "epoch: 743 / 500, time cost: 48.02 sec, \n",
      "          Loss: [Train: 1.99836], [Test: 2.42559],\n",
      "          Accuracy: [prop score:  0.96359], [q1: 0.71549], [q0: 0.83151],\n",
      "          Effect: [ate-q], [train: 0.16689], [test: 0.16974]\n",
      "********************************************************************************\n",
      "epoch: 744 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 1.99827], [Test: 2.42622],\n",
      "          Accuracy: [prop score:  0.96351], [q1: 0.69609], [q0: 0.85016],\n",
      "          Effect: [ate-q], [train: 0.16695], [test: 0.16994]\n",
      "********************************************************************************\n",
      "epoch: 745 / 500, time cost: 48.34 sec, \n",
      "          Loss: [Train: 1.99724], [Test: 2.42630],\n",
      "          Accuracy: [prop score:  0.96356], [q1: 0.69564], [q0: 0.85680],\n",
      "          Effect: [ate-q], [train: 0.16954], [test: 0.17250]\n",
      "********************************************************************************\n",
      "epoch: 746 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 1.99681], [Test: 2.42712],\n",
      "          Accuracy: [prop score:  0.96358], [q1: 0.68150], [q0: 0.84197],\n",
      "          Effect: [ate-q], [train: 0.15336], [test: 0.15632]\n",
      "********************************************************************************\n",
      "epoch: 747 / 500, time cost: 45.36 sec, \n",
      "          Loss: [Train: 1.99707], [Test: 2.42924],\n",
      "          Accuracy: [prop score:  0.96370], [q1: 0.67369], [q0: 0.80054],\n",
      "          Effect: [ate-q], [train: 0.13092], [test: 0.13382]\n",
      "********************************************************************************\n",
      "epoch: 748 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 1.99648], [Test: 2.43198],\n",
      "          Accuracy: [prop score:  0.96356], [q1: 0.68691], [q0: 0.84787],\n",
      "          Effect: [ate-q], [train: 0.16177], [test: 0.16463]\n",
      "********************************************************************************\n",
      "epoch: 749 / 500, time cost: 45.43 sec, \n",
      "          Loss: [Train: 1.99608], [Test: 2.43250],\n",
      "          Accuracy: [prop score:  0.96366], [q1: 0.68910], [q0: 0.85363],\n",
      "          Effect: [ate-q], [train: 0.16691], [test: 0.16983]\n",
      "********************************************************************************\n",
      "epoch: 750 / 500, time cost: 47.27 sec, \n",
      "          Loss: [Train: 1.99602], [Test: 2.43230],\n",
      "          Accuracy: [prop score:  0.96370], [q1: 0.69301], [q0: 0.82710],\n",
      "          Effect: [ate-q], [train: 0.15308], [test: 0.15599]\n",
      "********************************************************************************\n",
      "epoch: 751 / 500, time cost: 45.57 sec, \n",
      "          Loss: [Train: 1.99534], [Test: 2.43426],\n",
      "          Accuracy: [prop score:  0.96377], [q1: 0.68302], [q0: 0.82331],\n",
      "          Effect: [ate-q], [train: 0.14563], [test: 0.14859]\n",
      "********************************************************************************\n",
      "epoch: 752 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 1.99518], [Test: 2.43475],\n",
      "          Accuracy: [prop score:  0.96374], [q1: 0.71430], [q0: 0.85744],\n",
      "          Effect: [ate-q], [train: 0.18195], [test: 0.18488]\n",
      "********************************************************************************\n",
      "epoch: 753 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 1.99484], [Test: 2.43621],\n",
      "          Accuracy: [prop score:  0.96376], [q1: 0.70907], [q0: 0.84709],\n",
      "          Effect: [ate-q], [train: 0.17330], [test: 0.17618]\n",
      "********************************************************************************\n",
      "epoch: 754 / 500, time cost: 46.09 sec, \n",
      "          Loss: [Train: 1.99479], [Test: 2.43640],\n",
      "          Accuracy: [prop score:  0.96377], [q1: 0.71643], [q0: 0.81711],\n",
      "          Effect: [ate-q], [train: 0.16216], [test: 0.16506]\n",
      "********************************************************************************\n",
      "epoch: 755 / 500, time cost: 48.71 sec, \n",
      "          Loss: [Train: 1.99452], [Test: 2.43698],\n",
      "          Accuracy: [prop score:  0.96377], [q1: 0.71779], [q0: 0.83865],\n",
      "          Effect: [ate-q], [train: 0.17359], [test: 0.17657]\n",
      "********************************************************************************\n",
      "epoch: 756 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 1.99430], [Test: 2.43622],\n",
      "          Accuracy: [prop score:  0.96384], [q1: 0.70120], [q0: 0.83879],\n",
      "          Effect: [ate-q], [train: 0.16512], [test: 0.16800]\n",
      "********************************************************************************\n",
      "epoch: 757 / 500, time cost: 46.42 sec, \n",
      "          Loss: [Train: 1.99346], [Test: 2.43715],\n",
      "          Accuracy: [prop score:  0.96383], [q1: 0.68659], [q0: 0.82691],\n",
      "          Effect: [ate-q], [train: 0.15041], [test: 0.15334]\n",
      "********************************************************************************\n",
      "epoch: 758 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 1.99345], [Test: 2.43788],\n",
      "          Accuracy: [prop score:  0.96381], [q1: 0.70493], [q0: 0.86345],\n",
      "          Effect: [ate-q], [train: 0.18017], [test: 0.18322]\n",
      "********************************************************************************\n",
      "epoch: 759 / 500, time cost: 46.53 sec, \n",
      "          Loss: [Train: 1.99324], [Test: 2.43799],\n",
      "          Accuracy: [prop score:  0.96376], [q1: 0.70955], [q0: 0.84713],\n",
      "          Effect: [ate-q], [train: 0.17382], [test: 0.17681]\n",
      "********************************************************************************\n",
      "epoch: 760 / 500, time cost: 48.29 sec, \n",
      "          Loss: [Train: 1.99283], [Test: 2.43892],\n",
      "          Accuracy: [prop score:  0.96378], [q1: 0.69840], [q0: 0.86092],\n",
      "          Effect: [ate-q], [train: 0.17738], [test: 0.18027]\n",
      "********************************************************************************\n",
      "epoch: 761 / 500, time cost: 46.67 sec, \n",
      "          Loss: [Train: 1.99266], [Test: 2.43944],\n",
      "          Accuracy: [prop score:  0.96387], [q1: 0.68531], [q0: 0.82443],\n",
      "          Effect: [ate-q], [train: 0.15012], [test: 0.15298]\n",
      "********************************************************************************\n",
      "epoch: 762 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 1.99220], [Test: 2.44087],\n",
      "          Accuracy: [prop score:  0.96378], [q1: 0.72529], [q0: 0.82402],\n",
      "          Effect: [ate-q], [train: 0.17119], [test: 0.17414]\n",
      "********************************************************************************\n",
      "epoch: 763 / 500, time cost: 48.76 sec, \n",
      "          Loss: [Train: 1.99212], [Test: 2.44135],\n",
      "          Accuracy: [prop score:  0.96383], [q1: 0.70810], [q0: 0.83743],\n",
      "          Effect: [ate-q], [train: 0.16909], [test: 0.17207]\n",
      "********************************************************************************\n",
      "epoch: 764 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 1.99124], [Test: 2.44175],\n",
      "          Accuracy: [prop score:  0.96386], [q1: 0.65927], [q0: 0.83745],\n",
      "          Effect: [ate-q], [train: 0.14324], [test: 0.14618]\n",
      "********************************************************************************\n",
      "epoch: 765 / 500, time cost: 48.69 sec, \n",
      "          Loss: [Train: 1.99138], [Test: 2.44070],\n",
      "          Accuracy: [prop score:  0.96391], [q1: 0.70051], [q0: 0.82317],\n",
      "          Effect: [ate-q], [train: 0.15762], [test: 0.16057]\n",
      "********************************************************************************\n",
      "epoch: 766 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 1.99043], [Test: 2.44273],\n",
      "          Accuracy: [prop score:  0.96384], [q1: 0.69494], [q0: 0.85304],\n",
      "          Effect: [ate-q], [train: 0.17167], [test: 0.17466]\n",
      "********************************************************************************\n",
      "epoch: 767 / 500, time cost: 46.70 sec, \n",
      "          Loss: [Train: 1.99040], [Test: 2.44392],\n",
      "          Accuracy: [prop score:  0.96392], [q1: 0.71837], [q0: 0.83445],\n",
      "          Effect: [ate-q], [train: 0.17458], [test: 0.17747]\n",
      "********************************************************************************\n",
      "epoch: 768 / 500, time cost: 48.86 sec, \n",
      "          Loss: [Train: 1.98994], [Test: 2.44343],\n",
      "          Accuracy: [prop score:  0.96392], [q1: 0.68510], [q0: 0.84073],\n",
      "          Effect: [ate-q], [train: 0.15870], [test: 0.16172]\n",
      "********************************************************************************\n",
      "epoch: 769 / 500, time cost: 46.91 sec, \n",
      "          Loss: [Train: 1.98968], [Test: 2.44401],\n",
      "          Accuracy: [prop score:  0.96391], [q1: 0.69608], [q0: 0.83412],\n",
      "          Effect: [ate-q], [train: 0.16095], [test: 0.16393]\n",
      "********************************************************************************\n",
      "epoch: 770 / 500, time cost: 49.00 sec, \n",
      "          Loss: [Train: 1.98991], [Test: 2.44557],\n",
      "          Accuracy: [prop score:  0.96392], [q1: 0.71451], [q0: 0.81638],\n",
      "          Effect: [ate-q], [train: 0.16184], [test: 0.16471]\n",
      "********************************************************************************\n",
      "epoch: 771 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 1.98909], [Test: 2.44518],\n",
      "          Accuracy: [prop score:  0.96394], [q1: 0.68574], [q0: 0.83204],\n",
      "          Effect: [ate-q], [train: 0.15343], [test: 0.15641]\n",
      "********************************************************************************\n",
      "epoch: 772 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 1.98830], [Test: 2.44663],\n",
      "          Accuracy: [prop score:  0.96390], [q1: 0.71618], [q0: 0.82558],\n",
      "          Effect: [ate-q], [train: 0.16821], [test: 0.17115]\n",
      "********************************************************************************\n",
      "epoch: 773 / 500, time cost: 48.11 sec, \n",
      "          Loss: [Train: 1.98804], [Test: 2.44835],\n",
      "          Accuracy: [prop score:  0.96405], [q1: 0.71192], [q0: 0.82900],\n",
      "          Effect: [ate-q], [train: 0.16725], [test: 0.17015]\n",
      "********************************************************************************\n",
      "epoch: 774 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 1.98798], [Test: 2.44678],\n",
      "          Accuracy: [prop score:  0.96399], [q1: 0.68886], [q0: 0.82123],\n",
      "          Effect: [ate-q], [train: 0.15159], [test: 0.15450]\n",
      "********************************************************************************\n",
      "epoch: 775 / 500, time cost: 47.99 sec, \n",
      "          Loss: [Train: 1.98783], [Test: 2.44788],\n",
      "          Accuracy: [prop score:  0.96405], [q1: 0.67240], [q0: 0.84307],\n",
      "          Effect: [ate-q], [train: 0.15451], [test: 0.15747]\n",
      "********************************************************************************\n",
      "epoch: 776 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 1.98742], [Test: 2.44792],\n",
      "          Accuracy: [prop score:  0.96413], [q1: 0.66385], [q0: 0.84109],\n",
      "          Effect: [ate-q], [train: 0.14943], [test: 0.15242]\n",
      "********************************************************************************\n",
      "epoch: 777 / 500, time cost: 45.06 sec, \n",
      "          Loss: [Train: 1.98712], [Test: 2.44835],\n",
      "          Accuracy: [prop score:  0.96402], [q1: 0.70683], [q0: 0.84248],\n",
      "          Effect: [ate-q], [train: 0.17132], [test: 0.17435]\n",
      "********************************************************************************\n",
      "epoch: 778 / 500, time cost: 47.41 sec, \n",
      "          Loss: [Train: 1.98679], [Test: 2.44803],\n",
      "          Accuracy: [prop score:  0.96407], [q1: 0.70423], [q0: 0.84181],\n",
      "          Effect: [ate-q], [train: 0.17079], [test: 0.17368]\n",
      "********************************************************************************\n",
      "epoch: 779 / 500, time cost: 45.50 sec, \n",
      "          Loss: [Train: 1.98605], [Test: 2.44939],\n",
      "          Accuracy: [prop score:  0.96412], [q1: 0.69347], [q0: 0.81664],\n",
      "          Effect: [ate-q], [train: 0.15058], [test: 0.15360]\n",
      "********************************************************************************\n",
      "epoch: 780 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 1.98575], [Test: 2.44964],\n",
      "          Accuracy: [prop score:  0.96416], [q1: 0.68319], [q0: 0.83440],\n",
      "          Effect: [ate-q], [train: 0.15566], [test: 0.15872]\n",
      "********************************************************************************\n",
      "epoch: 781 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 1.98586], [Test: 2.44934],\n",
      "          Accuracy: [prop score:  0.96416], [q1: 0.69570], [q0: 0.82641],\n",
      "          Effect: [ate-q], [train: 0.15785], [test: 0.16083]\n",
      "********************************************************************************\n",
      "epoch: 782 / 500, time cost: 45.13 sec, \n",
      "          Loss: [Train: 1.98531], [Test: 2.45354],\n",
      "          Accuracy: [prop score:  0.96403], [q1: 0.68904], [q0: 0.86151],\n",
      "          Effect: [ate-q], [train: 0.17657], [test: 0.17957]\n",
      "********************************************************************************\n",
      "epoch: 783 / 500, time cost: 47.50 sec, \n",
      "          Loss: [Train: 1.98493], [Test: 2.45386],\n",
      "          Accuracy: [prop score:  0.96415], [q1: 0.69868], [q0: 0.83308],\n",
      "          Effect: [ate-q], [train: 0.16245], [test: 0.16543]\n",
      "********************************************************************************\n",
      "epoch: 784 / 500, time cost: 45.36 sec, \n",
      "          Loss: [Train: 1.98537], [Test: 2.45442],\n",
      "          Accuracy: [prop score:  0.96409], [q1: 0.70467], [q0: 0.81720],\n",
      "          Effect: [ate-q], [train: 0.15899], [test: 0.16201]\n",
      "********************************************************************************\n",
      "epoch: 785 / 500, time cost: 48.13 sec, \n",
      "          Loss: [Train: 1.98479], [Test: 2.45700],\n",
      "          Accuracy: [prop score:  0.96409], [q1: 0.67785], [q0: 0.84028],\n",
      "          Effect: [ate-q], [train: 0.15756], [test: 0.16059]\n",
      "********************************************************************************\n",
      "epoch: 786 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 1.98404], [Test: 2.45727],\n",
      "          Accuracy: [prop score:  0.96408], [q1: 0.69282], [q0: 0.80217],\n",
      "          Effect: [ate-q], [train: 0.14539], [test: 0.14830]\n",
      "********************************************************************************\n",
      "epoch: 787 / 500, time cost: 46.11 sec, \n",
      "          Loss: [Train: 1.98426], [Test: 2.46059],\n",
      "          Accuracy: [prop score:  0.96420], [q1: 0.69796], [q0: 0.81610],\n",
      "          Effect: [ate-q], [train: 0.15432], [test: 0.15726]\n",
      "********************************************************************************\n",
      "epoch: 788 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 1.98387], [Test: 2.46088],\n",
      "          Accuracy: [prop score:  0.96426], [q1: 0.69018], [q0: 0.83053],\n",
      "          Effect: [ate-q], [train: 0.15711], [test: 0.16011]\n",
      "********************************************************************************\n",
      "epoch: 789 / 500, time cost: 45.45 sec, \n",
      "          Loss: [Train: 1.98281], [Test: 2.46263],\n",
      "          Accuracy: [prop score:  0.96428], [q1: 0.69563], [q0: 0.82306],\n",
      "          Effect: [ate-q], [train: 0.15608], [test: 0.15905]\n",
      "********************************************************************************\n",
      "epoch: 790 / 500, time cost: 47.56 sec, \n",
      "          Loss: [Train: 1.98303], [Test: 2.46036],\n",
      "          Accuracy: [prop score:  0.96425], [q1: 0.69821], [q0: 0.84283],\n",
      "          Effect: [ate-q], [train: 0.16916], [test: 0.17216]\n",
      "********************************************************************************\n",
      "epoch: 791 / 500, time cost: 44.96 sec, \n",
      "          Loss: [Train: 1.98232], [Test: 2.46372],\n",
      "          Accuracy: [prop score:  0.96431], [q1: 0.69797], [q0: 0.83724],\n",
      "          Effect: [ate-q], [train: 0.16583], [test: 0.16881]\n",
      "********************************************************************************\n",
      "epoch: 792 / 500, time cost: 45.12 sec, \n",
      "          Loss: [Train: 1.98256], [Test: 2.46182],\n",
      "          Accuracy: [prop score:  0.96431], [q1: 0.70726], [q0: 0.84179],\n",
      "          Effect: [ate-q], [train: 0.17529], [test: 0.17825]\n",
      "********************************************************************************\n",
      "epoch: 793 / 500, time cost: 47.60 sec, \n",
      "          Loss: [Train: 1.98242], [Test: 2.46329],\n",
      "          Accuracy: [prop score:  0.96421], [q1: 0.69479], [q0: 0.81336],\n",
      "          Effect: [ate-q], [train: 0.15263], [test: 0.15562]\n",
      "********************************************************************************\n",
      "epoch: 794 / 500, time cost: 45.82 sec, \n",
      "          Loss: [Train: 1.98185], [Test: 2.46478],\n",
      "          Accuracy: [prop score:  0.96422], [q1: 0.69084], [q0: 0.81558],\n",
      "          Effect: [ate-q], [train: 0.15006], [test: 0.15313]\n",
      "********************************************************************************\n",
      "epoch: 795 / 500, time cost: 47.57 sec, \n",
      "          Loss: [Train: 1.98096], [Test: 2.46564],\n",
      "          Accuracy: [prop score:  0.96417], [q1: 0.66115], [q0: 0.82581],\n",
      "          Effect: [ate-q], [train: 0.14317], [test: 0.14627]\n",
      "********************************************************************************\n",
      "epoch: 796 / 500, time cost: 45.39 sec, \n",
      "          Loss: [Train: 1.98114], [Test: 2.46469],\n",
      "          Accuracy: [prop score:  0.96420], [q1: 0.67993], [q0: 0.83828],\n",
      "          Effect: [ate-q], [train: 0.15843], [test: 0.16153]\n",
      "********************************************************************************\n",
      "epoch: 797 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 1.98066], [Test: 2.46662],\n",
      "          Accuracy: [prop score:  0.96432], [q1: 0.71853], [q0: 0.84785],\n",
      "          Effect: [ate-q], [train: 0.18345], [test: 0.18647]\n",
      "********************************************************************************\n",
      "epoch: 798 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 1.98068], [Test: 2.46828],\n",
      "          Accuracy: [prop score:  0.96428], [q1: 0.69880], [q0: 0.79087],\n",
      "          Effect: [ate-q], [train: 0.14519], [test: 0.14820]\n",
      "********************************************************************************\n",
      "epoch: 799 / 500, time cost: 45.34 sec, \n",
      "          Loss: [Train: 1.98002], [Test: 2.46712],\n",
      "          Accuracy: [prop score:  0.96433], [q1: 0.69645], [q0: 0.81759],\n",
      "          Effect: [ate-q], [train: 0.15537], [test: 0.15831]\n",
      "********************************************************************************\n",
      "epoch: 800 / 500, time cost: 47.43 sec, \n",
      "          Loss: [Train: 1.97946], [Test: 2.46867],\n",
      "          Accuracy: [prop score:  0.96442], [q1: 0.67769], [q0: 0.82314],\n",
      "          Effect: [ate-q], [train: 0.14965], [test: 0.15272]\n",
      "********************************************************************************\n",
      "Finish training...\n"
     ]
    }
   ],
   "source": [
    "for e in range(501, 801):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        run_loss += loss.item()\n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, 801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
