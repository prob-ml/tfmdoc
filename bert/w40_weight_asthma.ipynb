{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM, BertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from data import CausalBertRealDataset\n",
    "from utils import DATA_PATH, make_dirs\n",
    "from causal_bert_real import CausalBOW, CausalBert, load_data, est_casual_effect\n",
    "from causal_tokens import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_casual_effect(data_loader, effect='ate', estimation='q'):\n",
    "    assert estimation == 'q', f'unallowed estimation: {estimation}'\n",
    "    \n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    Q1 = dataset.treatment * dataset.response + (1 - dataset.treatment) * dataset.pseudo_response\n",
    "    Q1 = Q1.cpu().data.numpy().squeeze()\n",
    "\n",
    "    Q0 = dataset.treatment * dataset.pseudo_response + (1 - dataset.treatment) * dataset.response\n",
    "    Q0 = Q0.cpu().data.numpy().squeeze()\n",
    "\n",
    "    treatment = dataset.treatment.cpu().data.numpy().squeeze()\n",
    "    prop_scores = dataset.prop_scores.cpu().data.numpy().squeeze()\n",
    "    \n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (treatment * (Q1 - Q0))\n",
    "            return phi.sum() / treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            return (Q1 - Q0).mean()\n",
    "        \n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            return phi / treatment.mean()\n",
    "        elif effect == 'ate': \n",
    "            return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = '/nfs/turbo/lsa-regier/bert-results/results/casualmodel/realdata/[BEST]_[asthma->flu]_C-BERT_64_15.pth'\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(trained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training set in 89.91 sec\n",
      "Training set: [treated: 0.0777], Training set: [response: 0.0606]\n",
      "Load validation set in 85.96 sec\n",
      "Validation set: [treated: 0.0780], Validation set: [response: 0.0604]\n",
      "****************************************************************************************************\n",
      "Unadjusted: [value: 0.0075]\n"
     ]
    }
   ],
   "source": [
    "pretreat_tokens = None\n",
    "treat_tokens = ASTHMA_TOKENS\n",
    "response_tokens = FLU_TOKENS\n",
    "\n",
    "train_loader, test_loader = load_data(pretreat_tokens, treat_tokens, response_tokens, 16, device=device)\n",
    "\n",
    "unadjust = (test_loader.dataset.responses[test_loader.dataset.treatments == 1].mean() - \n",
    "            test_loader.dataset.responses[test_loader.dataset.treatments == 0].mean()).item()\n",
    "print(f'Unadjusted: [value: {unadjust:.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_scores = []\n",
    "\n",
    "treatments = []\n",
    "responses = []\n",
    "\n",
    "tokens = []\n",
    "model.eval()\n",
    "for idx, (token, treatment, response) in enumerate(test_loader):\n",
    "    prop_score, _, _ = model(token)\n",
    "    if model.prop_is_logit:\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        prop_score = sigmoid(prop_score)\n",
    "    prop_scores.append(prop_score.cpu().data.numpy().squeeze())\n",
    "    treatments.append(treatment.cpu().data.numpy().squeeze())\n",
    "    responses.append(response.cpu().data.numpy().squeeze())\n",
    "    tokens.append(token.cpu().data.numpy().squeeze())\n",
    "    \n",
    "prop_scores = np.concatenate(prop_scores, axis=0)\n",
    "treatments = np.concatenate(treatments, axis=0)\n",
    "responses = np.concatenate(responses, axis=0)\n",
    "tokens = np.concatenate(tokens, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ate = treatments / prop_scores + (1 - treatments) / (1 - prop_scores)\n",
    "w_att = treatments + (1 - treatments) * prop_scores / (1 - prop_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_effect(responses, treatments, prop_scores, est, outlier=None):\n",
    "    \n",
    "    if outlier:\n",
    "        responses = responses[(prop_scores > outlier) & (prop_scores < 1 - outlier)]\n",
    "        treatments = treatments[(prop_scores > outlier) & (prop_scores < 1 - outlier)]\n",
    "        prop_scores = prop_scores[(prop_scores > outlier) & (prop_scores < 1 - outlier)]\n",
    "\n",
    "    w_ate = treatments / prop_scores + (1 - treatments) / (1 - prop_scores)\n",
    "    w_att = treatments + (1 - treatments) * prop_scores / (1 - prop_scores)\n",
    "\n",
    "    if est == 'ate':\n",
    "        w = w_ate\n",
    "    else:\n",
    "        w = w_att\n",
    "    e1 = (responses * treatments * w).sum() / (treatments * w).sum()\n",
    "    e2 = (responses * (1 - treatments) * w).sum() / ((1 - treatments) * w).sum()\n",
    "    \n",
    "    return e1 - e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007452067"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[treatments == 1].mean() - responses[treatments == 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.055631317"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_effect(responses, treatments, prop_scores, 'ate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14514136"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_effect(responses, treatments, prop_scores, 'att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(treatments, prop_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.array([treatments, prop_scores]).T\n",
    "dat = pd.DataFrame(dat, columns=['treatments', 'pred_prop_scores'])\n",
    "\n",
    "plt.cla()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "_ = sns.boxplot(y='pred_prop_scores', x='treatments', data=dat, palette=\"colorblind\", )\n",
    "ax.set_xlabel('Treatment')\n",
    "ax.set_ylabel('Predicted propensity scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_scores[treatments == 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_scores[treatments == 0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
