{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from data import CausalBertDataset, MLMDataset\n",
    "from causal_bert import CausalBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 256\n",
    "epoch = 500\n",
    "hidden_size = 64\n",
    "lr = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def true_casual_effect(data_loader, effect='ate', estimation='q'):\n",
    "    assert effect == 'ate' and estimation == 'q', f'unallowed effect/estimation: {effect}/{estimation}'\n",
    "    \n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    Q1 = dataset.treatment * dataset.response + (1 - dataset.treatment) * dataset.pseudo_response\n",
    "    Q1 = Q1.cpu().data.numpy().squeeze()\n",
    "\n",
    "    Q0 = dataset.treatment * dataset.pseudo_response + (1 - dataset.treatment) * dataset.response\n",
    "    Q0 = Q0.cpu().data.numpy().squeeze()\n",
    "\n",
    "    treatment = dataset.treatment.cpu().data.numpy().squeeze()\n",
    "    prop_scores = dataset.prop_scores.cpu().data.numpy().squeeze()\n",
    "    \n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (treatment * (Q1 - Q0))\n",
    "            return phi.sum() / treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            return (Q1 - Q0).mean()\n",
    "        \n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            return phi / treatment.mean()\n",
    "        elif effect == 'ate': \n",
    "            return phi\n",
    "        \n",
    "def est_casual_effect(data_loader, model, effect='ate', estimation='q', evaluate=True, **kwargs):\n",
    "    # We use `real_treatment` here to emphasize the estimations use real instead of estimated treatment.\n",
    "    real_response, real_treatment, real_prop_scores = [], [], []\n",
    "    prop_scores, Q1, Q0 = [], [], []\n",
    "    \n",
    "    if evaluate:\n",
    "        g_loss = kwargs.get('g_loss')\n",
    "        q_loss = kwargs.get('q_loss')\n",
    "        g_loss_test, q1_loss_test, q0_loss_test  = [], [], []\n",
    "        \n",
    "    model.eval()\n",
    "    for idx, (tokens, treatment, response, real_prop_score) in enumerate(data_loader):\n",
    "        real_response.append(response.cpu().data.numpy().squeeze())\n",
    "        real_treatment.append(treatment.cpu().data.numpy().squeeze())\n",
    "        real_prop_scores.append(real_prop_score.cpu().data.numpy().squeeze())\n",
    "\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        prop_scores.append(prop_score.cpu().data.numpy().squeeze())\n",
    "        Q1.append(q1.cpu().data.numpy().squeeze())\n",
    "        Q0.append(q0.cpu().data.numpy().squeeze())\n",
    "        \n",
    "        # Evaulate loss\n",
    "        if evaluate:\n",
    "            g_loss_val  = g_loss(prop_score, treatment)\n",
    "            q1_loss_val = q_loss(q1[treatment==1], response[treatment==1])\n",
    "            q0_loss_val = q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "            g_loss_test.append(g_loss_val.item())\n",
    "            q1_loss_test.append(q1_loss_val.item())\n",
    "            q0_loss_test.append(q0_loss_val.item())\n",
    "    \n",
    "    g_loss = np.array(g_loss_test).mean() if evaluate else None\n",
    "    q1_loss = np.array(q1_loss_test).mean() if evaluate else None\n",
    "    q0_loss = np.array(q0_loss_test).mean() if evaluate else None\n",
    "\n",
    "    Q1 = np.concatenate(Q1, axis=0)\n",
    "    Q0 = np.concatenate(Q0, axis=0)\n",
    "    prop_scores = np.concatenate(prop_scores, axis=0)\n",
    "    \n",
    "    real_response = np.concatenate(real_response, axis=0)\n",
    "    real_treatment = np.concatenate(real_treatment, axis=0)\n",
    "    real_prop_scores = np.concatenate(real_prop_scores, axis=0)\n",
    "    \n",
    "    # Evaluate accuracy.\n",
    "    if evaluate:\n",
    "        dataset = data_loader.dataset\n",
    "        \n",
    "        real_q1_prob = sigmoid(dataset.alpha + dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        real_q0_prob = sigmoid(dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        thre = (real_q1_prob + real_q0_prob) / 2\n",
    "\n",
    "    # prop score: real and estimated must locate one the same side of 0.5.\n",
    "    prop_accu = (1. * (((real_prop_scores - .5) * (prop_scores - .5)) > 0.)).mean() if evaluate else None\n",
    "    # q: estimate is more close to corresponding real value than the other.\n",
    "    q1_accu = (1. * (dataset.alpha > 0) * (Q1 > thre)).mean() if evaluate else None\n",
    "    q0_accu = (1. * (dataset.alpha > 0) * (Q0 < thre)).mean() if evaluate else None\n",
    "\n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (real_treatment * (Q1 - Q0))\n",
    "            effect = phi.sum() / real_treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            effect = (Q1 - Q0).mean()\n",
    "\n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            effect = phi / real_treatment.mean()\n",
    "        elif effect == 'ate':\n",
    "            effect = phi\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return effect, g_loss, q1_loss, q0_loss, prop_accu, q1_accu, q0_accu\n",
    "\n",
    "def show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch):\n",
    "    train_loss_hist = np.array(train_loss_hist)\n",
    "    test_loss_hist = np.array(test_loss_hist)\n",
    "    est_effect = np.array(est_effect)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    lns1 = ax.plot(np.arange(epoch), test_loss_hist, label='Eval loss')\n",
    "    ax_r = plt.twinx()\n",
    "    lns2 = ax_r.plot(np.arange(epoch), est_effect, color='coral', label='Estimate ATE')\n",
    "    lns3 = ax_r.plot(np.arange(epoch), np.ones(epoch) * real, color='red', ls='--', label='Real ATE')\n",
    "    lns4 = ax_r.plot(np.arange(epoch), np.ones(epoch) * unadjust, color='green', ls='--', label='Unadjusted ATE')\n",
    "\n",
    "    lns = lns1+lns2+lns3+lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax_r.legend(lns, labs, loc=0)\n",
    "    ax.set_ylabel('Eval loss')\n",
    "    ax_r.set_ylabel('ATEs')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 5.\n",
    "c = 0.2\n",
    "i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training set in 155.61 sec\n",
      "Load validation set in 100.26 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "                             group=list(range(1)), max_length=512, min_length=10,\n",
    "                             truncate_method='first', device=device, seed=1)\n",
    "\n",
    "print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "start = time.time()\n",
    "testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                            alpha=alpha, beta=beta, c=c, i=i, \n",
    "                            group=[9], max_length=512, min_length=10,\n",
    "                            truncate_method='first', device=device)\n",
    "\n",
    "print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [effect: ate], [estimation: q], [value: 0.11704]\n",
      "Unadjusted: [value: 0.1880]\n"
     ]
    }
   ],
   "source": [
    "real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "token_embed = model.get_input_embeddings()\n",
    "model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=hidden_size, prop_is_logit=True).to(device)\n",
    "\n",
    "pos_portion = trainset.treatment.mean()\n",
    "pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "epoch_iter = len(train_loader)\n",
    "total_steps = epoch * epoch_iter\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "q_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCELoss()\n",
    "prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Please specify the effect and estimation we want to use here.\n",
    "effect = 'ate'\n",
    "estimation = 'q'\n",
    "\n",
    "effect = effect.lower()\n",
    "estimation = estimation.lower()\n",
    "assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 500, time cost: 57.41 sec, \n",
      "          Loss: [Train: 2.41032], [Test: 2.40738],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.93595], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.06428], [test: 0.06428]\n",
      "********************************************************************************\n",
      "epoch: 2 / 500, time cost: 53.03 sec, \n",
      "          Loss: [Train: 2.39291], [Test: 2.38952],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.09683], [test: 0.09684]\n",
      "********************************************************************************\n",
      "epoch: 3 / 500, time cost: 51.21 sec, \n",
      "          Loss: [Train: 2.37579], [Test: 2.37641],\n",
      "          Accuracy: [prop score:  0.93655], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13793], [test: 0.13794]\n",
      "********************************************************************************\n",
      "epoch: 4 / 500, time cost: 51.12 sec, \n",
      "          Loss: [Train: 2.36626], [Test: 2.37167],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.16822], [test: 0.16823]\n",
      "********************************************************************************\n",
      "epoch: 5 / 500, time cost: 48.13 sec, \n",
      "          Loss: [Train: 2.36284], [Test: 2.37057],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18273], [test: 0.18274]\n",
      "********************************************************************************\n",
      "epoch: 6 / 500, time cost: 50.92 sec, \n",
      "          Loss: [Train: 2.36235], [Test: 2.37051],\n",
      "          Accuracy: [prop score:  0.93648], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18784], [test: 0.18785]\n",
      "********************************************************************************\n",
      "epoch: 7 / 500, time cost: 51.15 sec, \n",
      "          Loss: [Train: 2.36195], [Test: 2.37029],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18991], [test: 0.18992]\n",
      "********************************************************************************\n",
      "epoch: 8 / 500, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.36144], [Test: 2.36961],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19222], [test: 0.19223]\n",
      "********************************************************************************\n",
      "epoch: 9 / 500, time cost: 50.44 sec, \n",
      "          Loss: [Train: 2.36156], [Test: 2.36970],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19078], [test: 0.19079]\n",
      "********************************************************************************\n",
      "epoch: 10 / 500, time cost: 46.56 sec, \n",
      "          Loss: [Train: 2.36141], [Test: 2.36947],\n",
      "          Accuracy: [prop score:  0.93642], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18969], [test: 0.18969]\n",
      "********************************************************************************\n",
      "epoch: 11 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.36085], [Test: 2.36914],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19010], [test: 0.19011]\n",
      "********************************************************************************\n",
      "epoch: 12 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.36059], [Test: 2.36901],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19011], [test: 0.19012]\n",
      "********************************************************************************\n",
      "epoch: 13 / 500, time cost: 45.35 sec, \n",
      "          Loss: [Train: 2.36048], [Test: 2.36878],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19026], [test: 0.19026]\n",
      "********************************************************************************\n",
      "epoch: 14 / 500, time cost: 47.19 sec, \n",
      "          Loss: [Train: 2.36035], [Test: 2.36896],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18985], [test: 0.18986]\n",
      "********************************************************************************\n",
      "epoch: 15 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.36020], [Test: 2.36866],\n",
      "          Accuracy: [prop score:  0.93641], [q1: 0.93641], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19052], [test: 0.19052]\n",
      "********************************************************************************\n",
      "epoch: 16 / 500, time cost: 53.15 sec, \n",
      "          Loss: [Train: 2.36013], [Test: 2.36812],\n",
      "          Accuracy: [prop score:  0.93653], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18980], [test: 0.18980]\n",
      "********************************************************************************\n",
      "epoch: 17 / 500, time cost: 47.85 sec, \n",
      "          Loss: [Train: 2.35929], [Test: 2.36835],\n",
      "          Accuracy: [prop score:  0.93657], [q1: 0.93657], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19167], [test: 0.19167]\n",
      "********************************************************************************\n",
      "epoch: 18 / 500, time cost: 52.50 sec, \n",
      "          Loss: [Train: 2.35920], [Test: 2.36778],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19230], [test: 0.19230]\n",
      "********************************************************************************\n",
      "epoch: 19 / 500, time cost: 47.65 sec, \n",
      "          Loss: [Train: 2.35906], [Test: 2.36747],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18976], [test: 0.18976]\n",
      "********************************************************************************\n",
      "epoch: 20 / 500, time cost: 51.22 sec, \n",
      "          Loss: [Train: 2.35866], [Test: 2.36756],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19029], [test: 0.19028]\n",
      "********************************************************************************\n",
      "epoch: 21 / 500, time cost: 50.09 sec, \n",
      "          Loss: [Train: 2.35813], [Test: 2.36688],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19091], [test: 0.19091]\n",
      "********************************************************************************\n",
      "epoch: 22 / 500, time cost: 51.16 sec, \n",
      "          Loss: [Train: 2.35841], [Test: 2.36691],\n",
      "          Accuracy: [prop score:  0.93649], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18856], [test: 0.18856]\n",
      "********************************************************************************\n",
      "epoch: 23 / 500, time cost: 46.98 sec, \n",
      "          Loss: [Train: 2.35817], [Test: 2.36650],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19074], [test: 0.19074]\n",
      "********************************************************************************\n",
      "epoch: 24 / 500, time cost: 48.80 sec, \n",
      "          Loss: [Train: 2.35753], [Test: 2.36651],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19021], [test: 0.19021]\n",
      "********************************************************************************\n",
      "epoch: 25 / 500, time cost: 46.25 sec, \n",
      "          Loss: [Train: 2.35749], [Test: 2.36588],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18945], [test: 0.18944]\n",
      "********************************************************************************\n",
      "epoch: 26 / 500, time cost: 49.11 sec, \n",
      "          Loss: [Train: 2.35721], [Test: 2.36608],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18948], [test: 0.18948]\n",
      "********************************************************************************\n",
      "epoch: 27 / 500, time cost: 47.35 sec, \n",
      "          Loss: [Train: 2.35662], [Test: 2.36552],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19135], [test: 0.19135]\n",
      "********************************************************************************\n",
      "epoch: 28 / 500, time cost: 48.57 sec, \n",
      "          Loss: [Train: 2.35669], [Test: 2.36546],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19024], [test: 0.19024]\n",
      "********************************************************************************\n",
      "epoch: 29 / 500, time cost: 46.30 sec, \n",
      "          Loss: [Train: 2.35625], [Test: 2.36474],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18999], [test: 0.18998]\n",
      "********************************************************************************\n",
      "epoch: 30 / 500, time cost: 49.11 sec, \n",
      "          Loss: [Train: 2.35579], [Test: 2.36462],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18889], [test: 0.18889]\n",
      "********************************************************************************\n",
      "epoch: 31 / 500, time cost: 45.65 sec, \n",
      "          Loss: [Train: 2.35528], [Test: 2.36461],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19026], [test: 0.19026]\n",
      "********************************************************************************\n",
      "epoch: 32 / 500, time cost: 51.07 sec, \n",
      "          Loss: [Train: 2.35512], [Test: 2.36422],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19020], [test: 0.19020]\n",
      "********************************************************************************\n",
      "epoch: 33 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.35500], [Test: 2.36383],\n",
      "          Accuracy: [prop score:  0.93648], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18960], [test: 0.18959]\n",
      "********************************************************************************\n",
      "epoch: 34 / 500, time cost: 51.54 sec, \n",
      "          Loss: [Train: 2.35458], [Test: 2.36371],\n",
      "          Accuracy: [prop score:  0.93655], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18850], [test: 0.18850]\n",
      "********************************************************************************\n",
      "epoch: 35 / 500, time cost: 46.99 sec, \n",
      "          Loss: [Train: 2.35406], [Test: 2.36338],\n",
      "          Accuracy: [prop score:  0.93661], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19038], [test: 0.19037]\n",
      "********************************************************************************\n",
      "epoch: 36 / 500, time cost: 48.68 sec, \n",
      "          Loss: [Train: 2.35380], [Test: 2.36331],\n",
      "          Accuracy: [prop score:  0.93658], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19061], [test: 0.19060]\n",
      "********************************************************************************\n",
      "epoch: 37 / 500, time cost: 46.63 sec, \n",
      "          Loss: [Train: 2.35298], [Test: 2.36292],\n",
      "          Accuracy: [prop score:  0.93654], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19019], [test: 0.19018]\n",
      "********************************************************************************\n",
      "epoch: 38 / 500, time cost: 49.14 sec, \n",
      "          Loss: [Train: 2.35306], [Test: 2.36209],\n",
      "          Accuracy: [prop score:  0.93657], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19030], [test: 0.19029]\n",
      "********************************************************************************\n",
      "epoch: 39 / 500, time cost: 47.89 sec, \n",
      "          Loss: [Train: 2.35253], [Test: 2.36184],\n",
      "          Accuracy: [prop score:  0.93666], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18972], [test: 0.18972]\n",
      "********************************************************************************\n",
      "epoch: 40 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.35225], [Test: 2.36203],\n",
      "          Accuracy: [prop score:  0.93663], [q1: 0.93645], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19267], [test: 0.19267]\n",
      "********************************************************************************\n",
      "epoch: 41 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 2.35169], [Test: 2.36127],\n",
      "          Accuracy: [prop score:  0.93664], [q1: 0.93643], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19023], [test: 0.19023]\n",
      "********************************************************************************\n",
      "epoch: 42 / 500, time cost: 48.59 sec, \n",
      "          Loss: [Train: 2.35123], [Test: 2.36080],\n",
      "          Accuracy: [prop score:  0.93675], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19014], [test: 0.19014]\n",
      "********************************************************************************\n",
      "epoch: 43 / 500, time cost: 46.55 sec, \n",
      "          Loss: [Train: 2.35090], [Test: 2.36052],\n",
      "          Accuracy: [prop score:  0.93676], [q1: 0.93641], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19084], [test: 0.19084]\n",
      "********************************************************************************\n",
      "epoch: 44 / 500, time cost: 48.63 sec, \n",
      "          Loss: [Train: 2.35045], [Test: 2.36013],\n",
      "          Accuracy: [prop score:  0.93695], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18996], [test: 0.18995]\n",
      "********************************************************************************\n",
      "epoch: 45 / 500, time cost: 45.13 sec, \n",
      "          Loss: [Train: 2.34978], [Test: 2.35984],\n",
      "          Accuracy: [prop score:  0.93696], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.19033], [test: 0.19033]\n",
      "********************************************************************************\n",
      "epoch: 46 / 500, time cost: 48.96 sec, \n",
      "          Loss: [Train: 2.34997], [Test: 2.35936],\n",
      "          Accuracy: [prop score:  0.93717], [q1: 0.93662], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18861], [test: 0.18860]\n",
      "********************************************************************************\n",
      "epoch: 47 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.34924], [Test: 2.35884],\n",
      "          Accuracy: [prop score:  0.93708], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18934], [test: 0.18934]\n",
      "********************************************************************************\n",
      "epoch: 48 / 500, time cost: 50.83 sec, \n",
      "          Loss: [Train: 2.34836], [Test: 2.35853],\n",
      "          Accuracy: [prop score:  0.93717], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18799], [test: 0.18799]\n",
      "********************************************************************************\n",
      "epoch: 49 / 500, time cost: 49.48 sec, \n",
      "          Loss: [Train: 2.34816], [Test: 2.35804],\n",
      "          Accuracy: [prop score:  0.93724], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18767], [test: 0.18767]\n",
      "********************************************************************************\n",
      "epoch: 50 / 500, time cost: 51.42 sec, \n",
      "          Loss: [Train: 2.34793], [Test: 2.35764],\n",
      "          Accuracy: [prop score:  0.93734], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18741], [test: 0.18741]\n",
      "********************************************************************************\n",
      "epoch: 51 / 500, time cost: 47.25 sec, \n",
      "          Loss: [Train: 2.34779], [Test: 2.35680],\n",
      "          Accuracy: [prop score:  0.93734], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18537], [test: 0.18538]\n",
      "********************************************************************************\n",
      "epoch: 52 / 500, time cost: 49.86 sec, \n",
      "          Loss: [Train: 2.34679], [Test: 2.35648],\n",
      "          Accuracy: [prop score:  0.93736], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18675], [test: 0.18675]\n",
      "********************************************************************************\n",
      "epoch: 53 / 500, time cost: 48.75 sec, \n",
      "          Loss: [Train: 2.34626], [Test: 2.35608],\n",
      "          Accuracy: [prop score:  0.93746], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18688], [test: 0.18689]\n",
      "********************************************************************************\n",
      "epoch: 54 / 500, time cost: 49.11 sec, \n",
      "          Loss: [Train: 2.34538], [Test: 2.35558],\n",
      "          Accuracy: [prop score:  0.93758], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18676], [test: 0.18677]\n",
      "********************************************************************************\n",
      "epoch: 55 / 500, time cost: 47.35 sec, \n",
      "          Loss: [Train: 2.34490], [Test: 2.35531],\n",
      "          Accuracy: [prop score:  0.93768], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18658], [test: 0.18658]\n",
      "********************************************************************************\n",
      "epoch: 56 / 500, time cost: 49.51 sec, \n",
      "          Loss: [Train: 2.34435], [Test: 2.35489],\n",
      "          Accuracy: [prop score:  0.93769], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18621], [test: 0.18622]\n",
      "********************************************************************************\n",
      "epoch: 57 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.34387], [Test: 2.35465],\n",
      "          Accuracy: [prop score:  0.93789], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18821], [test: 0.18823]\n",
      "********************************************************************************\n",
      "epoch: 58 / 500, time cost: 49.65 sec, \n",
      "          Loss: [Train: 2.34365], [Test: 2.35320],\n",
      "          Accuracy: [prop score:  0.93792], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18549], [test: 0.18551]\n",
      "********************************************************************************\n",
      "epoch: 59 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.34296], [Test: 2.35340],\n",
      "          Accuracy: [prop score:  0.93804], [q1: 0.93658], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18569], [test: 0.18571]\n",
      "********************************************************************************\n",
      "epoch: 60 / 500, time cost: 49.27 sec, \n",
      "          Loss: [Train: 2.34239], [Test: 2.35299],\n",
      "          Accuracy: [prop score:  0.93801], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18598], [test: 0.18601]\n",
      "********************************************************************************\n",
      "epoch: 61 / 500, time cost: 47.61 sec, \n",
      "          Loss: [Train: 2.34149], [Test: 2.35158],\n",
      "          Accuracy: [prop score:  0.93820], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18688], [test: 0.18690]\n",
      "********************************************************************************\n",
      "epoch: 62 / 500, time cost: 48.78 sec, \n",
      "          Loss: [Train: 2.34108], [Test: 2.35157],\n",
      "          Accuracy: [prop score:  0.93841], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18490], [test: 0.18492]\n",
      "********************************************************************************\n",
      "epoch: 63 / 500, time cost: 48.33 sec, \n",
      "          Loss: [Train: 2.34050], [Test: 2.35107],\n",
      "          Accuracy: [prop score:  0.93857], [q1: 0.93656], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18711], [test: 0.18714]\n",
      "********************************************************************************\n",
      "epoch: 64 / 500, time cost: 48.75 sec, \n",
      "          Loss: [Train: 2.33980], [Test: 2.35031],\n",
      "          Accuracy: [prop score:  0.93864], [q1: 0.93658], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18035], [test: 0.18038]\n",
      "********************************************************************************\n",
      "epoch: 65 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.33880], [Test: 2.34981],\n",
      "          Accuracy: [prop score:  0.93877], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18312], [test: 0.18316]\n",
      "********************************************************************************\n",
      "epoch: 66 / 500, time cost: 50.06 sec, \n",
      "          Loss: [Train: 2.33865], [Test: 2.34914],\n",
      "          Accuracy: [prop score:  0.93890], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18440], [test: 0.18444]\n",
      "********************************************************************************\n",
      "epoch: 67 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 2.33724], [Test: 2.34863],\n",
      "          Accuracy: [prop score:  0.93895], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18340], [test: 0.18345]\n",
      "********************************************************************************\n",
      "epoch: 68 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.33707], [Test: 2.34832],\n",
      "          Accuracy: [prop score:  0.93908], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18396], [test: 0.18401]\n",
      "********************************************************************************\n",
      "epoch: 69 / 500, time cost: 48.11 sec, \n",
      "          Loss: [Train: 2.33654], [Test: 2.34780],\n",
      "          Accuracy: [prop score:  0.93920], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18278], [test: 0.18282]\n",
      "********************************************************************************\n",
      "epoch: 70 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.33587], [Test: 2.34708],\n",
      "          Accuracy: [prop score:  0.93925], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18611], [test: 0.18616]\n",
      "********************************************************************************\n",
      "epoch: 71 / 500, time cost: 47.41 sec, \n",
      "          Loss: [Train: 2.33486], [Test: 2.34631],\n",
      "          Accuracy: [prop score:  0.93927], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18192], [test: 0.18198]\n",
      "********************************************************************************\n",
      "epoch: 72 / 500, time cost: 49.86 sec, \n",
      "          Loss: [Train: 2.33416], [Test: 2.34610],\n",
      "          Accuracy: [prop score:  0.93939], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18476], [test: 0.18481]\n",
      "********************************************************************************\n",
      "epoch: 73 / 500, time cost: 46.98 sec, \n",
      "          Loss: [Train: 2.33376], [Test: 2.34540],\n",
      "          Accuracy: [prop score:  0.93945], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.18206], [test: 0.18213]\n",
      "********************************************************************************\n",
      "epoch: 74 / 500, time cost: 49.32 sec, \n",
      "          Loss: [Train: 2.33330], [Test: 2.34515],\n",
      "          Accuracy: [prop score:  0.93947], [q1: 0.93655], [q0: 0.99999],\n",
      "          Effect: [ate-q], [train: 0.18428], [test: 0.18434]\n",
      "********************************************************************************\n",
      "epoch: 75 / 500, time cost: 47.25 sec, \n",
      "          Loss: [Train: 2.33259], [Test: 2.34431],\n",
      "          Accuracy: [prop score:  0.93947], [q1: 0.93649], [q0: 0.99997],\n",
      "          Effect: [ate-q], [train: 0.17947], [test: 0.17953]\n",
      "********************************************************************************\n",
      "epoch: 76 / 500, time cost: 49.80 sec, \n",
      "          Loss: [Train: 2.33209], [Test: 2.34406],\n",
      "          Accuracy: [prop score:  0.93975], [q1: 0.93652], [q0: 0.99995],\n",
      "          Effect: [ate-q], [train: 0.17762], [test: 0.17769]\n",
      "********************************************************************************\n",
      "epoch: 77 / 500, time cost: 47.22 sec, \n",
      "          Loss: [Train: 2.33120], [Test: 2.34350],\n",
      "          Accuracy: [prop score:  0.93972], [q1: 0.93650], [q0: 0.99992],\n",
      "          Effect: [ate-q], [train: 0.18315], [test: 0.18323]\n",
      "********************************************************************************\n",
      "epoch: 78 / 500, time cost: 49.10 sec, \n",
      "          Loss: [Train: 2.33092], [Test: 2.34319],\n",
      "          Accuracy: [prop score:  0.93968], [q1: 0.93640], [q0: 0.99990],\n",
      "          Effect: [ate-q], [train: 0.18274], [test: 0.18283]\n",
      "********************************************************************************\n",
      "epoch: 79 / 500, time cost: 46.42 sec, \n",
      "          Loss: [Train: 2.33013], [Test: 2.34230],\n",
      "          Accuracy: [prop score:  0.93985], [q1: 0.93646], [q0: 0.99988],\n",
      "          Effect: [ate-q], [train: 0.17912], [test: 0.17920]\n",
      "********************************************************************************\n",
      "epoch: 80 / 500, time cost: 48.61 sec, \n",
      "          Loss: [Train: 2.32947], [Test: 2.34150],\n",
      "          Accuracy: [prop score:  0.93992], [q1: 0.93649], [q0: 0.99986],\n",
      "          Effect: [ate-q], [train: 0.17895], [test: 0.17904]\n",
      "********************************************************************************\n",
      "epoch: 81 / 500, time cost: 46.41 sec, \n",
      "          Loss: [Train: 2.32887], [Test: 2.34139],\n",
      "          Accuracy: [prop score:  0.94000], [q1: 0.93652], [q0: 0.99980],\n",
      "          Effect: [ate-q], [train: 0.17851], [test: 0.17861]\n",
      "********************************************************************************\n",
      "epoch: 82 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.32795], [Test: 2.34147],\n",
      "          Accuracy: [prop score:  0.94007], [q1: 0.93646], [q0: 0.99973],\n",
      "          Effect: [ate-q], [train: 0.18253], [test: 0.18264]\n",
      "********************************************************************************\n",
      "epoch: 83 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.32761], [Test: 2.34036],\n",
      "          Accuracy: [prop score:  0.94009], [q1: 0.93646], [q0: 0.99964],\n",
      "          Effect: [ate-q], [train: 0.17664], [test: 0.17674]\n",
      "********************************************************************************\n",
      "epoch: 84 / 500, time cost: 50.48 sec, \n",
      "          Loss: [Train: 2.32709], [Test: 2.34011],\n",
      "          Accuracy: [prop score:  0.94015], [q1: 0.93645], [q0: 0.99952],\n",
      "          Effect: [ate-q], [train: 0.17633], [test: 0.17643]\n",
      "********************************************************************************\n",
      "epoch: 85 / 500, time cost: 47.30 sec, \n",
      "          Loss: [Train: 2.32670], [Test: 2.34017],\n",
      "          Accuracy: [prop score:  0.94030], [q1: 0.93662], [q0: 0.99944],\n",
      "          Effect: [ate-q], [train: 0.17916], [test: 0.17927]\n",
      "********************************************************************************\n",
      "epoch: 86 / 500, time cost: 48.12 sec, \n",
      "          Loss: [Train: 2.32611], [Test: 2.33895],\n",
      "          Accuracy: [prop score:  0.94017], [q1: 0.93636], [q0: 0.99929],\n",
      "          Effect: [ate-q], [train: 0.17672], [test: 0.17682]\n",
      "********************************************************************************\n",
      "epoch: 87 / 500, time cost: 48.80 sec, \n",
      "          Loss: [Train: 2.32561], [Test: 2.33919],\n",
      "          Accuracy: [prop score:  0.94035], [q1: 0.93643], [q0: 0.99908],\n",
      "          Effect: [ate-q], [train: 0.17623], [test: 0.17634]\n",
      "********************************************************************************\n",
      "epoch: 88 / 500, time cost: 47.70 sec, \n",
      "          Loss: [Train: 2.32531], [Test: 2.33874],\n",
      "          Accuracy: [prop score:  0.94032], [q1: 0.93628], [q0: 0.99906],\n",
      "          Effect: [ate-q], [train: 0.17764], [test: 0.17777]\n",
      "********************************************************************************\n",
      "epoch: 89 / 500, time cost: 47.56 sec, \n",
      "          Loss: [Train: 2.32474], [Test: 2.33865],\n",
      "          Accuracy: [prop score:  0.94046], [q1: 0.93619], [q0: 0.99883],\n",
      "          Effect: [ate-q], [train: 0.17335], [test: 0.17346]\n",
      "********************************************************************************\n",
      "epoch: 90 / 500, time cost: 48.38 sec, \n",
      "          Loss: [Train: 2.32434], [Test: 2.33794],\n",
      "          Accuracy: [prop score:  0.94039], [q1: 0.93612], [q0: 0.99862],\n",
      "          Effect: [ate-q], [train: 0.17540], [test: 0.17553]\n",
      "********************************************************************************\n",
      "epoch: 91 / 500, time cost: 46.45 sec, \n",
      "          Loss: [Train: 2.32394], [Test: 2.33766],\n",
      "          Accuracy: [prop score:  0.94047], [q1: 0.93613], [q0: 0.99831],\n",
      "          Effect: [ate-q], [train: 0.17442], [test: 0.17455]\n",
      "********************************************************************************\n",
      "epoch: 92 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.32360], [Test: 2.33729],\n",
      "          Accuracy: [prop score:  0.94056], [q1: 0.93588], [q0: 0.99835],\n",
      "          Effect: [ate-q], [train: 0.17550], [test: 0.17564]\n",
      "********************************************************************************\n",
      "epoch: 93 / 500, time cost: 45.94 sec, \n",
      "          Loss: [Train: 2.32302], [Test: 2.33690],\n",
      "          Accuracy: [prop score:  0.94053], [q1: 0.93546], [q0: 0.99820],\n",
      "          Effect: [ate-q], [train: 0.17247], [test: 0.17259]\n",
      "********************************************************************************\n",
      "epoch: 94 / 500, time cost: 47.73 sec, \n",
      "          Loss: [Train: 2.32229], [Test: 2.33693],\n",
      "          Accuracy: [prop score:  0.94045], [q1: 0.93550], [q0: 0.99777],\n",
      "          Effect: [ate-q], [train: 0.17351], [test: 0.17365]\n",
      "********************************************************************************\n",
      "epoch: 95 / 500, time cost: 46.24 sec, \n",
      "          Loss: [Train: 2.32165], [Test: 2.33646],\n",
      "          Accuracy: [prop score:  0.94055], [q1: 0.93518], [q0: 0.99768],\n",
      "          Effect: [ate-q], [train: 0.17135], [test: 0.17149]\n",
      "********************************************************************************\n",
      "epoch: 96 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.32119], [Test: 2.33617],\n",
      "          Accuracy: [prop score:  0.94061], [q1: 0.93529], [q0: 0.99727],\n",
      "          Effect: [ate-q], [train: 0.17291], [test: 0.17304]\n",
      "********************************************************************************\n",
      "epoch: 97 / 500, time cost: 47.31 sec, \n",
      "          Loss: [Train: 2.32077], [Test: 2.33555],\n",
      "          Accuracy: [prop score:  0.94072], [q1: 0.93476], [q0: 0.99714],\n",
      "          Effect: [ate-q], [train: 0.17183], [test: 0.17195]\n",
      "********************************************************************************\n",
      "epoch: 98 / 500, time cost: 52.35 sec, \n",
      "          Loss: [Train: 2.32044], [Test: 2.33546],\n",
      "          Accuracy: [prop score:  0.94066], [q1: 0.93461], [q0: 0.99687],\n",
      "          Effect: [ate-q], [train: 0.17289], [test: 0.17304]\n",
      "********************************************************************************\n",
      "epoch: 99 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.32004], [Test: 2.33482],\n",
      "          Accuracy: [prop score:  0.94082], [q1: 0.93391], [q0: 0.99671],\n",
      "          Effect: [ate-q], [train: 0.17150], [test: 0.17165]\n",
      "********************************************************************************\n",
      "epoch: 100 / 500, time cost: 49.41 sec, \n",
      "          Loss: [Train: 2.31992], [Test: 2.33510],\n",
      "          Accuracy: [prop score:  0.94099], [q1: 0.93407], [q0: 0.99662],\n",
      "          Effect: [ate-q], [train: 0.17365], [test: 0.17380]\n",
      "********************************************************************************\n",
      "epoch: 101 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.31914], [Test: 2.33429],\n",
      "          Accuracy: [prop score:  0.94092], [q1: 0.93267], [q0: 0.99642],\n",
      "          Effect: [ate-q], [train: 0.16947], [test: 0.16961]\n",
      "********************************************************************************\n",
      "epoch: 102 / 500, time cost: 50.03 sec, \n",
      "          Loss: [Train: 2.31851], [Test: 2.33457],\n",
      "          Accuracy: [prop score:  0.94100], [q1: 0.93296], [q0: 0.99626],\n",
      "          Effect: [ate-q], [train: 0.17219], [test: 0.17236]\n",
      "********************************************************************************\n",
      "epoch: 103 / 500, time cost: 49.85 sec, \n",
      "          Loss: [Train: 2.31853], [Test: 2.33415],\n",
      "          Accuracy: [prop score:  0.94108], [q1: 0.93221], [q0: 0.99638],\n",
      "          Effect: [ate-q], [train: 0.17227], [test: 0.17241]\n",
      "********************************************************************************\n",
      "epoch: 104 / 500, time cost: 48.65 sec, \n",
      "          Loss: [Train: 2.31772], [Test: 2.33330],\n",
      "          Accuracy: [prop score:  0.94110], [q1: 0.93092], [q0: 0.99600],\n",
      "          Effect: [ate-q], [train: 0.16929], [test: 0.16946]\n",
      "********************************************************************************\n",
      "epoch: 105 / 500, time cost: 50.16 sec, \n",
      "          Loss: [Train: 2.31725], [Test: 2.33366],\n",
      "          Accuracy: [prop score:  0.94121], [q1: 0.93209], [q0: 0.99548],\n",
      "          Effect: [ate-q], [train: 0.17262], [test: 0.17279]\n",
      "********************************************************************************\n",
      "epoch: 106 / 500, time cost: 49.54 sec, \n",
      "          Loss: [Train: 2.31737], [Test: 2.33270],\n",
      "          Accuracy: [prop score:  0.94113], [q1: 0.93182], [q0: 0.99532],\n",
      "          Effect: [ate-q], [train: 0.17273], [test: 0.17289]\n",
      "********************************************************************************\n",
      "epoch: 107 / 500, time cost: 47.78 sec, \n",
      "          Loss: [Train: 2.31685], [Test: 2.33296],\n",
      "          Accuracy: [prop score:  0.94126], [q1: 0.92843], [q0: 0.99502],\n",
      "          Effect: [ate-q], [train: 0.16541], [test: 0.16557]\n",
      "********************************************************************************\n",
      "epoch: 108 / 500, time cost: 49.18 sec, \n",
      "          Loss: [Train: 2.31614], [Test: 2.33273],\n",
      "          Accuracy: [prop score:  0.94124], [q1: 0.93051], [q0: 0.99446],\n",
      "          Effect: [ate-q], [train: 0.16976], [test: 0.16994]\n",
      "********************************************************************************\n",
      "epoch: 109 / 500, time cost: 47.65 sec, \n",
      "          Loss: [Train: 2.31600], [Test: 2.33287],\n",
      "          Accuracy: [prop score:  0.94121], [q1: 0.92935], [q0: 0.99466],\n",
      "          Effect: [ate-q], [train: 0.16975], [test: 0.16993]\n",
      "********************************************************************************\n",
      "epoch: 110 / 500, time cost: 50.07 sec, \n",
      "          Loss: [Train: 2.31579], [Test: 2.33196],\n",
      "          Accuracy: [prop score:  0.94141], [q1: 0.92801], [q0: 0.99482],\n",
      "          Effect: [ate-q], [train: 0.16934], [test: 0.16952]\n",
      "********************************************************************************\n",
      "epoch: 111 / 500, time cost: 46.72 sec, \n",
      "          Loss: [Train: 2.31545], [Test: 2.33185],\n",
      "          Accuracy: [prop score:  0.94130], [q1: 0.92697], [q0: 0.99409],\n",
      "          Effect: [ate-q], [train: 0.16730], [test: 0.16748]\n",
      "********************************************************************************\n",
      "epoch: 112 / 500, time cost: 47.73 sec, \n",
      "          Loss: [Train: 2.31467], [Test: 2.33196],\n",
      "          Accuracy: [prop score:  0.94143], [q1: 0.92587], [q0: 0.99349],\n",
      "          Effect: [ate-q], [train: 0.16540], [test: 0.16558]\n",
      "********************************************************************************\n",
      "epoch: 113 / 500, time cost: 47.17 sec, \n",
      "          Loss: [Train: 2.31475], [Test: 2.33135],\n",
      "          Accuracy: [prop score:  0.94149], [q1: 0.92730], [q0: 0.99380],\n",
      "          Effect: [ate-q], [train: 0.16965], [test: 0.16982]\n",
      "********************************************************************************\n",
      "epoch: 114 / 500, time cost: 48.28 sec, \n",
      "          Loss: [Train: 2.31430], [Test: 2.33169],\n",
      "          Accuracy: [prop score:  0.94145], [q1: 0.92588], [q0: 0.99281],\n",
      "          Effect: [ate-q], [train: 0.16632], [test: 0.16651]\n",
      "********************************************************************************\n",
      "epoch: 115 / 500, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.31338], [Test: 2.33131],\n",
      "          Accuracy: [prop score:  0.94152], [q1: 0.92565], [q0: 0.99254],\n",
      "          Effect: [ate-q], [train: 0.16658], [test: 0.16677]\n",
      "********************************************************************************\n",
      "epoch: 116 / 500, time cost: 49.43 sec, \n",
      "          Loss: [Train: 2.31398], [Test: 2.33062],\n",
      "          Accuracy: [prop score:  0.94152], [q1: 0.92449], [q0: 0.99315],\n",
      "          Effect: [ate-q], [train: 0.16825], [test: 0.16843]\n",
      "********************************************************************************\n",
      "epoch: 117 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.31289], [Test: 2.33090],\n",
      "          Accuracy: [prop score:  0.94156], [q1: 0.92691], [q0: 0.99231],\n",
      "          Effect: [ate-q], [train: 0.17087], [test: 0.17107]\n",
      "********************************************************************************\n",
      "epoch: 118 / 500, time cost: 50.23 sec, \n",
      "          Loss: [Train: 2.31239], [Test: 2.33061],\n",
      "          Accuracy: [prop score:  0.94161], [q1: 0.92750], [q0: 0.99219],\n",
      "          Effect: [ate-q], [train: 0.17220], [test: 0.17239]\n",
      "********************************************************************************\n",
      "epoch: 119 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.31268], [Test: 2.33013],\n",
      "          Accuracy: [prop score:  0.94164], [q1: 0.92249], [q0: 0.99227],\n",
      "          Effect: [ate-q], [train: 0.16708], [test: 0.16728]\n",
      "********************************************************************************\n",
      "epoch: 120 / 500, time cost: 49.04 sec, \n",
      "          Loss: [Train: 2.31207], [Test: 2.33028],\n",
      "          Accuracy: [prop score:  0.94164], [q1: 0.92640], [q0: 0.99158],\n",
      "          Effect: [ate-q], [train: 0.17162], [test: 0.17181]\n",
      "********************************************************************************\n",
      "epoch: 121 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.31161], [Test: 2.33027],\n",
      "          Accuracy: [prop score:  0.94167], [q1: 0.92441], [q0: 0.99157],\n",
      "          Effect: [ate-q], [train: 0.17012], [test: 0.17030]\n",
      "********************************************************************************\n",
      "epoch: 122 / 500, time cost: 49.69 sec, \n",
      "          Loss: [Train: 2.31100], [Test: 2.32954],\n",
      "          Accuracy: [prop score:  0.94177], [q1: 0.92086], [q0: 0.99045],\n",
      "          Effect: [ate-q], [train: 0.16415], [test: 0.16434]\n",
      "********************************************************************************\n",
      "epoch: 123 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 2.31088], [Test: 2.32933],\n",
      "          Accuracy: [prop score:  0.94182], [q1: 0.92414], [q0: 0.99113],\n",
      "          Effect: [ate-q], [train: 0.17123], [test: 0.17143]\n",
      "********************************************************************************\n",
      "epoch: 124 / 500, time cost: 49.81 sec, \n",
      "          Loss: [Train: 2.31042], [Test: 2.32956],\n",
      "          Accuracy: [prop score:  0.94189], [q1: 0.92139], [q0: 0.99070],\n",
      "          Effect: [ate-q], [train: 0.16786], [test: 0.16806]\n",
      "********************************************************************************\n",
      "epoch: 125 / 500, time cost: 47.43 sec, \n",
      "          Loss: [Train: 2.31004], [Test: 2.32935],\n",
      "          Accuracy: [prop score:  0.94205], [q1: 0.92032], [q0: 0.99117],\n",
      "          Effect: [ate-q], [train: 0.16863], [test: 0.16882]\n",
      "********************************************************************************\n",
      "epoch: 126 / 500, time cost: 48.11 sec, \n",
      "          Loss: [Train: 2.31004], [Test: 2.32871],\n",
      "          Accuracy: [prop score:  0.94210], [q1: 0.92231], [q0: 0.99039],\n",
      "          Effect: [ate-q], [train: 0.17045], [test: 0.17066]\n",
      "********************************************************************************\n",
      "epoch: 127 / 500, time cost: 47.17 sec, \n",
      "          Loss: [Train: 2.30938], [Test: 2.32878],\n",
      "          Accuracy: [prop score:  0.94213], [q1: 0.91672], [q0: 0.99018],\n",
      "          Effect: [ate-q], [train: 0.16481], [test: 0.16502]\n",
      "********************************************************************************\n",
      "epoch: 128 / 500, time cost: 49.71 sec, \n",
      "          Loss: [Train: 2.30953], [Test: 2.32868],\n",
      "          Accuracy: [prop score:  0.94208], [q1: 0.91847], [q0: 0.98938],\n",
      "          Effect: [ate-q], [train: 0.16587], [test: 0.16610]\n",
      "********************************************************************************\n",
      "epoch: 129 / 500, time cost: 47.11 sec, \n",
      "          Loss: [Train: 2.30874], [Test: 2.32830],\n",
      "          Accuracy: [prop score:  0.94220], [q1: 0.91894], [q0: 0.98951],\n",
      "          Effect: [ate-q], [train: 0.16769], [test: 0.16789]\n",
      "********************************************************************************\n",
      "epoch: 130 / 500, time cost: 48.44 sec, \n",
      "          Loss: [Train: 2.30830], [Test: 2.32808],\n",
      "          Accuracy: [prop score:  0.94225], [q1: 0.91913], [q0: 0.98952],\n",
      "          Effect: [ate-q], [train: 0.16926], [test: 0.16946]\n",
      "********************************************************************************\n",
      "epoch: 131 / 500, time cost: 46.24 sec, \n",
      "          Loss: [Train: 2.30795], [Test: 2.32816],\n",
      "          Accuracy: [prop score:  0.94233], [q1: 0.91020], [q0: 0.98928],\n",
      "          Effect: [ate-q], [train: 0.16083], [test: 0.16105]\n",
      "********************************************************************************\n",
      "epoch: 132 / 500, time cost: 48.97 sec, \n",
      "          Loss: [Train: 2.30759], [Test: 2.32779],\n",
      "          Accuracy: [prop score:  0.94229], [q1: 0.91311], [q0: 0.98902],\n",
      "          Effect: [ate-q], [train: 0.16374], [test: 0.16396]\n",
      "********************************************************************************\n",
      "epoch: 133 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.30773], [Test: 2.32789],\n",
      "          Accuracy: [prop score:  0.94236], [q1: 0.91215], [q0: 0.98867],\n",
      "          Effect: [ate-q], [train: 0.16295], [test: 0.16316]\n",
      "********************************************************************************\n",
      "epoch: 134 / 500, time cost: 49.74 sec, \n",
      "          Loss: [Train: 2.30706], [Test: 2.32776],\n",
      "          Accuracy: [prop score:  0.94246], [q1: 0.91577], [q0: 0.98837],\n",
      "          Effect: [ate-q], [train: 0.16685], [test: 0.16705]\n",
      "********************************************************************************\n",
      "epoch: 135 / 500, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.30682], [Test: 2.32752],\n",
      "          Accuracy: [prop score:  0.94246], [q1: 0.91710], [q0: 0.98801],\n",
      "          Effect: [ate-q], [train: 0.16877], [test: 0.16897]\n",
      "********************************************************************************\n",
      "epoch: 136 / 500, time cost: 48.83 sec, \n",
      "          Loss: [Train: 2.30616], [Test: 2.32723],\n",
      "          Accuracy: [prop score:  0.94250], [q1: 0.91376], [q0: 0.98858],\n",
      "          Effect: [ate-q], [train: 0.16741], [test: 0.16766]\n",
      "********************************************************************************\n",
      "epoch: 137 / 500, time cost: 46.52 sec, \n",
      "          Loss: [Train: 2.30591], [Test: 2.32707],\n",
      "          Accuracy: [prop score:  0.94263], [q1: 0.90976], [q0: 0.98747],\n",
      "          Effect: [ate-q], [train: 0.16192], [test: 0.16213]\n",
      "********************************************************************************\n",
      "epoch: 138 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.30609], [Test: 2.32690],\n",
      "          Accuracy: [prop score:  0.94256], [q1: 0.91081], [q0: 0.98724],\n",
      "          Effect: [ate-q], [train: 0.16342], [test: 0.16364]\n",
      "********************************************************************************\n",
      "epoch: 139 / 500, time cost: 46.24 sec, \n",
      "          Loss: [Train: 2.30551], [Test: 2.32646],\n",
      "          Accuracy: [prop score:  0.94265], [q1: 0.91340], [q0: 0.98707],\n",
      "          Effect: [ate-q], [train: 0.16670], [test: 0.16693]\n",
      "********************************************************************************\n",
      "epoch: 140 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.30531], [Test: 2.32642],\n",
      "          Accuracy: [prop score:  0.94274], [q1: 0.90684], [q0: 0.98767],\n",
      "          Effect: [ate-q], [train: 0.16287], [test: 0.16311]\n",
      "********************************************************************************\n",
      "epoch: 141 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.30449], [Test: 2.32631],\n",
      "          Accuracy: [prop score:  0.94284], [q1: 0.90901], [q0: 0.98751],\n",
      "          Effect: [ate-q], [train: 0.16497], [test: 0.16521]\n",
      "********************************************************************************\n",
      "epoch: 142 / 500, time cost: 48.57 sec, \n",
      "          Loss: [Train: 2.30453], [Test: 2.32603],\n",
      "          Accuracy: [prop score:  0.94280], [q1: 0.90649], [q0: 0.98615],\n",
      "          Effect: [ate-q], [train: 0.16096], [test: 0.16121]\n",
      "********************************************************************************\n",
      "epoch: 143 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.30417], [Test: 2.32548],\n",
      "          Accuracy: [prop score:  0.94287], [q1: 0.90995], [q0: 0.98780],\n",
      "          Effect: [ate-q], [train: 0.16796], [test: 0.16821]\n",
      "********************************************************************************\n",
      "epoch: 144 / 500, time cost: 48.31 sec, \n",
      "          Loss: [Train: 2.30371], [Test: 2.32578],\n",
      "          Accuracy: [prop score:  0.94296], [q1: 0.91073], [q0: 0.98653],\n",
      "          Effect: [ate-q], [train: 0.16678], [test: 0.16701]\n",
      "********************************************************************************\n",
      "epoch: 145 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.30325], [Test: 2.32587],\n",
      "          Accuracy: [prop score:  0.94298], [q1: 0.90245], [q0: 0.98383],\n",
      "          Effect: [ate-q], [train: 0.15621], [test: 0.15646]\n",
      "********************************************************************************\n",
      "epoch: 146 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.30320], [Test: 2.32574],\n",
      "          Accuracy: [prop score:  0.94294], [q1: 0.90676], [q0: 0.98549],\n",
      "          Effect: [ate-q], [train: 0.16310], [test: 0.16336]\n",
      "********************************************************************************\n",
      "epoch: 147 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.30302], [Test: 2.32519],\n",
      "          Accuracy: [prop score:  0.94310], [q1: 0.90461], [q0: 0.98612],\n",
      "          Effect: [ate-q], [train: 0.16298], [test: 0.16322]\n",
      "********************************************************************************\n",
      "epoch: 148 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.30247], [Test: 2.32515],\n",
      "          Accuracy: [prop score:  0.94313], [q1: 0.90262], [q0: 0.98580],\n",
      "          Effect: [ate-q], [train: 0.16160], [test: 0.16184]\n",
      "********************************************************************************\n",
      "epoch: 149 / 500, time cost: 46.74 sec, \n",
      "          Loss: [Train: 2.30189], [Test: 2.32504],\n",
      "          Accuracy: [prop score:  0.94324], [q1: 0.90829], [q0: 0.98571],\n",
      "          Effect: [ate-q], [train: 0.16668], [test: 0.16693]\n",
      "********************************************************************************\n",
      "epoch: 150 / 500, time cost: 48.71 sec, \n",
      "          Loss: [Train: 2.30176], [Test: 2.32480],\n",
      "          Accuracy: [prop score:  0.94341], [q1: 0.90812], [q0: 0.98673],\n",
      "          Effect: [ate-q], [train: 0.16905], [test: 0.16931]\n",
      "********************************************************************************\n",
      "epoch: 151 / 500, time cost: 47.06 sec, \n",
      "          Loss: [Train: 2.30154], [Test: 2.32461],\n",
      "          Accuracy: [prop score:  0.94330], [q1: 0.90257], [q0: 0.98641],\n",
      "          Effect: [ate-q], [train: 0.16475], [test: 0.16502]\n",
      "********************************************************************************\n",
      "epoch: 152 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.30109], [Test: 2.32447],\n",
      "          Accuracy: [prop score:  0.94343], [q1: 0.89802], [q0: 0.98507],\n",
      "          Effect: [ate-q], [train: 0.15938], [test: 0.15964]\n",
      "********************************************************************************\n",
      "epoch: 153 / 500, time cost: 45.97 sec, \n",
      "          Loss: [Train: 2.30079], [Test: 2.32451],\n",
      "          Accuracy: [prop score:  0.94341], [q1: 0.90490], [q0: 0.98192],\n",
      "          Effect: [ate-q], [train: 0.16095], [test: 0.16120]\n",
      "********************************************************************************\n",
      "epoch: 154 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.30029], [Test: 2.32438],\n",
      "          Accuracy: [prop score:  0.94343], [q1: 0.90570], [q0: 0.98455],\n",
      "          Effect: [ate-q], [train: 0.16582], [test: 0.16609]\n",
      "********************************************************************************\n",
      "epoch: 155 / 500, time cost: 46.53 sec, \n",
      "          Loss: [Train: 2.30006], [Test: 2.32431],\n",
      "          Accuracy: [prop score:  0.94354], [q1: 0.90074], [q0: 0.98445],\n",
      "          Effect: [ate-q], [train: 0.16205], [test: 0.16230]\n",
      "********************************************************************************\n",
      "epoch: 156 / 500, time cost: 47.71 sec, \n",
      "          Loss: [Train: 2.30019], [Test: 2.32399],\n",
      "          Accuracy: [prop score:  0.94359], [q1: 0.89465], [q0: 0.98229],\n",
      "          Effect: [ate-q], [train: 0.15520], [test: 0.15544]\n",
      "********************************************************************************\n",
      "epoch: 157 / 500, time cost: 46.45 sec, \n",
      "          Loss: [Train: 2.29955], [Test: 2.32360],\n",
      "          Accuracy: [prop score:  0.94356], [q1: 0.90036], [q0: 0.98347],\n",
      "          Effect: [ate-q], [train: 0.16169], [test: 0.16196]\n",
      "********************************************************************************\n",
      "epoch: 158 / 500, time cost: 48.02 sec, \n",
      "          Loss: [Train: 2.29932], [Test: 2.32354],\n",
      "          Accuracy: [prop score:  0.94361], [q1: 0.89823], [q0: 0.98316],\n",
      "          Effect: [ate-q], [train: 0.16022], [test: 0.16052]\n",
      "********************************************************************************\n",
      "epoch: 159 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.29888], [Test: 2.32407],\n",
      "          Accuracy: [prop score:  0.94375], [q1: 0.89726], [q0: 0.98301],\n",
      "          Effect: [ate-q], [train: 0.15948], [test: 0.15975]\n",
      "********************************************************************************\n",
      "epoch: 160 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.29894], [Test: 2.32368],\n",
      "          Accuracy: [prop score:  0.94383], [q1: 0.89696], [q0: 0.98167],\n",
      "          Effect: [ate-q], [train: 0.15826], [test: 0.15854]\n",
      "********************************************************************************\n",
      "epoch: 161 / 500, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.29819], [Test: 2.32336],\n",
      "          Accuracy: [prop score:  0.94386], [q1: 0.89853], [q0: 0.98332],\n",
      "          Effect: [ate-q], [train: 0.16226], [test: 0.16254]\n",
      "********************************************************************************\n",
      "epoch: 162 / 500, time cost: 48.05 sec, \n",
      "          Loss: [Train: 2.29794], [Test: 2.32344],\n",
      "          Accuracy: [prop score:  0.94388], [q1: 0.89918], [q0: 0.98266],\n",
      "          Effect: [ate-q], [train: 0.16233], [test: 0.16259]\n",
      "********************************************************************************\n",
      "epoch: 163 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.29801], [Test: 2.32346],\n",
      "          Accuracy: [prop score:  0.94390], [q1: 0.89565], [q0: 0.98211],\n",
      "          Effect: [ate-q], [train: 0.15950], [test: 0.15979]\n",
      "********************************************************************************\n",
      "epoch: 164 / 500, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.29733], [Test: 2.32278],\n",
      "          Accuracy: [prop score:  0.94408], [q1: 0.89944], [q0: 0.98380],\n",
      "          Effect: [ate-q], [train: 0.16549], [test: 0.16576]\n",
      "********************************************************************************\n",
      "epoch: 165 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.29682], [Test: 2.32306],\n",
      "          Accuracy: [prop score:  0.94408], [q1: 0.90003], [q0: 0.98210],\n",
      "          Effect: [ate-q], [train: 0.16384], [test: 0.16411]\n",
      "********************************************************************************\n",
      "epoch: 166 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.29665], [Test: 2.32264],\n",
      "          Accuracy: [prop score:  0.94417], [q1: 0.89559], [q0: 0.98202],\n",
      "          Effect: [ate-q], [train: 0.16078], [test: 0.16108]\n",
      "********************************************************************************\n",
      "epoch: 167 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.29631], [Test: 2.32235],\n",
      "          Accuracy: [prop score:  0.94426], [q1: 0.89173], [q0: 0.98281],\n",
      "          Effect: [ate-q], [train: 0.15968], [test: 0.16000]\n",
      "********************************************************************************\n",
      "epoch: 168 / 500, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.29562], [Test: 2.32263],\n",
      "          Accuracy: [prop score:  0.94435], [q1: 0.89135], [q0: 0.98209],\n",
      "          Effect: [ate-q], [train: 0.15884], [test: 0.15915]\n",
      "********************************************************************************\n",
      "epoch: 169 / 500, time cost: 44.94 sec, \n",
      "          Loss: [Train: 2.29593], [Test: 2.32237],\n",
      "          Accuracy: [prop score:  0.94439], [q1: 0.89371], [q0: 0.98166],\n",
      "          Effect: [ate-q], [train: 0.16038], [test: 0.16067]\n",
      "********************************************************************************\n",
      "epoch: 170 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.29493], [Test: 2.32239],\n",
      "          Accuracy: [prop score:  0.94435], [q1: 0.88540], [q0: 0.98137],\n",
      "          Effect: [ate-q], [train: 0.15504], [test: 0.15536]\n",
      "********************************************************************************\n",
      "epoch: 171 / 500, time cost: 43.46 sec, \n",
      "          Loss: [Train: 2.29534], [Test: 2.32203],\n",
      "          Accuracy: [prop score:  0.94444], [q1: 0.88502], [q0: 0.98222],\n",
      "          Effect: [ate-q], [train: 0.15675], [test: 0.15705]\n",
      "********************************************************************************\n",
      "epoch: 172 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.29470], [Test: 2.32164],\n",
      "          Accuracy: [prop score:  0.94446], [q1: 0.88993], [q0: 0.97935],\n",
      "          Effect: [ate-q], [train: 0.15628], [test: 0.15659]\n",
      "********************************************************************************\n",
      "epoch: 173 / 500, time cost: 45.94 sec, \n",
      "          Loss: [Train: 2.29410], [Test: 2.32171],\n",
      "          Accuracy: [prop score:  0.94450], [q1: 0.89599], [q0: 0.98152],\n",
      "          Effect: [ate-q], [train: 0.16399], [test: 0.16428]\n",
      "********************************************************************************\n",
      "epoch: 174 / 500, time cost: 46.49 sec, \n",
      "          Loss: [Train: 2.29395], [Test: 2.32179],\n",
      "          Accuracy: [prop score:  0.94458], [q1: 0.88815], [q0: 0.97964],\n",
      "          Effect: [ate-q], [train: 0.15622], [test: 0.15654]\n",
      "********************************************************************************\n",
      "epoch: 175 / 500, time cost: 45.28 sec, \n",
      "          Loss: [Train: 2.29372], [Test: 2.32155],\n",
      "          Accuracy: [prop score:  0.94463], [q1: 0.88680], [q0: 0.98045],\n",
      "          Effect: [ate-q], [train: 0.15667], [test: 0.15699]\n",
      "********************************************************************************\n",
      "epoch: 176 / 500, time cost: 48.03 sec, \n",
      "          Loss: [Train: 2.29346], [Test: 2.32182],\n",
      "          Accuracy: [prop score:  0.94460], [q1: 0.88670], [q0: 0.98017],\n",
      "          Effect: [ate-q], [train: 0.15653], [test: 0.15686]\n",
      "********************************************************************************\n",
      "epoch: 177 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.29278], [Test: 2.32126],\n",
      "          Accuracy: [prop score:  0.94475], [q1: 0.89345], [q0: 0.98068],\n",
      "          Effect: [ate-q], [train: 0.16253], [test: 0.16285]\n",
      "********************************************************************************\n",
      "epoch: 178 / 500, time cost: 49.39 sec, \n",
      "          Loss: [Train: 2.29254], [Test: 2.32102],\n",
      "          Accuracy: [prop score:  0.94467], [q1: 0.87927], [q0: 0.97895],\n",
      "          Effect: [ate-q], [train: 0.15161], [test: 0.15195]\n",
      "********************************************************************************\n",
      "epoch: 179 / 500, time cost: 46.95 sec, \n",
      "          Loss: [Train: 2.29220], [Test: 2.32136],\n",
      "          Accuracy: [prop score:  0.94491], [q1: 0.89159], [q0: 0.98135],\n",
      "          Effect: [ate-q], [train: 0.16318], [test: 0.16350]\n",
      "********************************************************************************\n",
      "epoch: 180 / 500, time cost: 49.47 sec, \n",
      "          Loss: [Train: 2.29251], [Test: 2.32123],\n",
      "          Accuracy: [prop score:  0.94480], [q1: 0.88372], [q0: 0.98154],\n",
      "          Effect: [ate-q], [train: 0.15866], [test: 0.15899]\n",
      "********************************************************************************\n",
      "epoch: 181 / 500, time cost: 49.80 sec, \n",
      "          Loss: [Train: 2.29208], [Test: 2.32062],\n",
      "          Accuracy: [prop score:  0.94490], [q1: 0.88107], [q0: 0.97962],\n",
      "          Effect: [ate-q], [train: 0.15461], [test: 0.15495]\n",
      "********************************************************************************\n",
      "epoch: 182 / 500, time cost: 50.61 sec, \n",
      "          Loss: [Train: 2.29178], [Test: 2.32088],\n",
      "          Accuracy: [prop score:  0.94488], [q1: 0.88868], [q0: 0.97986],\n",
      "          Effect: [ate-q], [train: 0.16053], [test: 0.16088]\n",
      "********************************************************************************\n",
      "epoch: 183 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.29116], [Test: 2.32088],\n",
      "          Accuracy: [prop score:  0.94495], [q1: 0.88435], [q0: 0.98105],\n",
      "          Effect: [ate-q], [train: 0.15950], [test: 0.15985]\n",
      "********************************************************************************\n",
      "epoch: 184 / 500, time cost: 47.26 sec, \n",
      "          Loss: [Train: 2.29039], [Test: 2.32085],\n",
      "          Accuracy: [prop score:  0.94488], [q1: 0.88377], [q0: 0.98120],\n",
      "          Effect: [ate-q], [train: 0.16008], [test: 0.16044]\n",
      "********************************************************************************\n",
      "epoch: 185 / 500, time cost: 47.19 sec, \n",
      "          Loss: [Train: 2.29024], [Test: 2.32088],\n",
      "          Accuracy: [prop score:  0.94505], [q1: 0.88740], [q0: 0.97930],\n",
      "          Effect: [ate-q], [train: 0.16005], [test: 0.16040]\n",
      "********************************************************************************\n",
      "epoch: 186 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.29041], [Test: 2.32014],\n",
      "          Accuracy: [prop score:  0.94521], [q1: 0.87902], [q0: 0.98083],\n",
      "          Effect: [ate-q], [train: 0.15707], [test: 0.15744]\n",
      "********************************************************************************\n",
      "epoch: 187 / 500, time cost: 48.75 sec, \n",
      "          Loss: [Train: 2.28964], [Test: 2.32009],\n",
      "          Accuracy: [prop score:  0.94519], [q1: 0.88261], [q0: 0.97835],\n",
      "          Effect: [ate-q], [train: 0.15647], [test: 0.15683]\n",
      "********************************************************************************\n",
      "epoch: 188 / 500, time cost: 50.33 sec, \n",
      "          Loss: [Train: 2.28920], [Test: 2.32034],\n",
      "          Accuracy: [prop score:  0.94528], [q1: 0.88592], [q0: 0.97656],\n",
      "          Effect: [ate-q], [train: 0.15740], [test: 0.15776]\n",
      "********************************************************************************\n",
      "epoch: 189 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 2.28885], [Test: 2.32015],\n",
      "          Accuracy: [prop score:  0.94524], [q1: 0.87632], [q0: 0.97873],\n",
      "          Effect: [ate-q], [train: 0.15416], [test: 0.15452]\n",
      "********************************************************************************\n",
      "epoch: 190 / 500, time cost: 46.43 sec, \n",
      "          Loss: [Train: 2.28856], [Test: 2.32004],\n",
      "          Accuracy: [prop score:  0.94543], [q1: 0.87284], [q0: 0.97980],\n",
      "          Effect: [ate-q], [train: 0.15387], [test: 0.15424]\n",
      "********************************************************************************\n",
      "epoch: 191 / 500, time cost: 46.41 sec, \n",
      "          Loss: [Train: 2.28864], [Test: 2.32016],\n",
      "          Accuracy: [prop score:  0.94539], [q1: 0.88802], [q0: 0.97810],\n",
      "          Effect: [ate-q], [train: 0.16180], [test: 0.16217]\n",
      "********************************************************************************\n",
      "epoch: 192 / 500, time cost: 47.37 sec, \n",
      "          Loss: [Train: 2.28836], [Test: 2.31955],\n",
      "          Accuracy: [prop score:  0.94545], [q1: 0.88338], [q0: 0.98134],\n",
      "          Effect: [ate-q], [train: 0.16368], [test: 0.16406]\n",
      "********************************************************************************\n",
      "epoch: 193 / 500, time cost: 46.56 sec, \n",
      "          Loss: [Train: 2.28757], [Test: 2.31959],\n",
      "          Accuracy: [prop score:  0.94559], [q1: 0.88265], [q0: 0.97809],\n",
      "          Effect: [ate-q], [train: 0.15909], [test: 0.15947]\n",
      "********************************************************************************\n",
      "epoch: 194 / 500, time cost: 48.46 sec, \n",
      "          Loss: [Train: 2.28768], [Test: 2.31979],\n",
      "          Accuracy: [prop score:  0.94561], [q1: 0.88466], [q0: 0.97816],\n",
      "          Effect: [ate-q], [train: 0.16081], [test: 0.16119]\n",
      "********************************************************************************\n",
      "epoch: 195 / 500, time cost: 45.01 sec, \n",
      "          Loss: [Train: 2.28687], [Test: 2.31973],\n",
      "          Accuracy: [prop score:  0.94564], [q1: 0.87859], [q0: 0.97593],\n",
      "          Effect: [ate-q], [train: 0.15459], [test: 0.15498]\n",
      "********************************************************************************\n",
      "epoch: 196 / 500, time cost: 46.91 sec, \n",
      "          Loss: [Train: 2.28715], [Test: 2.31980],\n",
      "          Accuracy: [prop score:  0.94565], [q1: 0.88046], [q0: 0.97838],\n",
      "          Effect: [ate-q], [train: 0.15912], [test: 0.15950]\n",
      "********************************************************************************\n",
      "epoch: 197 / 500, time cost: 46.49 sec, \n",
      "          Loss: [Train: 2.28596], [Test: 2.31970],\n",
      "          Accuracy: [prop score:  0.94578], [q1: 0.87800], [q0: 0.97789],\n",
      "          Effect: [ate-q], [train: 0.15721], [test: 0.15759]\n",
      "********************************************************************************\n",
      "epoch: 198 / 500, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.28611], [Test: 2.31967],\n",
      "          Accuracy: [prop score:  0.94579], [q1: 0.87760], [q0: 0.97732],\n",
      "          Effect: [ate-q], [train: 0.15668], [test: 0.15708]\n",
      "********************************************************************************\n",
      "epoch: 199 / 500, time cost: 45.25 sec, \n",
      "          Loss: [Train: 2.28570], [Test: 2.31923],\n",
      "          Accuracy: [prop score:  0.94587], [q1: 0.86809], [q0: 0.97584],\n",
      "          Effect: [ate-q], [train: 0.14997], [test: 0.15038]\n",
      "********************************************************************************\n",
      "epoch: 200 / 500, time cost: 47.22 sec, \n",
      "          Loss: [Train: 2.28544], [Test: 2.31896],\n",
      "          Accuracy: [prop score:  0.94588], [q1: 0.87089], [q0: 0.97764],\n",
      "          Effect: [ate-q], [train: 0.15443], [test: 0.15485]\n",
      "********************************************************************************\n",
      "epoch: 201 / 500, time cost: 44.92 sec, \n",
      "          Loss: [Train: 2.28564], [Test: 2.31946],\n",
      "          Accuracy: [prop score:  0.94597], [q1: 0.87657], [q0: 0.97616],\n",
      "          Effect: [ate-q], [train: 0.15662], [test: 0.15703]\n",
      "********************************************************************************\n",
      "epoch: 202 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.28455], [Test: 2.31930],\n",
      "          Accuracy: [prop score:  0.94602], [q1: 0.87345], [q0: 0.97605],\n",
      "          Effect: [ate-q], [train: 0.15459], [test: 0.15499]\n",
      "********************************************************************************\n",
      "epoch: 203 / 500, time cost: 43.06 sec, \n",
      "          Loss: [Train: 2.28437], [Test: 2.31907],\n",
      "          Accuracy: [prop score:  0.94609], [q1: 0.87302], [q0: 0.97794],\n",
      "          Effect: [ate-q], [train: 0.15705], [test: 0.15749]\n",
      "********************************************************************************\n",
      "epoch: 204 / 500, time cost: 47.08 sec, \n",
      "          Loss: [Train: 2.28379], [Test: 2.31879],\n",
      "          Accuracy: [prop score:  0.94608], [q1: 0.87267], [q0: 0.97612],\n",
      "          Effect: [ate-q], [train: 0.15508], [test: 0.15550]\n",
      "********************************************************************************\n",
      "epoch: 205 / 500, time cost: 46.63 sec, \n",
      "          Loss: [Train: 2.28386], [Test: 2.31878],\n",
      "          Accuracy: [prop score:  0.94606], [q1: 0.86882], [q0: 0.97512],\n",
      "          Effect: [ate-q], [train: 0.15225], [test: 0.15266]\n",
      "********************************************************************************\n",
      "epoch: 206 / 500, time cost: 47.62 sec, \n",
      "          Loss: [Train: 2.28356], [Test: 2.31877],\n",
      "          Accuracy: [prop score:  0.94612], [q1: 0.86895], [q0: 0.97493],\n",
      "          Effect: [ate-q], [train: 0.15215], [test: 0.15256]\n",
      "********************************************************************************\n",
      "epoch: 207 / 500, time cost: 45.97 sec, \n",
      "          Loss: [Train: 2.28313], [Test: 2.31839],\n",
      "          Accuracy: [prop score:  0.94614], [q1: 0.86478], [q0: 0.97495],\n",
      "          Effect: [ate-q], [train: 0.15016], [test: 0.15059]\n",
      "********************************************************************************\n",
      "epoch: 208 / 500, time cost: 48.56 sec, \n",
      "          Loss: [Train: 2.28270], [Test: 2.31905],\n",
      "          Accuracy: [prop score:  0.94626], [q1: 0.85915], [q0: 0.97512],\n",
      "          Effect: [ate-q], [train: 0.14772], [test: 0.14817]\n",
      "********************************************************************************\n",
      "epoch: 209 / 500, time cost: 44.68 sec, \n",
      "          Loss: [Train: 2.28212], [Test: 2.31878],\n",
      "          Accuracy: [prop score:  0.94622], [q1: 0.88044], [q0: 0.97672],\n",
      "          Effect: [ate-q], [train: 0.16327], [test: 0.16370]\n",
      "********************************************************************************\n",
      "epoch: 210 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.28236], [Test: 2.31835],\n",
      "          Accuracy: [prop score:  0.94635], [q1: 0.86537], [q0: 0.97403],\n",
      "          Effect: [ate-q], [train: 0.15099], [test: 0.15145]\n",
      "********************************************************************************\n",
      "epoch: 211 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.28181], [Test: 2.31871],\n",
      "          Accuracy: [prop score:  0.94638], [q1: 0.86930], [q0: 0.97658],\n",
      "          Effect: [ate-q], [train: 0.15648], [test: 0.15693]\n",
      "********************************************************************************\n",
      "epoch: 212 / 500, time cost: 49.13 sec, \n",
      "          Loss: [Train: 2.28097], [Test: 2.31841],\n",
      "          Accuracy: [prop score:  0.94642], [q1: 0.86400], [q0: 0.97727],\n",
      "          Effect: [ate-q], [train: 0.15479], [test: 0.15525]\n",
      "********************************************************************************\n",
      "epoch: 213 / 500, time cost: 46.90 sec, \n",
      "          Loss: [Train: 2.28121], [Test: 2.31869],\n",
      "          Accuracy: [prop score:  0.94649], [q1: 0.86750], [q0: 0.97594],\n",
      "          Effect: [ate-q], [train: 0.15533], [test: 0.15579]\n",
      "********************************************************************************\n",
      "epoch: 214 / 500, time cost: 54.07 sec, \n",
      "          Loss: [Train: 2.28074], [Test: 2.31808],\n",
      "          Accuracy: [prop score:  0.94668], [q1: 0.86918], [q0: 0.97626],\n",
      "          Effect: [ate-q], [train: 0.15721], [test: 0.15767]\n",
      "********************************************************************************\n",
      "epoch: 215 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.28039], [Test: 2.31814],\n",
      "          Accuracy: [prop score:  0.94665], [q1: 0.86335], [q0: 0.97652],\n",
      "          Effect: [ate-q], [train: 0.15454], [test: 0.15499]\n",
      "********************************************************************************\n",
      "epoch: 216 / 500, time cost: 48.33 sec, \n",
      "          Loss: [Train: 2.28008], [Test: 2.31841],\n",
      "          Accuracy: [prop score:  0.94675], [q1: 0.86340], [q0: 0.97461],\n",
      "          Effect: [ate-q], [train: 0.15257], [test: 0.15304]\n",
      "********************************************************************************\n",
      "epoch: 217 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 2.27989], [Test: 2.31847],\n",
      "          Accuracy: [prop score:  0.94676], [q1: 0.86201], [q0: 0.97467],\n",
      "          Effect: [ate-q], [train: 0.15215], [test: 0.15263]\n",
      "********************************************************************************\n",
      "epoch: 218 / 500, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.27936], [Test: 2.31827],\n",
      "          Accuracy: [prop score:  0.94693], [q1: 0.87037], [q0: 0.97589],\n",
      "          Effect: [ate-q], [train: 0.15899], [test: 0.15947]\n",
      "********************************************************************************\n",
      "epoch: 219 / 500, time cost: 44.94 sec, \n",
      "          Loss: [Train: 2.27900], [Test: 2.31788],\n",
      "          Accuracy: [prop score:  0.94681], [q1: 0.86713], [q0: 0.97463],\n",
      "          Effect: [ate-q], [train: 0.15598], [test: 0.15645]\n",
      "********************************************************************************\n",
      "epoch: 220 / 500, time cost: 47.00 sec, \n",
      "          Loss: [Train: 2.27833], [Test: 2.31816],\n",
      "          Accuracy: [prop score:  0.94685], [q1: 0.86411], [q0: 0.97377],\n",
      "          Effect: [ate-q], [train: 0.15323], [test: 0.15370]\n",
      "********************************************************************************\n",
      "epoch: 221 / 500, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.27803], [Test: 2.31796],\n",
      "          Accuracy: [prop score:  0.94703], [q1: 0.86068], [q0: 0.97478],\n",
      "          Effect: [ate-q], [train: 0.15275], [test: 0.15323]\n",
      "********************************************************************************\n",
      "epoch: 222 / 500, time cost: 48.71 sec, \n",
      "          Loss: [Train: 2.27803], [Test: 2.31825],\n",
      "          Accuracy: [prop score:  0.94711], [q1: 0.86467], [q0: 0.97682],\n",
      "          Effect: [ate-q], [train: 0.15792], [test: 0.15839]\n",
      "********************************************************************************\n",
      "epoch: 223 / 500, time cost: 44.93 sec, \n",
      "          Loss: [Train: 2.27772], [Test: 2.31718],\n",
      "          Accuracy: [prop score:  0.94722], [q1: 0.85509], [q0: 0.97666],\n",
      "          Effect: [ate-q], [train: 0.15250], [test: 0.15300]\n",
      "********************************************************************************\n",
      "epoch: 224 / 500, time cost: 47.50 sec, \n",
      "          Loss: [Train: 2.27696], [Test: 2.31750],\n",
      "          Accuracy: [prop score:  0.94718], [q1: 0.85501], [q0: 0.97142],\n",
      "          Effect: [ate-q], [train: 0.14698], [test: 0.14748]\n",
      "********************************************************************************\n",
      "epoch: 225 / 500, time cost: 45.06 sec, \n",
      "          Loss: [Train: 2.27684], [Test: 2.31800],\n",
      "          Accuracy: [prop score:  0.94721], [q1: 0.85983], [q0: 0.97709],\n",
      "          Effect: [ate-q], [train: 0.15698], [test: 0.15749]\n",
      "********************************************************************************\n",
      "epoch: 226 / 500, time cost: 47.53 sec, \n",
      "          Loss: [Train: 2.27649], [Test: 2.31755],\n",
      "          Accuracy: [prop score:  0.94732], [q1: 0.86080], [q0: 0.97436],\n",
      "          Effect: [ate-q], [train: 0.15421], [test: 0.15472]\n",
      "********************************************************************************\n",
      "epoch: 227 / 500, time cost: 44.29 sec, \n",
      "          Loss: [Train: 2.27621], [Test: 2.31788],\n",
      "          Accuracy: [prop score:  0.94745], [q1: 0.85660], [q0: 0.97446],\n",
      "          Effect: [ate-q], [train: 0.15220], [test: 0.15272]\n",
      "********************************************************************************\n",
      "epoch: 228 / 500, time cost: 47.74 sec, \n",
      "          Loss: [Train: 2.27577], [Test: 2.31784],\n",
      "          Accuracy: [prop score:  0.94751], [q1: 0.85049], [q0: 0.97264],\n",
      "          Effect: [ate-q], [train: 0.14744], [test: 0.14797]\n",
      "********************************************************************************\n",
      "epoch: 229 / 500, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.27513], [Test: 2.31781],\n",
      "          Accuracy: [prop score:  0.94758], [q1: 0.86719], [q0: 0.97672],\n",
      "          Effect: [ate-q], [train: 0.16232], [test: 0.16287]\n",
      "********************************************************************************\n",
      "epoch: 230 / 500, time cost: 47.68 sec, \n",
      "          Loss: [Train: 2.27467], [Test: 2.31729],\n",
      "          Accuracy: [prop score:  0.94759], [q1: 0.84981], [q0: 0.97708],\n",
      "          Effect: [ate-q], [train: 0.15301], [test: 0.15354]\n",
      "********************************************************************************\n",
      "epoch: 231 / 500, time cost: 48.88 sec, \n",
      "          Loss: [Train: 2.27487], [Test: 2.31739],\n",
      "          Accuracy: [prop score:  0.94764], [q1: 0.86157], [q0: 0.97448],\n",
      "          Effect: [ate-q], [train: 0.15684], [test: 0.15736]\n",
      "********************************************************************************\n",
      "epoch: 232 / 500, time cost: 48.61 sec, \n",
      "          Loss: [Train: 2.27441], [Test: 2.31735],\n",
      "          Accuracy: [prop score:  0.94766], [q1: 0.84439], [q0: 0.97379],\n",
      "          Effect: [ate-q], [train: 0.14669], [test: 0.14724]\n",
      "********************************************************************************\n",
      "epoch: 233 / 500, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.27435], [Test: 2.31756],\n",
      "          Accuracy: [prop score:  0.94769], [q1: 0.84731], [q0: 0.97709],\n",
      "          Effect: [ate-q], [train: 0.15281], [test: 0.15333]\n",
      "********************************************************************************\n",
      "epoch: 234 / 500, time cost: 46.85 sec, \n",
      "          Loss: [Train: 2.27391], [Test: 2.31703],\n",
      "          Accuracy: [prop score:  0.94774], [q1: 0.84287], [q0: 0.97417],\n",
      "          Effect: [ate-q], [train: 0.14697], [test: 0.14750]\n",
      "********************************************************************************\n",
      "epoch: 235 / 500, time cost: 44.89 sec, \n",
      "          Loss: [Train: 2.27281], [Test: 2.31751],\n",
      "          Accuracy: [prop score:  0.94782], [q1: 0.86182], [q0: 0.97545],\n",
      "          Effect: [ate-q], [train: 0.15956], [test: 0.16010]\n",
      "********************************************************************************\n",
      "epoch: 236 / 500, time cost: 51.79 sec, \n",
      "          Loss: [Train: 2.27291], [Test: 2.31728],\n",
      "          Accuracy: [prop score:  0.94781], [q1: 0.84945], [q0: 0.97319],\n",
      "          Effect: [ate-q], [train: 0.14981], [test: 0.15038]\n",
      "********************************************************************************\n",
      "epoch: 237 / 500, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.27258], [Test: 2.31722],\n",
      "          Accuracy: [prop score:  0.94790], [q1: 0.84568], [q0: 0.97493],\n",
      "          Effect: [ate-q], [train: 0.15074], [test: 0.15131]\n",
      "********************************************************************************\n",
      "epoch: 238 / 500, time cost: 47.78 sec, \n",
      "          Loss: [Train: 2.27216], [Test: 2.31739],\n",
      "          Accuracy: [prop score:  0.94791], [q1: 0.83302], [q0: 0.97469],\n",
      "          Effect: [ate-q], [train: 0.14403], [test: 0.14460]\n",
      "********************************************************************************\n",
      "epoch: 239 / 500, time cost: 46.47 sec, \n",
      "          Loss: [Train: 2.27183], [Test: 2.31701],\n",
      "          Accuracy: [prop score:  0.94793], [q1: 0.85369], [q0: 0.97548],\n",
      "          Effect: [ate-q], [train: 0.15580], [test: 0.15636]\n",
      "********************************************************************************\n",
      "epoch: 240 / 500, time cost: 47.02 sec, \n",
      "          Loss: [Train: 2.27105], [Test: 2.31662],\n",
      "          Accuracy: [prop score:  0.94801], [q1: 0.84813], [q0: 0.97436],\n",
      "          Effect: [ate-q], [train: 0.15181], [test: 0.15237]\n",
      "********************************************************************************\n",
      "epoch: 241 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.27092], [Test: 2.31735],\n",
      "          Accuracy: [prop score:  0.94794], [q1: 0.84324], [q0: 0.97018],\n",
      "          Effect: [ate-q], [train: 0.14499], [test: 0.14558]\n",
      "********************************************************************************\n",
      "epoch: 242 / 500, time cost: 48.55 sec, \n",
      "          Loss: [Train: 2.27068], [Test: 2.31753],\n",
      "          Accuracy: [prop score:  0.94814], [q1: 0.85305], [q0: 0.97374],\n",
      "          Effect: [ate-q], [train: 0.15452], [test: 0.15508]\n",
      "********************************************************************************\n",
      "epoch: 243 / 500, time cost: 46.89 sec, \n",
      "          Loss: [Train: 2.27003], [Test: 2.31676],\n",
      "          Accuracy: [prop score:  0.94820], [q1: 0.83992], [q0: 0.97280],\n",
      "          Effect: [ate-q], [train: 0.14699], [test: 0.14758]\n",
      "********************************************************************************\n",
      "epoch: 244 / 500, time cost: 48.60 sec, \n",
      "          Loss: [Train: 2.27006], [Test: 2.31689],\n",
      "          Accuracy: [prop score:  0.94824], [q1: 0.84632], [q0: 0.97491],\n",
      "          Effect: [ate-q], [train: 0.15323], [test: 0.15382]\n",
      "********************************************************************************\n",
      "epoch: 245 / 500, time cost: 44.63 sec, \n",
      "          Loss: [Train: 2.26978], [Test: 2.31746],\n",
      "          Accuracy: [prop score:  0.94825], [q1: 0.85364], [q0: 0.97486],\n",
      "          Effect: [ate-q], [train: 0.15788], [test: 0.15849]\n",
      "********************************************************************************\n",
      "epoch: 246 / 500, time cost: 45.06 sec, \n",
      "          Loss: [Train: 2.26951], [Test: 2.31728],\n",
      "          Accuracy: [prop score:  0.94834], [q1: 0.82935], [q0: 0.97135],\n",
      "          Effect: [ate-q], [train: 0.14067], [test: 0.14128]\n",
      "********************************************************************************\n",
      "epoch: 247 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 2.26908], [Test: 2.31687],\n",
      "          Accuracy: [prop score:  0.94838], [q1: 0.84420], [q0: 0.97313],\n",
      "          Effect: [ate-q], [train: 0.15075], [test: 0.15136]\n",
      "********************************************************************************\n",
      "epoch: 248 / 500, time cost: 46.57 sec, \n",
      "          Loss: [Train: 2.26859], [Test: 2.31679],\n",
      "          Accuracy: [prop score:  0.94838], [q1: 0.84480], [q0: 0.97457],\n",
      "          Effect: [ate-q], [train: 0.15352], [test: 0.15414]\n",
      "********************************************************************************\n",
      "epoch: 249 / 500, time cost: 45.02 sec, \n",
      "          Loss: [Train: 2.26751], [Test: 2.31704],\n",
      "          Accuracy: [prop score:  0.94850], [q1: 0.85244], [q0: 0.97536],\n",
      "          Effect: [ate-q], [train: 0.15966], [test: 0.16027]\n",
      "********************************************************************************\n",
      "epoch: 250 / 500, time cost: 47.75 sec, \n",
      "          Loss: [Train: 2.26768], [Test: 2.31672],\n",
      "          Accuracy: [prop score:  0.94852], [q1: 0.83784], [q0: 0.97123],\n",
      "          Effect: [ate-q], [train: 0.14656], [test: 0.14720]\n",
      "********************************************************************************\n",
      "epoch: 251 / 500, time cost: 44.65 sec, \n",
      "          Loss: [Train: 2.26765], [Test: 2.31645],\n",
      "          Accuracy: [prop score:  0.94860], [q1: 0.84294], [q0: 0.97411],\n",
      "          Effect: [ate-q], [train: 0.15286], [test: 0.15349]\n",
      "********************************************************************************\n",
      "epoch: 252 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.26718], [Test: 2.31696],\n",
      "          Accuracy: [prop score:  0.94862], [q1: 0.83874], [q0: 0.97356],\n",
      "          Effect: [ate-q], [train: 0.15020], [test: 0.15084]\n",
      "********************************************************************************\n",
      "epoch: 253 / 500, time cost: 44.21 sec, \n",
      "          Loss: [Train: 2.26656], [Test: 2.31697],\n",
      "          Accuracy: [prop score:  0.94869], [q1: 0.82933], [q0: 0.97657],\n",
      "          Effect: [ate-q], [train: 0.14979], [test: 0.15040]\n",
      "********************************************************************************\n",
      "epoch: 254 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.26595], [Test: 2.31724],\n",
      "          Accuracy: [prop score:  0.94878], [q1: 0.86027], [q0: 0.97259],\n",
      "          Effect: [ate-q], [train: 0.16268], [test: 0.16333]\n",
      "********************************************************************************\n",
      "epoch: 255 / 500, time cost: 43.85 sec, \n",
      "          Loss: [Train: 2.26592], [Test: 2.31690],\n",
      "          Accuracy: [prop score:  0.94884], [q1: 0.83091], [q0: 0.97153],\n",
      "          Effect: [ate-q], [train: 0.14479], [test: 0.14545]\n",
      "********************************************************************************\n",
      "epoch: 256 / 500, time cost: 46.97 sec, \n",
      "          Loss: [Train: 2.26528], [Test: 2.31690],\n",
      "          Accuracy: [prop score:  0.94887], [q1: 0.82260], [q0: 0.97153],\n",
      "          Effect: [ate-q], [train: 0.14107], [test: 0.14171]\n",
      "********************************************************************************\n",
      "epoch: 257 / 500, time cost: 43.72 sec, \n",
      "          Loss: [Train: 2.26522], [Test: 2.31721],\n",
      "          Accuracy: [prop score:  0.94884], [q1: 0.84942], [q0: 0.97322],\n",
      "          Effect: [ate-q], [train: 0.15784], [test: 0.15852]\n",
      "********************************************************************************\n",
      "epoch: 258 / 500, time cost: 47.19 sec, \n",
      "          Loss: [Train: 2.26451], [Test: 2.31725],\n",
      "          Accuracy: [prop score:  0.94892], [q1: 0.82264], [q0: 0.97475],\n",
      "          Effect: [ate-q], [train: 0.14554], [test: 0.14620]\n",
      "********************************************************************************\n",
      "epoch: 259 / 500, time cost: 45.17 sec, \n",
      "          Loss: [Train: 2.26428], [Test: 2.31785],\n",
      "          Accuracy: [prop score:  0.94889], [q1: 0.83254], [q0: 0.97296],\n",
      "          Effect: [ate-q], [train: 0.14870], [test: 0.14935]\n",
      "********************************************************************************\n",
      "epoch: 260 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.26416], [Test: 2.31768],\n",
      "          Accuracy: [prop score:  0.94898], [q1: 0.84338], [q0: 0.96877],\n",
      "          Effect: [ate-q], [train: 0.15049], [test: 0.15118]\n",
      "********************************************************************************\n",
      "epoch: 261 / 500, time cost: 46.30 sec, \n",
      "          Loss: [Train: 2.26366], [Test: 2.31706],\n",
      "          Accuracy: [prop score:  0.94909], [q1: 0.83551], [q0: 0.97481],\n",
      "          Effect: [ate-q], [train: 0.15320], [test: 0.15388]\n",
      "********************************************************************************\n",
      "epoch: 262 / 500, time cost: 47.17 sec, \n",
      "          Loss: [Train: 2.26314], [Test: 2.31735],\n",
      "          Accuracy: [prop score:  0.94902], [q1: 0.83721], [q0: 0.97135],\n",
      "          Effect: [ate-q], [train: 0.15044], [test: 0.15113]\n",
      "********************************************************************************\n",
      "epoch: 263 / 500, time cost: 46.04 sec, \n",
      "          Loss: [Train: 2.26250], [Test: 2.31750],\n",
      "          Accuracy: [prop score:  0.94916], [q1: 0.82116], [q0: 0.97052],\n",
      "          Effect: [ate-q], [train: 0.14168], [test: 0.14238]\n",
      "********************************************************************************\n",
      "epoch: 264 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.26209], [Test: 2.31683],\n",
      "          Accuracy: [prop score:  0.94922], [q1: 0.82565], [q0: 0.97297],\n",
      "          Effect: [ate-q], [train: 0.14665], [test: 0.14734]\n",
      "********************************************************************************\n",
      "epoch: 265 / 500, time cost: 46.15 sec, \n",
      "          Loss: [Train: 2.26235], [Test: 2.31703],\n",
      "          Accuracy: [prop score:  0.94919], [q1: 0.83800], [q0: 0.97143],\n",
      "          Effect: [ate-q], [train: 0.15191], [test: 0.15261]\n",
      "********************************************************************************\n",
      "epoch: 266 / 500, time cost: 48.02 sec, \n",
      "          Loss: [Train: 2.26156], [Test: 2.31742],\n",
      "          Accuracy: [prop score:  0.94921], [q1: 0.82045], [q0: 0.97188],\n",
      "          Effect: [ate-q], [train: 0.14362], [test: 0.14433]\n",
      "********************************************************************************\n",
      "epoch: 267 / 500, time cost: 45.38 sec, \n",
      "          Loss: [Train: 2.26142], [Test: 2.31709],\n",
      "          Accuracy: [prop score:  0.94938], [q1: 0.81416], [q0: 0.97225],\n",
      "          Effect: [ate-q], [train: 0.14131], [test: 0.14202]\n",
      "********************************************************************************\n",
      "epoch: 268 / 500, time cost: 47.49 sec, \n",
      "          Loss: [Train: 2.26086], [Test: 2.31722],\n",
      "          Accuracy: [prop score:  0.94931], [q1: 0.83820], [q0: 0.97131],\n",
      "          Effect: [ate-q], [train: 0.15319], [test: 0.15392]\n",
      "********************************************************************************\n",
      "epoch: 269 / 500, time cost: 44.86 sec, \n",
      "          Loss: [Train: 2.26029], [Test: 2.31753],\n",
      "          Accuracy: [prop score:  0.94939], [q1: 0.81274], [q0: 0.96925],\n",
      "          Effect: [ate-q], [train: 0.13826], [test: 0.13902]\n",
      "********************************************************************************\n",
      "epoch: 270 / 500, time cost: 48.00 sec, \n",
      "          Loss: [Train: 2.25938], [Test: 2.31721],\n",
      "          Accuracy: [prop score:  0.94949], [q1: 0.82793], [q0: 0.97200],\n",
      "          Effect: [ate-q], [train: 0.14904], [test: 0.14977]\n",
      "********************************************************************************\n",
      "epoch: 271 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.25992], [Test: 2.31765],\n",
      "          Accuracy: [prop score:  0.94948], [q1: 0.83349], [q0: 0.96620],\n",
      "          Effect: [ate-q], [train: 0.14703], [test: 0.14776]\n",
      "********************************************************************************\n",
      "epoch: 272 / 500, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.25929], [Test: 2.31722],\n",
      "          Accuracy: [prop score:  0.94954], [q1: 0.82337], [q0: 0.97467],\n",
      "          Effect: [ate-q], [train: 0.15080], [test: 0.15151]\n",
      "********************************************************************************\n",
      "epoch: 273 / 500, time cost: 43.92 sec, \n",
      "          Loss: [Train: 2.25853], [Test: 2.31727],\n",
      "          Accuracy: [prop score:  0.94959], [q1: 0.81965], [q0: 0.97260],\n",
      "          Effect: [ate-q], [train: 0.14665], [test: 0.14738]\n",
      "********************************************************************************\n",
      "epoch: 274 / 500, time cost: 46.32 sec, \n",
      "          Loss: [Train: 2.25852], [Test: 2.31799],\n",
      "          Accuracy: [prop score:  0.94961], [q1: 0.84376], [q0: 0.97103],\n",
      "          Effect: [ate-q], [train: 0.15830], [test: 0.15906]\n",
      "********************************************************************************\n",
      "epoch: 275 / 500, time cost: 43.16 sec, \n",
      "          Loss: [Train: 2.25812], [Test: 2.31772],\n",
      "          Accuracy: [prop score:  0.94971], [q1: 0.83561], [q0: 0.97061],\n",
      "          Effect: [ate-q], [train: 0.15328], [test: 0.15403]\n",
      "********************************************************************************\n",
      "epoch: 276 / 500, time cost: 46.67 sec, \n",
      "          Loss: [Train: 2.25763], [Test: 2.31762],\n",
      "          Accuracy: [prop score:  0.94975], [q1: 0.82632], [q0: 0.97184],\n",
      "          Effect: [ate-q], [train: 0.14986], [test: 0.15063]\n",
      "********************************************************************************\n",
      "epoch: 277 / 500, time cost: 45.24 sec, \n",
      "          Loss: [Train: 2.25708], [Test: 2.31749],\n",
      "          Accuracy: [prop score:  0.94977], [q1: 0.81352], [q0: 0.96853],\n",
      "          Effect: [ate-q], [train: 0.14066], [test: 0.14145]\n",
      "********************************************************************************\n",
      "epoch: 278 / 500, time cost: 46.70 sec, \n",
      "          Loss: [Train: 2.25721], [Test: 2.31786],\n",
      "          Accuracy: [prop score:  0.94990], [q1: 0.83117], [q0: 0.96910],\n",
      "          Effect: [ate-q], [train: 0.15059], [test: 0.15138]\n",
      "********************************************************************************\n",
      "epoch: 279 / 500, time cost: 44.77 sec, \n",
      "          Loss: [Train: 2.25631], [Test: 2.31791],\n",
      "          Accuracy: [prop score:  0.94995], [q1: 0.80121], [q0: 0.96700],\n",
      "          Effect: [ate-q], [train: 0.13453], [test: 0.13528]\n",
      "********************************************************************************\n",
      "epoch: 280 / 500, time cost: 47.03 sec, \n",
      "          Loss: [Train: 2.25576], [Test: 2.31720],\n",
      "          Accuracy: [prop score:  0.94992], [q1: 0.82323], [q0: 0.96950],\n",
      "          Effect: [ate-q], [train: 0.14748], [test: 0.14824]\n",
      "********************************************************************************\n",
      "epoch: 281 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.25547], [Test: 2.31777],\n",
      "          Accuracy: [prop score:  0.95004], [q1: 0.82195], [q0: 0.97106],\n",
      "          Effect: [ate-q], [train: 0.14899], [test: 0.14978]\n",
      "********************************************************************************\n",
      "epoch: 282 / 500, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.25499], [Test: 2.31764],\n",
      "          Accuracy: [prop score:  0.95008], [q1: 0.82612], [q0: 0.97004],\n",
      "          Effect: [ate-q], [train: 0.15046], [test: 0.15125]\n",
      "********************************************************************************\n",
      "epoch: 283 / 500, time cost: 46.28 sec, \n",
      "          Loss: [Train: 2.25426], [Test: 2.31775],\n",
      "          Accuracy: [prop score:  0.95013], [q1: 0.80615], [q0: 0.97064],\n",
      "          Effect: [ate-q], [train: 0.14145], [test: 0.14226]\n",
      "********************************************************************************\n",
      "epoch: 284 / 500, time cost: 47.86 sec, \n",
      "          Loss: [Train: 2.25413], [Test: 2.31822],\n",
      "          Accuracy: [prop score:  0.95016], [q1: 0.81538], [q0: 0.97034],\n",
      "          Effect: [ate-q], [train: 0.14580], [test: 0.14657]\n",
      "********************************************************************************\n",
      "epoch: 285 / 500, time cost: 45.82 sec, \n",
      "          Loss: [Train: 2.25413], [Test: 2.31810],\n",
      "          Accuracy: [prop score:  0.95023], [q1: 0.81329], [q0: 0.96895],\n",
      "          Effect: [ate-q], [train: 0.14387], [test: 0.14468]\n",
      "********************************************************************************\n",
      "epoch: 286 / 500, time cost: 46.87 sec, \n",
      "          Loss: [Train: 2.25385], [Test: 2.31839],\n",
      "          Accuracy: [prop score:  0.95030], [q1: 0.81869], [q0: 0.96782],\n",
      "          Effect: [ate-q], [train: 0.14573], [test: 0.14655]\n",
      "********************************************************************************\n",
      "epoch: 287 / 500, time cost: 44.84 sec, \n",
      "          Loss: [Train: 2.25267], [Test: 2.31835],\n",
      "          Accuracy: [prop score:  0.95026], [q1: 0.82893], [q0: 0.97186],\n",
      "          Effect: [ate-q], [train: 0.15615], [test: 0.15699]\n",
      "********************************************************************************\n",
      "epoch: 288 / 500, time cost: 47.44 sec, \n",
      "          Loss: [Train: 2.25292], [Test: 2.31797],\n",
      "          Accuracy: [prop score:  0.95030], [q1: 0.81038], [q0: 0.97210],\n",
      "          Effect: [ate-q], [train: 0.14699], [test: 0.14780]\n",
      "********************************************************************************\n",
      "epoch: 289 / 500, time cost: 45.68 sec, \n",
      "          Loss: [Train: 2.25223], [Test: 2.31804],\n",
      "          Accuracy: [prop score:  0.95030], [q1: 0.80279], [q0: 0.97044],\n",
      "          Effect: [ate-q], [train: 0.14187], [test: 0.14273]\n",
      "********************************************************************************\n",
      "epoch: 290 / 500, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.25214], [Test: 2.31839],\n",
      "          Accuracy: [prop score:  0.95035], [q1: 0.80620], [q0: 0.96482],\n",
      "          Effect: [ate-q], [train: 0.13833], [test: 0.13918]\n",
      "********************************************************************************\n",
      "epoch: 291 / 500, time cost: 44.82 sec, \n",
      "          Loss: [Train: 2.25116], [Test: 2.31797],\n",
      "          Accuracy: [prop score:  0.95035], [q1: 0.80543], [q0: 0.97453],\n",
      "          Effect: [ate-q], [train: 0.14828], [test: 0.14915]\n",
      "********************************************************************************\n",
      "epoch: 292 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.25055], [Test: 2.31847],\n",
      "          Accuracy: [prop score:  0.95050], [q1: 0.80678], [q0: 0.96968],\n",
      "          Effect: [ate-q], [train: 0.14386], [test: 0.14469]\n",
      "********************************************************************************\n",
      "epoch: 293 / 500, time cost: 44.85 sec, \n",
      "          Loss: [Train: 2.25057], [Test: 2.31849],\n",
      "          Accuracy: [prop score:  0.95045], [q1: 0.80563], [q0: 0.96998],\n",
      "          Effect: [ate-q], [train: 0.14428], [test: 0.14515]\n",
      "********************************************************************************\n",
      "epoch: 294 / 500, time cost: 47.20 sec, \n",
      "          Loss: [Train: 2.24990], [Test: 2.31842],\n",
      "          Accuracy: [prop score:  0.95048], [q1: 0.80290], [q0: 0.96575],\n",
      "          Effect: [ate-q], [train: 0.13921], [test: 0.14004]\n",
      "********************************************************************************\n",
      "epoch: 295 / 500, time cost: 44.79 sec, \n",
      "          Loss: [Train: 2.24966], [Test: 2.31920],\n",
      "          Accuracy: [prop score:  0.95055], [q1: 0.82108], [q0: 0.96919],\n",
      "          Effect: [ate-q], [train: 0.15205], [test: 0.15290]\n",
      "********************************************************************************\n",
      "epoch: 296 / 500, time cost: 46.59 sec, \n",
      "          Loss: [Train: 2.24924], [Test: 2.31861],\n",
      "          Accuracy: [prop score:  0.95061], [q1: 0.81957], [q0: 0.97172],\n",
      "          Effect: [ate-q], [train: 0.15427], [test: 0.15514]\n",
      "********************************************************************************\n",
      "epoch: 297 / 500, time cost: 44.73 sec, \n",
      "          Loss: [Train: 2.24875], [Test: 2.31884],\n",
      "          Accuracy: [prop score:  0.95062], [q1: 0.79368], [q0: 0.96997],\n",
      "          Effect: [ate-q], [train: 0.14013], [test: 0.14101]\n",
      "********************************************************************************\n",
      "epoch: 298 / 500, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.24875], [Test: 2.31936],\n",
      "          Accuracy: [prop score:  0.95071], [q1: 0.78220], [q0: 0.96344],\n",
      "          Effect: [ate-q], [train: 0.12969], [test: 0.13057]\n",
      "********************************************************************************\n",
      "epoch: 299 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 2.24776], [Test: 2.31917],\n",
      "          Accuracy: [prop score:  0.95066], [q1: 0.79607], [q0: 0.96646],\n",
      "          Effect: [ate-q], [train: 0.13848], [test: 0.13935]\n",
      "********************************************************************************\n",
      "epoch: 300 / 500, time cost: 49.59 sec, \n",
      "          Loss: [Train: 2.24820], [Test: 2.31881],\n",
      "          Accuracy: [prop score:  0.95072], [q1: 0.78738], [q0: 0.96761],\n",
      "          Effect: [ate-q], [train: 0.13600], [test: 0.13688]\n",
      "********************************************************************************\n",
      "epoch: 301 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 2.24750], [Test: 2.31907],\n",
      "          Accuracy: [prop score:  0.95071], [q1: 0.79293], [q0: 0.96820],\n",
      "          Effect: [ate-q], [train: 0.13934], [test: 0.14023]\n",
      "********************************************************************************\n",
      "epoch: 302 / 500, time cost: 47.02 sec, \n",
      "          Loss: [Train: 2.24689], [Test: 2.31906],\n",
      "          Accuracy: [prop score:  0.95076], [q1: 0.80448], [q0: 0.96866],\n",
      "          Effect: [ate-q], [train: 0.14577], [test: 0.14668]\n",
      "********************************************************************************\n",
      "epoch: 303 / 500, time cost: 45.27 sec, \n",
      "          Loss: [Train: 2.24660], [Test: 2.31900],\n",
      "          Accuracy: [prop score:  0.95080], [q1: 0.79887], [q0: 0.96918],\n",
      "          Effect: [ate-q], [train: 0.14389], [test: 0.14480]\n",
      "********************************************************************************\n",
      "epoch: 304 / 500, time cost: 47.43 sec, \n",
      "          Loss: [Train: 2.24635], [Test: 2.31969],\n",
      "          Accuracy: [prop score:  0.95091], [q1: 0.79595], [q0: 0.96984],\n",
      "          Effect: [ate-q], [train: 0.14354], [test: 0.14444]\n",
      "********************************************************************************\n",
      "epoch: 305 / 500, time cost: 45.17 sec, \n",
      "          Loss: [Train: 2.24531], [Test: 2.31953],\n",
      "          Accuracy: [prop score:  0.95091], [q1: 0.79954], [q0: 0.96635],\n",
      "          Effect: [ate-q], [train: 0.14206], [test: 0.14298]\n",
      "********************************************************************************\n",
      "epoch: 306 / 500, time cost: 47.97 sec, \n",
      "          Loss: [Train: 2.24526], [Test: 2.31982],\n",
      "          Accuracy: [prop score:  0.95090], [q1: 0.80376], [q0: 0.96507],\n",
      "          Effect: [ate-q], [train: 0.14331], [test: 0.14423]\n",
      "********************************************************************************\n",
      "epoch: 307 / 500, time cost: 45.14 sec, \n",
      "          Loss: [Train: 2.24470], [Test: 2.31935],\n",
      "          Accuracy: [prop score:  0.95098], [q1: 0.81120], [q0: 0.96391],\n",
      "          Effect: [ate-q], [train: 0.14649], [test: 0.14743]\n",
      "********************************************************************************\n",
      "epoch: 308 / 500, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.24400], [Test: 2.31959],\n",
      "          Accuracy: [prop score:  0.95092], [q1: 0.78996], [q0: 0.96810],\n",
      "          Effect: [ate-q], [train: 0.14052], [test: 0.14149]\n",
      "********************************************************************************\n",
      "epoch: 309 / 500, time cost: 46.89 sec, \n",
      "          Loss: [Train: 2.24407], [Test: 2.31951],\n",
      "          Accuracy: [prop score:  0.95112], [q1: 0.78112], [q0: 0.97047],\n",
      "          Effect: [ate-q], [train: 0.13959], [test: 0.14050]\n",
      "********************************************************************************\n",
      "epoch: 310 / 500, time cost: 47.37 sec, \n",
      "          Loss: [Train: 2.24332], [Test: 2.31999],\n",
      "          Accuracy: [prop score:  0.95101], [q1: 0.81055], [q0: 0.96537],\n",
      "          Effect: [ate-q], [train: 0.14819], [test: 0.14912]\n",
      "********************************************************************************\n",
      "epoch: 311 / 500, time cost: 46.68 sec, \n",
      "          Loss: [Train: 2.24271], [Test: 2.31992],\n",
      "          Accuracy: [prop score:  0.95106], [q1: 0.80061], [q0: 0.96840],\n",
      "          Effect: [ate-q], [train: 0.14673], [test: 0.14769]\n",
      "********************************************************************************\n",
      "epoch: 312 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.24247], [Test: 2.32067],\n",
      "          Accuracy: [prop score:  0.95105], [q1: 0.78142], [q0: 0.96561],\n",
      "          Effect: [ate-q], [train: 0.13564], [test: 0.13658]\n",
      "********************************************************************************\n",
      "epoch: 313 / 500, time cost: 45.18 sec, \n",
      "          Loss: [Train: 2.24215], [Test: 2.31990],\n",
      "          Accuracy: [prop score:  0.95116], [q1: 0.80721], [q0: 0.97007],\n",
      "          Effect: [ate-q], [train: 0.15247], [test: 0.15344]\n",
      "********************************************************************************\n",
      "epoch: 314 / 500, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.24153], [Test: 2.32003],\n",
      "          Accuracy: [prop score:  0.95120], [q1: 0.81429], [q0: 0.96647],\n",
      "          Effect: [ate-q], [train: 0.15277], [test: 0.15375]\n",
      "********************************************************************************\n",
      "epoch: 315 / 500, time cost: 44.96 sec, \n",
      "          Loss: [Train: 2.24178], [Test: 2.32029],\n",
      "          Accuracy: [prop score:  0.95122], [q1: 0.80297], [q0: 0.96444],\n",
      "          Effect: [ate-q], [train: 0.14567], [test: 0.14666]\n",
      "********************************************************************************\n",
      "epoch: 316 / 500, time cost: 46.87 sec, \n",
      "          Loss: [Train: 2.24045], [Test: 2.32059],\n",
      "          Accuracy: [prop score:  0.95129], [q1: 0.78228], [q0: 0.96902],\n",
      "          Effect: [ate-q], [train: 0.14099], [test: 0.14196]\n",
      "********************************************************************************\n",
      "epoch: 317 / 500, time cost: 45.76 sec, \n",
      "          Loss: [Train: 2.24026], [Test: 2.32120],\n",
      "          Accuracy: [prop score:  0.95132], [q1: 0.77142], [q0: 0.95893],\n",
      "          Effect: [ate-q], [train: 0.12796], [test: 0.12889]\n",
      "********************************************************************************\n",
      "epoch: 318 / 500, time cost: 47.36 sec, \n",
      "          Loss: [Train: 2.24016], [Test: 2.32083],\n",
      "          Accuracy: [prop score:  0.95133], [q1: 0.80293], [q0: 0.96559],\n",
      "          Effect: [ate-q], [train: 0.14759], [test: 0.14860]\n",
      "********************************************************************************\n",
      "epoch: 319 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 2.23984], [Test: 2.32133],\n",
      "          Accuracy: [prop score:  0.95130], [q1: 0.79862], [q0: 0.96786],\n",
      "          Effect: [ate-q], [train: 0.14823], [test: 0.14921]\n",
      "********************************************************************************\n",
      "epoch: 320 / 500, time cost: 49.91 sec, \n",
      "          Loss: [Train: 2.23923], [Test: 2.32102],\n",
      "          Accuracy: [prop score:  0.95140], [q1: 0.80138], [q0: 0.96189],\n",
      "          Effect: [ate-q], [train: 0.14441], [test: 0.14539]\n",
      "********************************************************************************\n",
      "epoch: 321 / 500, time cost: 47.15 sec, \n",
      "          Loss: [Train: 2.23878], [Test: 2.32136],\n",
      "          Accuracy: [prop score:  0.95121], [q1: 0.78863], [q0: 0.96257],\n",
      "          Effect: [ate-q], [train: 0.13934], [test: 0.14038]\n",
      "********************************************************************************\n",
      "epoch: 322 / 500, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.23839], [Test: 2.32122],\n",
      "          Accuracy: [prop score:  0.95135], [q1: 0.79785], [q0: 0.96726],\n",
      "          Effect: [ate-q], [train: 0.14841], [test: 0.14944]\n",
      "********************************************************************************\n",
      "epoch: 323 / 500, time cost: 45.32 sec, \n",
      "          Loss: [Train: 2.23749], [Test: 2.32145],\n",
      "          Accuracy: [prop score:  0.95139], [q1: 0.79869], [q0: 0.96570],\n",
      "          Effect: [ate-q], [train: 0.14733], [test: 0.14834]\n",
      "********************************************************************************\n",
      "epoch: 324 / 500, time cost: 46.46 sec, \n",
      "          Loss: [Train: 2.23704], [Test: 2.32116],\n",
      "          Accuracy: [prop score:  0.95148], [q1: 0.79800], [q0: 0.96331],\n",
      "          Effect: [ate-q], [train: 0.14525], [test: 0.14624]\n",
      "********************************************************************************\n",
      "epoch: 325 / 500, time cost: 44.99 sec, \n",
      "          Loss: [Train: 2.23765], [Test: 2.32217],\n",
      "          Accuracy: [prop score:  0.95148], [q1: 0.78151], [q0: 0.96975],\n",
      "          Effect: [ate-q], [train: 0.14463], [test: 0.14567]\n",
      "********************************************************************************\n",
      "epoch: 326 / 500, time cost: 46.68 sec, \n",
      "          Loss: [Train: 2.23638], [Test: 2.32195],\n",
      "          Accuracy: [prop score:  0.95148], [q1: 0.78873], [q0: 0.96688],\n",
      "          Effect: [ate-q], [train: 0.14505], [test: 0.14608]\n",
      "********************************************************************************\n",
      "epoch: 327 / 500, time cost: 44.74 sec, \n",
      "          Loss: [Train: 2.23625], [Test: 2.32217],\n",
      "          Accuracy: [prop score:  0.95146], [q1: 0.78768], [q0: 0.96092],\n",
      "          Effect: [ate-q], [train: 0.13975], [test: 0.14076]\n",
      "********************************************************************************\n",
      "epoch: 328 / 500, time cost: 46.63 sec, \n",
      "          Loss: [Train: 2.23549], [Test: 2.32200],\n",
      "          Accuracy: [prop score:  0.95153], [q1: 0.79623], [q0: 0.96868],\n",
      "          Effect: [ate-q], [train: 0.15145], [test: 0.15248]\n",
      "********************************************************************************\n",
      "epoch: 329 / 500, time cost: 44.48 sec, \n",
      "          Loss: [Train: 2.23565], [Test: 2.32257],\n",
      "          Accuracy: [prop score:  0.95165], [q1: 0.76619], [q0: 0.96143],\n",
      "          Effect: [ate-q], [train: 0.13124], [test: 0.13227]\n",
      "********************************************************************************\n",
      "epoch: 330 / 500, time cost: 46.45 sec, \n",
      "          Loss: [Train: 2.23483], [Test: 2.32227],\n",
      "          Accuracy: [prop score:  0.95162], [q1: 0.76144], [q0: 0.96221],\n",
      "          Effect: [ate-q], [train: 0.13043], [test: 0.13145]\n",
      "********************************************************************************\n",
      "epoch: 331 / 500, time cost: 45.50 sec, \n",
      "          Loss: [Train: 2.23469], [Test: 2.32216],\n",
      "          Accuracy: [prop score:  0.95163], [q1: 0.79093], [q0: 0.96644],\n",
      "          Effect: [ate-q], [train: 0.14729], [test: 0.14833]\n",
      "********************************************************************************\n",
      "epoch: 332 / 500, time cost: 47.18 sec, \n",
      "          Loss: [Train: 2.23410], [Test: 2.32194],\n",
      "          Accuracy: [prop score:  0.95172], [q1: 0.80010], [q0: 0.96363],\n",
      "          Effect: [ate-q], [train: 0.14994], [test: 0.15100]\n",
      "********************************************************************************\n",
      "epoch: 333 / 500, time cost: 44.92 sec, \n",
      "          Loss: [Train: 2.23326], [Test: 2.32269],\n",
      "          Accuracy: [prop score:  0.95174], [q1: 0.78684], [q0: 0.96297],\n",
      "          Effect: [ate-q], [train: 0.14283], [test: 0.14387]\n",
      "********************************************************************************\n",
      "epoch: 334 / 500, time cost: 46.62 sec, \n",
      "          Loss: [Train: 2.23311], [Test: 2.32284],\n",
      "          Accuracy: [prop score:  0.95167], [q1: 0.76792], [q0: 0.95774],\n",
      "          Effect: [ate-q], [train: 0.13092], [test: 0.13194]\n",
      "********************************************************************************\n",
      "epoch: 335 / 500, time cost: 44.65 sec, \n",
      "          Loss: [Train: 2.23245], [Test: 2.32306],\n",
      "          Accuracy: [prop score:  0.95174], [q1: 0.76910], [q0: 0.96120],\n",
      "          Effect: [ate-q], [train: 0.13422], [test: 0.13529]\n",
      "********************************************************************************\n",
      "epoch: 336 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.23222], [Test: 2.32255],\n",
      "          Accuracy: [prop score:  0.95170], [q1: 0.77247], [q0: 0.96389],\n",
      "          Effect: [ate-q], [train: 0.13817], [test: 0.13923]\n",
      "********************************************************************************\n",
      "epoch: 337 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 2.23147], [Test: 2.32317],\n",
      "          Accuracy: [prop score:  0.95171], [q1: 0.79043], [q0: 0.96426],\n",
      "          Effect: [ate-q], [train: 0.14729], [test: 0.14836]\n",
      "********************************************************************************\n",
      "epoch: 338 / 500, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.23089], [Test: 2.32321],\n",
      "          Accuracy: [prop score:  0.95175], [q1: 0.76780], [q0: 0.96741],\n",
      "          Effect: [ate-q], [train: 0.14041], [test: 0.14152]\n",
      "********************************************************************************\n",
      "epoch: 339 / 500, time cost: 44.87 sec, \n",
      "          Loss: [Train: 2.23065], [Test: 2.32360],\n",
      "          Accuracy: [prop score:  0.95180], [q1: 0.76234], [q0: 0.95642],\n",
      "          Effect: [ate-q], [train: 0.12920], [test: 0.13030]\n",
      "********************************************************************************\n",
      "epoch: 340 / 500, time cost: 45.20 sec, \n",
      "          Loss: [Train: 2.23015], [Test: 2.32342],\n",
      "          Accuracy: [prop score:  0.95183], [q1: 0.79111], [q0: 0.95959],\n",
      "          Effect: [ate-q], [train: 0.14472], [test: 0.14582]\n",
      "********************************************************************************\n",
      "epoch: 341 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 2.23037], [Test: 2.32350],\n",
      "          Accuracy: [prop score:  0.95182], [q1: 0.76719], [q0: 0.96728],\n",
      "          Effect: [ate-q], [train: 0.14095], [test: 0.14205]\n",
      "********************************************************************************\n",
      "epoch: 342 / 500, time cost: 47.27 sec, \n",
      "          Loss: [Train: 2.22989], [Test: 2.32416],\n",
      "          Accuracy: [prop score:  0.95183], [q1: 0.77339], [q0: 0.96038],\n",
      "          Effect: [ate-q], [train: 0.13773], [test: 0.13888]\n",
      "********************************************************************************\n",
      "epoch: 343 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.22884], [Test: 2.32439],\n",
      "          Accuracy: [prop score:  0.95184], [q1: 0.76115], [q0: 0.95980],\n",
      "          Effect: [ate-q], [train: 0.13247], [test: 0.13353]\n",
      "********************************************************************************\n",
      "epoch: 344 / 500, time cost: 47.14 sec, \n",
      "          Loss: [Train: 2.22847], [Test: 2.32424],\n",
      "          Accuracy: [prop score:  0.95187], [q1: 0.75580], [q0: 0.96605],\n",
      "          Effect: [ate-q], [train: 0.13603], [test: 0.13715]\n",
      "********************************************************************************\n",
      "epoch: 345 / 500, time cost: 44.67 sec, \n",
      "          Loss: [Train: 2.22815], [Test: 2.32393],\n",
      "          Accuracy: [prop score:  0.95183], [q1: 0.76761], [q0: 0.96290],\n",
      "          Effect: [ate-q], [train: 0.13823], [test: 0.13934]\n",
      "********************************************************************************\n",
      "epoch: 346 / 500, time cost: 47.18 sec, \n",
      "          Loss: [Train: 2.22725], [Test: 2.32477],\n",
      "          Accuracy: [prop score:  0.95199], [q1: 0.77871], [q0: 0.96342],\n",
      "          Effect: [ate-q], [train: 0.14420], [test: 0.14531]\n",
      "********************************************************************************\n",
      "epoch: 347 / 500, time cost: 44.44 sec, \n",
      "          Loss: [Train: 2.22691], [Test: 2.32435],\n",
      "          Accuracy: [prop score:  0.95192], [q1: 0.76307], [q0: 0.95754],\n",
      "          Effect: [ate-q], [train: 0.13291], [test: 0.13402]\n",
      "********************************************************************************\n",
      "epoch: 348 / 500, time cost: 46.80 sec, \n",
      "          Loss: [Train: 2.22690], [Test: 2.32474],\n",
      "          Accuracy: [prop score:  0.95201], [q1: 0.78453], [q0: 0.95982],\n",
      "          Effect: [ate-q], [train: 0.14430], [test: 0.14545]\n",
      "********************************************************************************\n",
      "epoch: 349 / 500, time cost: 44.68 sec, \n",
      "          Loss: [Train: 2.22670], [Test: 2.32486],\n",
      "          Accuracy: [prop score:  0.95202], [q1: 0.77341], [q0: 0.96474],\n",
      "          Effect: [ate-q], [train: 0.14378], [test: 0.14490]\n",
      "********************************************************************************\n",
      "epoch: 350 / 500, time cost: 46.25 sec, \n",
      "          Loss: [Train: 2.22646], [Test: 2.32529],\n",
      "          Accuracy: [prop score:  0.95204], [q1: 0.75260], [q0: 0.96074],\n",
      "          Effect: [ate-q], [train: 0.13192], [test: 0.13306]\n",
      "********************************************************************************\n",
      "epoch: 351 / 500, time cost: 44.82 sec, \n",
      "          Loss: [Train: 2.22561], [Test: 2.32516],\n",
      "          Accuracy: [prop score:  0.95201], [q1: 0.75749], [q0: 0.96344],\n",
      "          Effect: [ate-q], [train: 0.13645], [test: 0.13757]\n",
      "********************************************************************************\n",
      "epoch: 352 / 500, time cost: 47.85 sec, \n",
      "          Loss: [Train: 2.22511], [Test: 2.32593],\n",
      "          Accuracy: [prop score:  0.95209], [q1: 0.75913], [q0: 0.96188],\n",
      "          Effect: [ate-q], [train: 0.13626], [test: 0.13740]\n",
      "********************************************************************************\n",
      "epoch: 353 / 500, time cost: 45.16 sec, \n",
      "          Loss: [Train: 2.22447], [Test: 2.32575],\n",
      "          Accuracy: [prop score:  0.95204], [q1: 0.75959], [q0: 0.96037],\n",
      "          Effect: [ate-q], [train: 0.13547], [test: 0.13668]\n",
      "********************************************************************************\n",
      "epoch: 354 / 500, time cost: 46.93 sec, \n",
      "          Loss: [Train: 2.22413], [Test: 2.32600],\n",
      "          Accuracy: [prop score:  0.95210], [q1: 0.76119], [q0: 0.95879],\n",
      "          Effect: [ate-q], [train: 0.13515], [test: 0.13631]\n",
      "********************************************************************************\n",
      "epoch: 355 / 500, time cost: 45.00 sec, \n",
      "          Loss: [Train: 2.22354], [Test: 2.32586],\n",
      "          Accuracy: [prop score:  0.95215], [q1: 0.77086], [q0: 0.95564],\n",
      "          Effect: [ate-q], [train: 0.13739], [test: 0.13857]\n",
      "********************************************************************************\n",
      "epoch: 356 / 500, time cost: 46.59 sec, \n",
      "          Loss: [Train: 2.22333], [Test: 2.32692],\n",
      "          Accuracy: [prop score:  0.95217], [q1: 0.77215], [q0: 0.94965],\n",
      "          Effect: [ate-q], [train: 0.13480], [test: 0.13599]\n",
      "********************************************************************************\n",
      "epoch: 357 / 500, time cost: 45.00 sec, \n",
      "          Loss: [Train: 2.22293], [Test: 2.32632],\n",
      "          Accuracy: [prop score:  0.95221], [q1: 0.75406], [q0: 0.95687],\n",
      "          Effect: [ate-q], [train: 0.13165], [test: 0.13284]\n",
      "********************************************************************************\n",
      "epoch: 358 / 500, time cost: 47.02 sec, \n",
      "          Loss: [Train: 2.22197], [Test: 2.32691],\n",
      "          Accuracy: [prop score:  0.95226], [q1: 0.77266], [q0: 0.95924],\n",
      "          Effect: [ate-q], [train: 0.14160], [test: 0.14281]\n",
      "********************************************************************************\n",
      "epoch: 359 / 500, time cost: 45.19 sec, \n",
      "          Loss: [Train: 2.22214], [Test: 2.32636],\n",
      "          Accuracy: [prop score:  0.95231], [q1: 0.77195], [q0: 0.96049],\n",
      "          Effect: [ate-q], [train: 0.14271], [test: 0.14394]\n",
      "********************************************************************************\n",
      "epoch: 360 / 500, time cost: 46.53 sec, \n",
      "          Loss: [Train: 2.22120], [Test: 2.32709],\n",
      "          Accuracy: [prop score:  0.95235], [q1: 0.74151], [q0: 0.95104],\n",
      "          Effect: [ate-q], [train: 0.12406], [test: 0.12526]\n",
      "********************************************************************************\n",
      "epoch: 361 / 500, time cost: 44.43 sec, \n",
      "          Loss: [Train: 2.22079], [Test: 2.32710],\n",
      "          Accuracy: [prop score:  0.95224], [q1: 0.76535], [q0: 0.95437],\n",
      "          Effect: [ate-q], [train: 0.13602], [test: 0.13725]\n",
      "********************************************************************************\n",
      "epoch: 362 / 500, time cost: 46.32 sec, \n",
      "          Loss: [Train: 2.22050], [Test: 2.32756],\n",
      "          Accuracy: [prop score:  0.95237], [q1: 0.74676], [q0: 0.95984],\n",
      "          Effect: [ate-q], [train: 0.13253], [test: 0.13374]\n",
      "********************************************************************************\n",
      "epoch: 363 / 500, time cost: 45.53 sec, \n",
      "          Loss: [Train: 2.21981], [Test: 2.32742],\n",
      "          Accuracy: [prop score:  0.95240], [q1: 0.76031], [q0: 0.96024],\n",
      "          Effect: [ate-q], [train: 0.13867], [test: 0.13990]\n",
      "********************************************************************************\n",
      "epoch: 364 / 500, time cost: 47.11 sec, \n",
      "          Loss: [Train: 2.21979], [Test: 2.32773],\n",
      "          Accuracy: [prop score:  0.95235], [q1: 0.76335], [q0: 0.95275],\n",
      "          Effect: [ate-q], [train: 0.13503], [test: 0.13625]\n",
      "********************************************************************************\n",
      "epoch: 365 / 500, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.21922], [Test: 2.32776],\n",
      "          Accuracy: [prop score:  0.95247], [q1: 0.74092], [q0: 0.95907],\n",
      "          Effect: [ate-q], [train: 0.13070], [test: 0.13193]\n",
      "********************************************************************************\n",
      "epoch: 366 / 500, time cost: 47.97 sec, \n",
      "          Loss: [Train: 2.21885], [Test: 2.32823],\n",
      "          Accuracy: [prop score:  0.95239], [q1: 0.75564], [q0: 0.95784],\n",
      "          Effect: [ate-q], [train: 0.13557], [test: 0.13680]\n",
      "********************************************************************************\n",
      "epoch: 367 / 500, time cost: 45.14 sec, \n",
      "          Loss: [Train: 2.21811], [Test: 2.32830],\n",
      "          Accuracy: [prop score:  0.95244], [q1: 0.75549], [q0: 0.96048],\n",
      "          Effect: [ate-q], [train: 0.13788], [test: 0.13914]\n",
      "********************************************************************************\n",
      "epoch: 368 / 500, time cost: 47.74 sec, \n",
      "          Loss: [Train: 2.21793], [Test: 2.32876],\n",
      "          Accuracy: [prop score:  0.95248], [q1: 0.76195], [q0: 0.95774],\n",
      "          Effect: [ate-q], [train: 0.13886], [test: 0.14015]\n",
      "********************************************************************************\n",
      "epoch: 369 / 500, time cost: 44.52 sec, \n",
      "          Loss: [Train: 2.21678], [Test: 2.32834],\n",
      "          Accuracy: [prop score:  0.95253], [q1: 0.75573], [q0: 0.95377],\n",
      "          Effect: [ate-q], [train: 0.13376], [test: 0.13498]\n",
      "********************************************************************************\n",
      "epoch: 370 / 500, time cost: 47.29 sec, \n",
      "          Loss: [Train: 2.21657], [Test: 2.32892],\n",
      "          Accuracy: [prop score:  0.95251], [q1: 0.77426], [q0: 0.95097],\n",
      "          Effect: [ate-q], [train: 0.14088], [test: 0.14213]\n",
      "********************************************************************************\n",
      "epoch: 371 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 2.21640], [Test: 2.32854],\n",
      "          Accuracy: [prop score:  0.95253], [q1: 0.77128], [q0: 0.96499],\n",
      "          Effect: [ate-q], [train: 0.15037], [test: 0.15163]\n",
      "********************************************************************************\n",
      "epoch: 372 / 500, time cost: 49.52 sec, \n",
      "          Loss: [Train: 2.21544], [Test: 2.32908],\n",
      "          Accuracy: [prop score:  0.95249], [q1: 0.77270], [q0: 0.95137],\n",
      "          Effect: [ate-q], [train: 0.14111], [test: 0.14237]\n",
      "********************************************************************************\n",
      "epoch: 373 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.21522], [Test: 2.32960],\n",
      "          Accuracy: [prop score:  0.95252], [q1: 0.72451], [q0: 0.95492],\n",
      "          Effect: [ate-q], [train: 0.12345], [test: 0.12469]\n",
      "********************************************************************************\n",
      "epoch: 374 / 500, time cost: 47.56 sec, \n",
      "          Loss: [Train: 2.21567], [Test: 2.32976],\n",
      "          Accuracy: [prop score:  0.95259], [q1: 0.75133], [q0: 0.96502],\n",
      "          Effect: [ate-q], [train: 0.14222], [test: 0.14349]\n",
      "********************************************************************************\n",
      "epoch: 375 / 500, time cost: 46.80 sec, \n",
      "          Loss: [Train: 2.21437], [Test: 2.32929],\n",
      "          Accuracy: [prop score:  0.95254], [q1: 0.76401], [q0: 0.95895],\n",
      "          Effect: [ate-q], [train: 0.14256], [test: 0.14389]\n",
      "********************************************************************************\n",
      "epoch: 376 / 500, time cost: 50.92 sec, \n",
      "          Loss: [Train: 2.21368], [Test: 2.33017],\n",
      "          Accuracy: [prop score:  0.95263], [q1: 0.77016], [q0: 0.96088],\n",
      "          Effect: [ate-q], [train: 0.14727], [test: 0.14857]\n",
      "********************************************************************************\n",
      "epoch: 377 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 2.21295], [Test: 2.33054],\n",
      "          Accuracy: [prop score:  0.95268], [q1: 0.78312], [q0: 0.96119],\n",
      "          Effect: [ate-q], [train: 0.15451], [test: 0.15584]\n",
      "********************************************************************************\n",
      "epoch: 378 / 500, time cost: 47.01 sec, \n",
      "          Loss: [Train: 2.21306], [Test: 2.33003],\n",
      "          Accuracy: [prop score:  0.95261], [q1: 0.75092], [q0: 0.95477],\n",
      "          Effect: [ate-q], [train: 0.13480], [test: 0.13610]\n",
      "********************************************************************************\n",
      "epoch: 379 / 500, time cost: 44.76 sec, \n",
      "          Loss: [Train: 2.21201], [Test: 2.33012],\n",
      "          Accuracy: [prop score:  0.95278], [q1: 0.76742], [q0: 0.96299],\n",
      "          Effect: [ate-q], [train: 0.14864], [test: 0.14997]\n",
      "********************************************************************************\n",
      "epoch: 380 / 500, time cost: 47.15 sec, \n",
      "          Loss: [Train: 2.21238], [Test: 2.33120],\n",
      "          Accuracy: [prop score:  0.95274], [q1: 0.77021], [q0: 0.95162],\n",
      "          Effect: [ate-q], [train: 0.14216], [test: 0.14350]\n",
      "********************************************************************************\n",
      "epoch: 381 / 500, time cost: 45.13 sec, \n",
      "          Loss: [Train: 2.21194], [Test: 2.33115],\n",
      "          Accuracy: [prop score:  0.95278], [q1: 0.76626], [q0: 0.95457],\n",
      "          Effect: [ate-q], [train: 0.14228], [test: 0.14363]\n",
      "********************************************************************************\n",
      "epoch: 382 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.21132], [Test: 2.33143],\n",
      "          Accuracy: [prop score:  0.95276], [q1: 0.77330], [q0: 0.95237],\n",
      "          Effect: [ate-q], [train: 0.14463], [test: 0.14597]\n",
      "********************************************************************************\n",
      "epoch: 383 / 500, time cost: 44.87 sec, \n",
      "          Loss: [Train: 2.21070], [Test: 2.33115],\n",
      "          Accuracy: [prop score:  0.95278], [q1: 0.74788], [q0: 0.95457],\n",
      "          Effect: [ate-q], [train: 0.13472], [test: 0.13606]\n",
      "********************************************************************************\n",
      "epoch: 384 / 500, time cost: 47.28 sec, \n",
      "          Loss: [Train: 2.21052], [Test: 2.33168],\n",
      "          Accuracy: [prop score:  0.95283], [q1: 0.75573], [q0: 0.95269],\n",
      "          Effect: [ate-q], [train: 0.13726], [test: 0.13862]\n",
      "********************************************************************************\n",
      "epoch: 385 / 500, time cost: 45.10 sec, \n",
      "          Loss: [Train: 2.20994], [Test: 2.33165],\n",
      "          Accuracy: [prop score:  0.95285], [q1: 0.74254], [q0: 0.95172],\n",
      "          Effect: [ate-q], [train: 0.13116], [test: 0.13251]\n",
      "********************************************************************************\n",
      "epoch: 386 / 500, time cost: 47.12 sec, \n",
      "          Loss: [Train: 2.20973], [Test: 2.33236],\n",
      "          Accuracy: [prop score:  0.95298], [q1: 0.71640], [q0: 0.95151],\n",
      "          Effect: [ate-q], [train: 0.12186], [test: 0.12319]\n",
      "********************************************************************************\n",
      "epoch: 387 / 500, time cost: 45.04 sec, \n",
      "          Loss: [Train: 2.20870], [Test: 2.33213],\n",
      "          Accuracy: [prop score:  0.95297], [q1: 0.74357], [q0: 0.95618],\n",
      "          Effect: [ate-q], [train: 0.13539], [test: 0.13679]\n",
      "********************************************************************************\n",
      "epoch: 388 / 500, time cost: 47.64 sec, \n",
      "          Loss: [Train: 2.20858], [Test: 2.33233],\n",
      "          Accuracy: [prop score:  0.95304], [q1: 0.74339], [q0: 0.95887],\n",
      "          Effect: [ate-q], [train: 0.13770], [test: 0.13905]\n",
      "********************************************************************************\n",
      "epoch: 389 / 500, time cost: 44.84 sec, \n",
      "          Loss: [Train: 2.20816], [Test: 2.33285],\n",
      "          Accuracy: [prop score:  0.95307], [q1: 0.74684], [q0: 0.94787],\n",
      "          Effect: [ate-q], [train: 0.13212], [test: 0.13352]\n",
      "********************************************************************************\n",
      "epoch: 390 / 500, time cost: 47.52 sec, \n",
      "          Loss: [Train: 2.20715], [Test: 2.33390],\n",
      "          Accuracy: [prop score:  0.95312], [q1: 0.70032], [q0: 0.95178],\n",
      "          Effect: [ate-q], [train: 0.11776], [test: 0.11910]\n",
      "********************************************************************************\n",
      "epoch: 391 / 500, time cost: 45.02 sec, \n",
      "          Loss: [Train: 2.20733], [Test: 2.33347],\n",
      "          Accuracy: [prop score:  0.95307], [q1: 0.73933], [q0: 0.96027],\n",
      "          Effect: [ate-q], [train: 0.13856], [test: 0.13993]\n",
      "********************************************************************************\n",
      "epoch: 392 / 500, time cost: 46.68 sec, \n",
      "          Loss: [Train: 2.20672], [Test: 2.33331],\n",
      "          Accuracy: [prop score:  0.95309], [q1: 0.75873], [q0: 0.94995],\n",
      "          Effect: [ate-q], [train: 0.13975], [test: 0.14116]\n",
      "********************************************************************************\n",
      "epoch: 393 / 500, time cost: 44.70 sec, \n",
      "          Loss: [Train: 2.20575], [Test: 2.33367],\n",
      "          Accuracy: [prop score:  0.95311], [q1: 0.77625], [q0: 0.95150],\n",
      "          Effect: [ate-q], [train: 0.14945], [test: 0.15085]\n",
      "********************************************************************************\n",
      "epoch: 394 / 500, time cost: 47.09 sec, \n",
      "          Loss: [Train: 2.20561], [Test: 2.33351],\n",
      "          Accuracy: [prop score:  0.95312], [q1: 0.75637], [q0: 0.95685],\n",
      "          Effect: [ate-q], [train: 0.14403], [test: 0.14543]\n",
      "********************************************************************************\n",
      "epoch: 395 / 500, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.20501], [Test: 2.33415],\n",
      "          Accuracy: [prop score:  0.95313], [q1: 0.73937], [q0: 0.95304],\n",
      "          Effect: [ate-q], [train: 0.13414], [test: 0.13554]\n",
      "********************************************************************************\n",
      "epoch: 396 / 500, time cost: 47.27 sec, \n",
      "          Loss: [Train: 2.20463], [Test: 2.33430],\n",
      "          Accuracy: [prop score:  0.95315], [q1: 0.76983], [q0: 0.94951],\n",
      "          Effect: [ate-q], [train: 0.14587], [test: 0.14730]\n",
      "********************************************************************************\n",
      "epoch: 397 / 500, time cost: 43.69 sec, \n",
      "          Loss: [Train: 2.20470], [Test: 2.33435],\n",
      "          Accuracy: [prop score:  0.95316], [q1: 0.75187], [q0: 0.94755],\n",
      "          Effect: [ate-q], [train: 0.13662], [test: 0.13808]\n",
      "********************************************************************************\n",
      "epoch: 398 / 500, time cost: 46.51 sec, \n",
      "          Loss: [Train: 2.20375], [Test: 2.33558],\n",
      "          Accuracy: [prop score:  0.95322], [q1: 0.73114], [q0: 0.95693],\n",
      "          Effect: [ate-q], [train: 0.13413], [test: 0.13554]\n",
      "********************************************************************************\n",
      "epoch: 399 / 500, time cost: 45.63 sec, \n",
      "          Loss: [Train: 2.20263], [Test: 2.33465],\n",
      "          Accuracy: [prop score:  0.95321], [q1: 0.73249], [q0: 0.94784],\n",
      "          Effect: [ate-q], [train: 0.12886], [test: 0.13029]\n",
      "********************************************************************************\n",
      "epoch: 400 / 500, time cost: 47.27 sec, \n",
      "          Loss: [Train: 2.20236], [Test: 2.33580],\n",
      "          Accuracy: [prop score:  0.95322], [q1: 0.73408], [q0: 0.95010],\n",
      "          Effect: [ate-q], [train: 0.13123], [test: 0.13267]\n",
      "********************************************************************************\n",
      "epoch: 401 / 500, time cost: 45.12 sec, \n",
      "          Loss: [Train: 2.20200], [Test: 2.33609],\n",
      "          Accuracy: [prop score:  0.95331], [q1: 0.69987], [q0: 0.94296],\n",
      "          Effect: [ate-q], [train: 0.11476], [test: 0.11621]\n",
      "********************************************************************************\n",
      "epoch: 402 / 500, time cost: 46.85 sec, \n",
      "          Loss: [Train: 2.20160], [Test: 2.33581],\n",
      "          Accuracy: [prop score:  0.95327], [q1: 0.73142], [q0: 0.94996],\n",
      "          Effect: [ate-q], [train: 0.13098], [test: 0.13240]\n",
      "********************************************************************************\n",
      "epoch: 403 / 500, time cost: 44.87 sec, \n",
      "          Loss: [Train: 2.20114], [Test: 2.33632],\n",
      "          Accuracy: [prop score:  0.95324], [q1: 0.72679], [q0: 0.94394],\n",
      "          Effect: [ate-q], [train: 0.12623], [test: 0.12770]\n",
      "********************************************************************************\n",
      "epoch: 404 / 500, time cost: 46.76 sec, \n",
      "          Loss: [Train: 2.20082], [Test: 2.33664],\n",
      "          Accuracy: [prop score:  0.95330], [q1: 0.74988], [q0: 0.95376],\n",
      "          Effect: [ate-q], [train: 0.14172], [test: 0.14321]\n",
      "********************************************************************************\n",
      "epoch: 405 / 500, time cost: 45.04 sec, \n",
      "          Loss: [Train: 2.20037], [Test: 2.33629],\n",
      "          Accuracy: [prop score:  0.95325], [q1: 0.73313], [q0: 0.95108],\n",
      "          Effect: [ate-q], [train: 0.13314], [test: 0.13459]\n",
      "********************************************************************************\n",
      "epoch: 406 / 500, time cost: 47.08 sec, \n",
      "          Loss: [Train: 2.19993], [Test: 2.33700],\n",
      "          Accuracy: [prop score:  0.95336], [q1: 0.73004], [q0: 0.94957],\n",
      "          Effect: [ate-q], [train: 0.13116], [test: 0.13264]\n",
      "********************************************************************************\n",
      "epoch: 407 / 500, time cost: 44.53 sec, \n",
      "          Loss: [Train: 2.19967], [Test: 2.33681],\n",
      "          Accuracy: [prop score:  0.95338], [q1: 0.73440], [q0: 0.94105],\n",
      "          Effect: [ate-q], [train: 0.12838], [test: 0.12987]\n",
      "********************************************************************************\n",
      "epoch: 408 / 500, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.19858], [Test: 2.33760],\n",
      "          Accuracy: [prop score:  0.95347], [q1: 0.76044], [q0: 0.94408],\n",
      "          Effect: [ate-q], [train: 0.14189], [test: 0.14347]\n",
      "********************************************************************************\n",
      "epoch: 409 / 500, time cost: 45.10 sec, \n",
      "          Loss: [Train: 2.19883], [Test: 2.33804],\n",
      "          Accuracy: [prop score:  0.95348], [q1: 0.73708], [q0: 0.93850],\n",
      "          Effect: [ate-q], [train: 0.12885], [test: 0.13034]\n",
      "********************************************************************************\n",
      "epoch: 410 / 500, time cost: 47.59 sec, \n",
      "          Loss: [Train: 2.19774], [Test: 2.33786],\n",
      "          Accuracy: [prop score:  0.95340], [q1: 0.73098], [q0: 0.93870],\n",
      "          Effect: [ate-q], [train: 0.12658], [test: 0.12811]\n",
      "********************************************************************************\n",
      "epoch: 411 / 500, time cost: 45.33 sec, \n",
      "          Loss: [Train: 2.19728], [Test: 2.33827],\n",
      "          Accuracy: [prop score:  0.95339], [q1: 0.73425], [q0: 0.94754],\n",
      "          Effect: [ate-q], [train: 0.13305], [test: 0.13457]\n",
      "********************************************************************************\n",
      "epoch: 412 / 500, time cost: 46.81 sec, \n",
      "          Loss: [Train: 2.19705], [Test: 2.33884],\n",
      "          Accuracy: [prop score:  0.95349], [q1: 0.70423], [q0: 0.94699],\n",
      "          Effect: [ate-q], [train: 0.12140], [test: 0.12295]\n",
      "********************************************************************************\n",
      "epoch: 413 / 500, time cost: 44.83 sec, \n",
      "          Loss: [Train: 2.19646], [Test: 2.33850],\n",
      "          Accuracy: [prop score:  0.95347], [q1: 0.74404], [q0: 0.94299],\n",
      "          Effect: [ate-q], [train: 0.13536], [test: 0.13692]\n",
      "********************************************************************************\n",
      "epoch: 414 / 500, time cost: 47.31 sec, \n",
      "          Loss: [Train: 2.19599], [Test: 2.33890],\n",
      "          Accuracy: [prop score:  0.95349], [q1: 0.73030], [q0: 0.95333],\n",
      "          Effect: [ate-q], [train: 0.13644], [test: 0.13800]\n",
      "********************************************************************************\n",
      "epoch: 415 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.19553], [Test: 2.33888],\n",
      "          Accuracy: [prop score:  0.95340], [q1: 0.74982], [q0: 0.95284],\n",
      "          Effect: [ate-q], [train: 0.14482], [test: 0.14639]\n",
      "********************************************************************************\n",
      "epoch: 416 / 500, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.19453], [Test: 2.33940],\n",
      "          Accuracy: [prop score:  0.95349], [q1: 0.75176], [q0: 0.93465],\n",
      "          Effect: [ate-q], [train: 0.13559], [test: 0.13720]\n",
      "********************************************************************************\n",
      "epoch: 417 / 500, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.19488], [Test: 2.33999],\n",
      "          Accuracy: [prop score:  0.95350], [q1: 0.74104], [q0: 0.94554],\n",
      "          Effect: [ate-q], [train: 0.13680], [test: 0.13835]\n",
      "********************************************************************************\n",
      "epoch: 418 / 500, time cost: 47.72 sec, \n",
      "          Loss: [Train: 2.19418], [Test: 2.33979],\n",
      "          Accuracy: [prop score:  0.95350], [q1: 0.74024], [q0: 0.94760],\n",
      "          Effect: [ate-q], [train: 0.13777], [test: 0.13933]\n",
      "********************************************************************************\n",
      "epoch: 419 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.19396], [Test: 2.34072],\n",
      "          Accuracy: [prop score:  0.95363], [q1: 0.69866], [q0: 0.93508],\n",
      "          Effect: [ate-q], [train: 0.11561], [test: 0.11716]\n",
      "********************************************************************************\n",
      "epoch: 420 / 500, time cost: 48.88 sec, \n",
      "          Loss: [Train: 2.19327], [Test: 2.34000],\n",
      "          Accuracy: [prop score:  0.95346], [q1: 0.74148], [q0: 0.94563],\n",
      "          Effect: [ate-q], [train: 0.13804], [test: 0.13966]\n",
      "********************************************************************************\n",
      "epoch: 421 / 500, time cost: 45.57 sec, \n",
      "          Loss: [Train: 2.19272], [Test: 2.34041],\n",
      "          Accuracy: [prop score:  0.95352], [q1: 0.73470], [q0: 0.94359],\n",
      "          Effect: [ate-q], [train: 0.13417], [test: 0.13574]\n",
      "********************************************************************************\n",
      "epoch: 422 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.19249], [Test: 2.34102],\n",
      "          Accuracy: [prop score:  0.95357], [q1: 0.75166], [q0: 0.93588],\n",
      "          Effect: [ate-q], [train: 0.13792], [test: 0.13953]\n",
      "********************************************************************************\n",
      "epoch: 423 / 500, time cost: 45.42 sec, \n",
      "          Loss: [Train: 2.19220], [Test: 2.34199],\n",
      "          Accuracy: [prop score:  0.95351], [q1: 0.74509], [q0: 0.94024],\n",
      "          Effect: [ate-q], [train: 0.13758], [test: 0.13915]\n",
      "********************************************************************************\n",
      "epoch: 424 / 500, time cost: 47.81 sec, \n",
      "          Loss: [Train: 2.19130], [Test: 2.34082],\n",
      "          Accuracy: [prop score:  0.95352], [q1: 0.73691], [q0: 0.94248],\n",
      "          Effect: [ate-q], [train: 0.13570], [test: 0.13734]\n",
      "********************************************************************************\n",
      "epoch: 425 / 500, time cost: 45.59 sec, \n",
      "          Loss: [Train: 2.19036], [Test: 2.34237],\n",
      "          Accuracy: [prop score:  0.95361], [q1: 0.75626], [q0: 0.94034],\n",
      "          Effect: [ate-q], [train: 0.14330], [test: 0.14490]\n",
      "********************************************************************************\n",
      "epoch: 426 / 500, time cost: 47.08 sec, \n",
      "          Loss: [Train: 2.19037], [Test: 2.34245],\n",
      "          Accuracy: [prop score:  0.95359], [q1: 0.74575], [q0: 0.94510],\n",
      "          Effect: [ate-q], [train: 0.14169], [test: 0.14329]\n",
      "********************************************************************************\n",
      "epoch: 427 / 500, time cost: 45.08 sec, \n",
      "          Loss: [Train: 2.18931], [Test: 2.34224],\n",
      "          Accuracy: [prop score:  0.95362], [q1: 0.71834], [q0: 0.94672],\n",
      "          Effect: [ate-q], [train: 0.13132], [test: 0.13295]\n",
      "********************************************************************************\n",
      "epoch: 428 / 500, time cost: 47.35 sec, \n",
      "          Loss: [Train: 2.18882], [Test: 2.34262],\n",
      "          Accuracy: [prop score:  0.95360], [q1: 0.73951], [q0: 0.92978],\n",
      "          Effect: [ate-q], [train: 0.13157], [test: 0.13318]\n",
      "********************************************************************************\n",
      "epoch: 429 / 500, time cost: 45.19 sec, \n",
      "          Loss: [Train: 2.18893], [Test: 2.34327],\n",
      "          Accuracy: [prop score:  0.95360], [q1: 0.71243], [q0: 0.94568],\n",
      "          Effect: [ate-q], [train: 0.12971], [test: 0.13133]\n",
      "********************************************************************************\n",
      "epoch: 430 / 500, time cost: 47.34 sec, \n",
      "          Loss: [Train: 2.18796], [Test: 2.34441],\n",
      "          Accuracy: [prop score:  0.95366], [q1: 0.68283], [q0: 0.93920],\n",
      "          Effect: [ate-q], [train: 0.11641], [test: 0.11804]\n",
      "********************************************************************************\n",
      "epoch: 431 / 500, time cost: 44.41 sec, \n",
      "          Loss: [Train: 2.18764], [Test: 2.34340],\n",
      "          Accuracy: [prop score:  0.95366], [q1: 0.71968], [q0: 0.95298],\n",
      "          Effect: [ate-q], [train: 0.13770], [test: 0.13938]\n",
      "********************************************************************************\n",
      "epoch: 432 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.18732], [Test: 2.34423],\n",
      "          Accuracy: [prop score:  0.95370], [q1: 0.75136], [q0: 0.94969],\n",
      "          Effect: [ate-q], [train: 0.14909], [test: 0.15077]\n",
      "********************************************************************************\n",
      "epoch: 433 / 500, time cost: 44.65 sec, \n",
      "          Loss: [Train: 2.18699], [Test: 2.34495],\n",
      "          Accuracy: [prop score:  0.95365], [q1: 0.76481], [q0: 0.92654],\n",
      "          Effect: [ate-q], [train: 0.14351], [test: 0.14518]\n",
      "********************************************************************************\n",
      "epoch: 434 / 500, time cost: 46.51 sec, \n",
      "          Loss: [Train: 2.18620], [Test: 2.34420],\n",
      "          Accuracy: [prop score:  0.95364], [q1: 0.74215], [q0: 0.93714],\n",
      "          Effect: [ate-q], [train: 0.13824], [test: 0.13992]\n",
      "********************************************************************************\n",
      "epoch: 435 / 500, time cost: 44.63 sec, \n",
      "          Loss: [Train: 2.18587], [Test: 2.34493],\n",
      "          Accuracy: [prop score:  0.95361], [q1: 0.71281], [q0: 0.92582],\n",
      "          Effect: [ate-q], [train: 0.12100], [test: 0.12265]\n",
      "********************************************************************************\n",
      "epoch: 436 / 500, time cost: 46.87 sec, \n",
      "          Loss: [Train: 2.18536], [Test: 2.34494],\n",
      "          Accuracy: [prop score:  0.95372], [q1: 0.71527], [q0: 0.93035],\n",
      "          Effect: [ate-q], [train: 0.12483], [test: 0.12653]\n",
      "********************************************************************************\n",
      "epoch: 437 / 500, time cost: 44.48 sec, \n",
      "          Loss: [Train: 2.18486], [Test: 2.34533],\n",
      "          Accuracy: [prop score:  0.95369], [q1: 0.73598], [q0: 0.92493],\n",
      "          Effect: [ate-q], [train: 0.13055], [test: 0.13221]\n",
      "********************************************************************************\n",
      "epoch: 438 / 500, time cost: 46.97 sec, \n",
      "          Loss: [Train: 2.18431], [Test: 2.34565],\n",
      "          Accuracy: [prop score:  0.95368], [q1: 0.70593], [q0: 0.94472],\n",
      "          Effect: [ate-q], [train: 0.12961], [test: 0.13128]\n",
      "********************************************************************************\n",
      "epoch: 439 / 500, time cost: 45.03 sec, \n",
      "          Loss: [Train: 2.18354], [Test: 2.34603],\n",
      "          Accuracy: [prop score:  0.95367], [q1: 0.72568], [q0: 0.93069],\n",
      "          Effect: [ate-q], [train: 0.12951], [test: 0.13126]\n",
      "********************************************************************************\n",
      "epoch: 440 / 500, time cost: 47.50 sec, \n",
      "          Loss: [Train: 2.18303], [Test: 2.34595],\n",
      "          Accuracy: [prop score:  0.95367], [q1: 0.72141], [q0: 0.94183],\n",
      "          Effect: [ate-q], [train: 0.13400], [test: 0.13567]\n",
      "********************************************************************************\n",
      "epoch: 441 / 500, time cost: 44.91 sec, \n",
      "          Loss: [Train: 2.18299], [Test: 2.34672],\n",
      "          Accuracy: [prop score:  0.95367], [q1: 0.73483], [q0: 0.93293],\n",
      "          Effect: [ate-q], [train: 0.13505], [test: 0.13677]\n",
      "********************************************************************************\n",
      "epoch: 442 / 500, time cost: 46.79 sec, \n",
      "          Loss: [Train: 2.18222], [Test: 2.34731],\n",
      "          Accuracy: [prop score:  0.95374], [q1: 0.71300], [q0: 0.93758],\n",
      "          Effect: [ate-q], [train: 0.12956], [test: 0.13126]\n",
      "********************************************************************************\n",
      "epoch: 443 / 500, time cost: 45.31 sec, \n",
      "          Loss: [Train: 2.18187], [Test: 2.34709],\n",
      "          Accuracy: [prop score:  0.95377], [q1: 0.72894], [q0: 0.93526],\n",
      "          Effect: [ate-q], [train: 0.13446], [test: 0.13616]\n",
      "********************************************************************************\n",
      "epoch: 444 / 500, time cost: 48.55 sec, \n",
      "          Loss: [Train: 2.18114], [Test: 2.34861],\n",
      "          Accuracy: [prop score:  0.95373], [q1: 0.67719], [q0: 0.93070],\n",
      "          Effect: [ate-q], [train: 0.11325], [test: 0.11492]\n",
      "********************************************************************************\n",
      "epoch: 445 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.18120], [Test: 2.34860],\n",
      "          Accuracy: [prop score:  0.95386], [q1: 0.70271], [q0: 0.92759],\n",
      "          Effect: [ate-q], [train: 0.12073], [test: 0.12247]\n",
      "********************************************************************************\n",
      "epoch: 446 / 500, time cost: 47.61 sec, \n",
      "          Loss: [Train: 2.18036], [Test: 2.34858],\n",
      "          Accuracy: [prop score:  0.95372], [q1: 0.75703], [q0: 0.93795],\n",
      "          Effect: [ate-q], [train: 0.15023], [test: 0.15204]\n",
      "********************************************************************************\n",
      "epoch: 447 / 500, time cost: 44.95 sec, \n",
      "          Loss: [Train: 2.17987], [Test: 2.34911],\n",
      "          Accuracy: [prop score:  0.95382], [q1: 0.74034], [q0: 0.93554],\n",
      "          Effect: [ate-q], [train: 0.14089], [test: 0.14268]\n",
      "********************************************************************************\n",
      "epoch: 448 / 500, time cost: 47.00 sec, \n",
      "          Loss: [Train: 2.17951], [Test: 2.34885],\n",
      "          Accuracy: [prop score:  0.95382], [q1: 0.72011], [q0: 0.93668],\n",
      "          Effect: [ate-q], [train: 0.13321], [test: 0.13499]\n",
      "********************************************************************************\n",
      "epoch: 449 / 500, time cost: 45.20 sec, \n",
      "          Loss: [Train: 2.17862], [Test: 2.34911],\n",
      "          Accuracy: [prop score:  0.95379], [q1: 0.74219], [q0: 0.91628],\n",
      "          Effect: [ate-q], [train: 0.13320], [test: 0.13497]\n",
      "********************************************************************************\n",
      "epoch: 450 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.17837], [Test: 2.34944],\n",
      "          Accuracy: [prop score:  0.95383], [q1: 0.72950], [q0: 0.92902],\n",
      "          Effect: [ate-q], [train: 0.13392], [test: 0.13568]\n",
      "********************************************************************************\n",
      "epoch: 451 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.17806], [Test: 2.35023],\n",
      "          Accuracy: [prop score:  0.95382], [q1: 0.70661], [q0: 0.94275],\n",
      "          Effect: [ate-q], [train: 0.13240], [test: 0.13414]\n",
      "********************************************************************************\n",
      "epoch: 452 / 500, time cost: 47.59 sec, \n",
      "          Loss: [Train: 2.17735], [Test: 2.35097],\n",
      "          Accuracy: [prop score:  0.95384], [q1: 0.70220], [q0: 0.91954],\n",
      "          Effect: [ate-q], [train: 0.11913], [test: 0.12085]\n",
      "********************************************************************************\n",
      "epoch: 453 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.17731], [Test: 2.35084],\n",
      "          Accuracy: [prop score:  0.95399], [q1: 0.71851], [q0: 0.94364],\n",
      "          Effect: [ate-q], [train: 0.13822], [test: 0.14003]\n",
      "********************************************************************************\n",
      "epoch: 454 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.17670], [Test: 2.35203],\n",
      "          Accuracy: [prop score:  0.95392], [q1: 0.68849], [q0: 0.92433],\n",
      "          Effect: [ate-q], [train: 0.11697], [test: 0.11872]\n",
      "********************************************************************************\n",
      "epoch: 455 / 500, time cost: 44.93 sec, \n",
      "          Loss: [Train: 2.17545], [Test: 2.35192],\n",
      "          Accuracy: [prop score:  0.95390], [q1: 0.68694], [q0: 0.92331],\n",
      "          Effect: [ate-q], [train: 0.11652], [test: 0.11828]\n",
      "********************************************************************************\n",
      "epoch: 456 / 500, time cost: 46.81 sec, \n",
      "          Loss: [Train: 2.17571], [Test: 2.35140],\n",
      "          Accuracy: [prop score:  0.95389], [q1: 0.72218], [q0: 0.91765],\n",
      "          Effect: [ate-q], [train: 0.12711], [test: 0.12887]\n",
      "********************************************************************************\n",
      "epoch: 457 / 500, time cost: 45.15 sec, \n",
      "          Loss: [Train: 2.17495], [Test: 2.35201],\n",
      "          Accuracy: [prop score:  0.95387], [q1: 0.70411], [q0: 0.92949],\n",
      "          Effect: [ate-q], [train: 0.12553], [test: 0.12727]\n",
      "********************************************************************************\n",
      "epoch: 458 / 500, time cost: 47.41 sec, \n",
      "          Loss: [Train: 2.17456], [Test: 2.35251],\n",
      "          Accuracy: [prop score:  0.95391], [q1: 0.70785], [q0: 0.92214],\n",
      "          Effect: [ate-q], [train: 0.12341], [test: 0.12517]\n",
      "********************************************************************************\n",
      "epoch: 459 / 500, time cost: 45.62 sec, \n",
      "          Loss: [Train: 2.17357], [Test: 2.35296],\n",
      "          Accuracy: [prop score:  0.95393], [q1: 0.71946], [q0: 0.93227],\n",
      "          Effect: [ate-q], [train: 0.13378], [test: 0.13558]\n",
      "********************************************************************************\n",
      "epoch: 460 / 500, time cost: 46.63 sec, \n",
      "          Loss: [Train: 2.17373], [Test: 2.35271],\n",
      "          Accuracy: [prop score:  0.95399], [q1: 0.72363], [q0: 0.93187],\n",
      "          Effect: [ate-q], [train: 0.13625], [test: 0.13810]\n",
      "********************************************************************************\n",
      "epoch: 461 / 500, time cost: 44.03 sec, \n",
      "          Loss: [Train: 2.17269], [Test: 2.35382],\n",
      "          Accuracy: [prop score:  0.95398], [q1: 0.71123], [q0: 0.91667],\n",
      "          Effect: [ate-q], [train: 0.12371], [test: 0.12549]\n",
      "********************************************************************************\n",
      "epoch: 462 / 500, time cost: 46.44 sec, \n",
      "          Loss: [Train: 2.17218], [Test: 2.35387],\n",
      "          Accuracy: [prop score:  0.95393], [q1: 0.73674], [q0: 0.91677],\n",
      "          Effect: [ate-q], [train: 0.13497], [test: 0.13679]\n",
      "********************************************************************************\n",
      "epoch: 463 / 500, time cost: 44.58 sec, \n",
      "          Loss: [Train: 2.17173], [Test: 2.35387],\n",
      "          Accuracy: [prop score:  0.95394], [q1: 0.71184], [q0: 0.91995],\n",
      "          Effect: [ate-q], [train: 0.12583], [test: 0.12766]\n",
      "********************************************************************************\n",
      "epoch: 464 / 500, time cost: 46.57 sec, \n",
      "          Loss: [Train: 2.17197], [Test: 2.35466],\n",
      "          Accuracy: [prop score:  0.95389], [q1: 0.71656], [q0: 0.92303],\n",
      "          Effect: [ate-q], [train: 0.12982], [test: 0.13166]\n",
      "********************************************************************************\n",
      "epoch: 465 / 500, time cost: 43.92 sec, \n",
      "          Loss: [Train: 2.17109], [Test: 2.35456],\n",
      "          Accuracy: [prop score:  0.95401], [q1: 0.70325], [q0: 0.93495],\n",
      "          Effect: [ate-q], [train: 0.13139], [test: 0.13318]\n",
      "********************************************************************************\n",
      "epoch: 466 / 500, time cost: 45.56 sec, \n",
      "          Loss: [Train: 2.17027], [Test: 2.35490],\n",
      "          Accuracy: [prop score:  0.95396], [q1: 0.73630], [q0: 0.93459],\n",
      "          Effect: [ate-q], [train: 0.14513], [test: 0.14704]\n",
      "********************************************************************************\n",
      "epoch: 467 / 500, time cost: 43.31 sec, \n",
      "          Loss: [Train: 2.16977], [Test: 2.35540],\n",
      "          Accuracy: [prop score:  0.95410], [q1: 0.73222], [q0: 0.92638],\n",
      "          Effect: [ate-q], [train: 0.13878], [test: 0.14059]\n",
      "********************************************************************************\n",
      "epoch: 468 / 500, time cost: 45.08 sec, \n",
      "          Loss: [Train: 2.17011], [Test: 2.35591],\n",
      "          Accuracy: [prop score:  0.95406], [q1: 0.71164], [q0: 0.91676],\n",
      "          Effect: [ate-q], [train: 0.12587], [test: 0.12769]\n",
      "********************************************************************************\n",
      "epoch: 469 / 500, time cost: 43.42 sec, \n",
      "          Loss: [Train: 2.16860], [Test: 2.35679],\n",
      "          Accuracy: [prop score:  0.95408], [q1: 0.67676], [q0: 0.92013],\n",
      "          Effect: [ate-q], [train: 0.11494], [test: 0.11677]\n",
      "********************************************************************************\n",
      "epoch: 470 / 500, time cost: 47.12 sec, \n",
      "          Loss: [Train: 2.16902], [Test: 2.35694],\n",
      "          Accuracy: [prop score:  0.95402], [q1: 0.72652], [q0: 0.91112],\n",
      "          Effect: [ate-q], [train: 0.13017], [test: 0.13206]\n",
      "********************************************************************************\n",
      "epoch: 471 / 500, time cost: 44.36 sec, \n",
      "          Loss: [Train: 2.16860], [Test: 2.35732],\n",
      "          Accuracy: [prop score:  0.95407], [q1: 0.72154], [q0: 0.92777],\n",
      "          Effect: [ate-q], [train: 0.13588], [test: 0.13775]\n",
      "********************************************************************************\n",
      "epoch: 472 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.16778], [Test: 2.35835],\n",
      "          Accuracy: [prop score:  0.95411], [q1: 0.69637], [q0: 0.90873],\n",
      "          Effect: [ate-q], [train: 0.11795], [test: 0.11978]\n",
      "********************************************************************************\n",
      "epoch: 473 / 500, time cost: 43.58 sec, \n",
      "          Loss: [Train: 2.16741], [Test: 2.35774],\n",
      "          Accuracy: [prop score:  0.95415], [q1: 0.70382], [q0: 0.93543],\n",
      "          Effect: [ate-q], [train: 0.13374], [test: 0.13564]\n",
      "********************************************************************************\n",
      "epoch: 474 / 500, time cost: 46.78 sec, \n",
      "          Loss: [Train: 2.16692], [Test: 2.35842],\n",
      "          Accuracy: [prop score:  0.95410], [q1: 0.71357], [q0: 0.92207],\n",
      "          Effect: [ate-q], [train: 0.13063], [test: 0.13249]\n",
      "********************************************************************************\n",
      "epoch: 475 / 500, time cost: 44.85 sec, \n",
      "          Loss: [Train: 2.16669], [Test: 2.35835],\n",
      "          Accuracy: [prop score:  0.95417], [q1: 0.72915], [q0: 0.91366],\n",
      "          Effect: [ate-q], [train: 0.13421], [test: 0.13606]\n",
      "********************************************************************************\n",
      "epoch: 476 / 500, time cost: 46.99 sec, \n",
      "          Loss: [Train: 2.16585], [Test: 2.35963],\n",
      "          Accuracy: [prop score:  0.95415], [q1: 0.67148], [q0: 0.91104],\n",
      "          Effect: [ate-q], [train: 0.11103], [test: 0.11285]\n",
      "********************************************************************************\n",
      "epoch: 477 / 500, time cost: 44.48 sec, \n",
      "          Loss: [Train: 2.16499], [Test: 2.35988],\n",
      "          Accuracy: [prop score:  0.95410], [q1: 0.75711], [q0: 0.91374],\n",
      "          Effect: [ate-q], [train: 0.14851], [test: 0.15049]\n",
      "********************************************************************************\n",
      "epoch: 478 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.16423], [Test: 2.36024],\n",
      "          Accuracy: [prop score:  0.95412], [q1: 0.72602], [q0: 0.89158],\n",
      "          Effect: [ate-q], [train: 0.12509], [test: 0.12700]\n",
      "********************************************************************************\n",
      "epoch: 479 / 500, time cost: 45.04 sec, \n",
      "          Loss: [Train: 2.16459], [Test: 2.36052],\n",
      "          Accuracy: [prop score:  0.95420], [q1: 0.69890], [q0: 0.92389],\n",
      "          Effect: [ate-q], [train: 0.12848], [test: 0.13038]\n",
      "********************************************************************************\n",
      "epoch: 480 / 500, time cost: 47.20 sec, \n",
      "          Loss: [Train: 2.16410], [Test: 2.36076],\n",
      "          Accuracy: [prop score:  0.95425], [q1: 0.69506], [q0: 0.92542],\n",
      "          Effect: [ate-q], [train: 0.12792], [test: 0.12984]\n",
      "********************************************************************************\n",
      "epoch: 481 / 500, time cost: 44.94 sec, \n",
      "          Loss: [Train: 2.16348], [Test: 2.36202],\n",
      "          Accuracy: [prop score:  0.95435], [q1: 0.74479], [q0: 0.87979],\n",
      "          Effect: [ate-q], [train: 0.13023], [test: 0.13219]\n",
      "********************************************************************************\n",
      "epoch: 482 / 500, time cost: 46.87 sec, \n",
      "          Loss: [Train: 2.16280], [Test: 2.36215],\n",
      "          Accuracy: [prop score:  0.95422], [q1: 0.73470], [q0: 0.87715],\n",
      "          Effect: [ate-q], [train: 0.12528], [test: 0.12721]\n",
      "********************************************************************************\n",
      "epoch: 483 / 500, time cost: 44.89 sec, \n",
      "          Loss: [Train: 2.16254], [Test: 2.36148],\n",
      "          Accuracy: [prop score:  0.95431], [q1: 0.71847], [q0: 0.89867],\n",
      "          Effect: [ate-q], [train: 0.12561], [test: 0.12751]\n",
      "********************************************************************************\n",
      "epoch: 484 / 500, time cost: 47.29 sec, \n",
      "          Loss: [Train: 2.16209], [Test: 2.36217],\n",
      "          Accuracy: [prop score:  0.95422], [q1: 0.72865], [q0: 0.90900],\n",
      "          Effect: [ate-q], [train: 0.13524], [test: 0.13716]\n",
      "********************************************************************************\n",
      "epoch: 485 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.16173], [Test: 2.36241],\n",
      "          Accuracy: [prop score:  0.95433], [q1: 0.73173], [q0: 0.91388],\n",
      "          Effect: [ate-q], [train: 0.13878], [test: 0.14068]\n",
      "********************************************************************************\n",
      "epoch: 486 / 500, time cost: 47.31 sec, \n",
      "          Loss: [Train: 2.16060], [Test: 2.36262],\n",
      "          Accuracy: [prop score:  0.95433], [q1: 0.70413], [q0: 0.90407],\n",
      "          Effect: [ate-q], [train: 0.12276], [test: 0.12465]\n",
      "********************************************************************************\n",
      "epoch: 487 / 500, time cost: 44.75 sec, \n",
      "          Loss: [Train: 2.16043], [Test: 2.36385],\n",
      "          Accuracy: [prop score:  0.95434], [q1: 0.71260], [q0: 0.89706],\n",
      "          Effect: [ate-q], [train: 0.12370], [test: 0.12555]\n",
      "********************************************************************************\n",
      "epoch: 488 / 500, time cost: 46.91 sec, \n",
      "          Loss: [Train: 2.16020], [Test: 2.36369],\n",
      "          Accuracy: [prop score:  0.95437], [q1: 0.69189], [q0: 0.90288],\n",
      "          Effect: [ate-q], [train: 0.11950], [test: 0.12143]\n",
      "********************************************************************************\n",
      "epoch: 489 / 500, time cost: 45.23 sec, \n",
      "          Loss: [Train: 2.15956], [Test: 2.36382],\n",
      "          Accuracy: [prop score:  0.95442], [q1: 0.70305], [q0: 0.90927],\n",
      "          Effect: [ate-q], [train: 0.12619], [test: 0.12811]\n",
      "********************************************************************************\n",
      "epoch: 490 / 500, time cost: 47.28 sec, \n",
      "          Loss: [Train: 2.15870], [Test: 2.36464],\n",
      "          Accuracy: [prop score:  0.95432], [q1: 0.67432], [q0: 0.91931],\n",
      "          Effect: [ate-q], [train: 0.12119], [test: 0.12303]\n",
      "********************************************************************************\n",
      "epoch: 491 / 500, time cost: 44.62 sec, \n",
      "          Loss: [Train: 2.15870], [Test: 2.36504],\n",
      "          Accuracy: [prop score:  0.95432], [q1: 0.70982], [q0: 0.91243],\n",
      "          Effect: [ate-q], [train: 0.13055], [test: 0.13244]\n",
      "********************************************************************************\n",
      "epoch: 492 / 500, time cost: 46.47 sec, \n",
      "          Loss: [Train: 2.15833], [Test: 2.36499],\n",
      "          Accuracy: [prop score:  0.95431], [q1: 0.70797], [q0: 0.90151],\n",
      "          Effect: [ate-q], [train: 0.12543], [test: 0.12734]\n",
      "********************************************************************************\n",
      "epoch: 493 / 500, time cost: 44.81 sec, \n",
      "          Loss: [Train: 2.15782], [Test: 2.36640],\n",
      "          Accuracy: [prop score:  0.95427], [q1: 0.68710], [q0: 0.91200],\n",
      "          Effect: [ate-q], [train: 0.12200], [test: 0.12393]\n",
      "********************************************************************************\n",
      "epoch: 494 / 500, time cost: 47.11 sec, \n",
      "          Loss: [Train: 2.15675], [Test: 2.36616],\n",
      "          Accuracy: [prop score:  0.95427], [q1: 0.70776], [q0: 0.91564],\n",
      "          Effect: [ate-q], [train: 0.13267], [test: 0.13458]\n",
      "********************************************************************************\n",
      "epoch: 495 / 500, time cost: 45.26 sec, \n",
      "          Loss: [Train: 2.15669], [Test: 2.36653],\n",
      "          Accuracy: [prop score:  0.95440], [q1: 0.72091], [q0: 0.89928],\n",
      "          Effect: [ate-q], [train: 0.13017], [test: 0.13209]\n",
      "********************************************************************************\n",
      "epoch: 496 / 500, time cost: 46.79 sec, \n",
      "          Loss: [Train: 2.15596], [Test: 2.36706],\n",
      "          Accuracy: [prop score:  0.95433], [q1: 0.69288], [q0: 0.89317],\n",
      "          Effect: [ate-q], [train: 0.11719], [test: 0.11914]\n",
      "********************************************************************************\n",
      "epoch: 497 / 500, time cost: 44.51 sec, \n",
      "          Loss: [Train: 2.15583], [Test: 2.36772],\n",
      "          Accuracy: [prop score:  0.95434], [q1: 0.69278], [q0: 0.89711],\n",
      "          Effect: [ate-q], [train: 0.11829], [test: 0.12024]\n",
      "********************************************************************************\n",
      "epoch: 498 / 500, time cost: 47.01 sec, \n",
      "          Loss: [Train: 2.15513], [Test: 2.36796],\n",
      "          Accuracy: [prop score:  0.95440], [q1: 0.71864], [q0: 0.91017],\n",
      "          Effect: [ate-q], [train: 0.13507], [test: 0.13701]\n",
      "********************************************************************************\n",
      "epoch: 499 / 500, time cost: 45.35 sec, \n",
      "          Loss: [Train: 2.15532], [Test: 2.36841],\n",
      "          Accuracy: [prop score:  0.95441], [q1: 0.68608], [q0: 0.90574],\n",
      "          Effect: [ate-q], [train: 0.12066], [test: 0.12262]\n",
      "********************************************************************************\n",
      "epoch: 500 / 500, time cost: 46.95 sec, \n",
      "          Loss: [Train: 2.15437], [Test: 2.36873],\n",
      "          Accuracy: [prop score:  0.95435], [q1: 0.74071], [q0: 0.89705],\n",
      "          Effect: [ate-q], [train: 0.13993], [test: 0.14195]\n",
      "********************************************************************************\n",
      "Finish training...\n"
     ]
    }
   ],
   "source": [
    "rs_loss, rq1_loss, rq0_loss = [0.] * 3\n",
    "\n",
    "train_loss_hist, test_loss_hist, est_effect = [], [], []\n",
    "for e in range(1, epoch + 1):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        run_loss += loss.item()\n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW in 500 epochslr=5e-6, no scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 501 / 800, time cost: 47.43 sec, \n",
      "          Loss: [Train: 2.15395], [Test: 2.36882],\n",
      "          Accuracy: [prop score:  0.95440], [q1: 0.72793], [q0: 0.91147],\n",
      "          Effect: [ate-q], [train: 0.14053], [test: 0.14255]\n",
      "********************************************************************************\n",
      "epoch: 502 / 800, time cost: 45.52 sec, \n",
      "          Loss: [Train: 2.15336], [Test: 2.36920],\n",
      "          Accuracy: [prop score:  0.95434], [q1: 0.70324], [q0: 0.88207],\n",
      "          Effect: [ate-q], [train: 0.11822], [test: 0.12022]\n",
      "********************************************************************************\n",
      "epoch: 503 / 800, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.15272], [Test: 2.36974],\n",
      "          Accuracy: [prop score:  0.95442], [q1: 0.69903], [q0: 0.90299],\n",
      "          Effect: [ate-q], [train: 0.12502], [test: 0.12699]\n",
      "********************************************************************************\n",
      "epoch: 504 / 800, time cost: 45.29 sec, \n",
      "          Loss: [Train: 2.15250], [Test: 2.36958],\n",
      "          Accuracy: [prop score:  0.95442], [q1: 0.69790], [q0: 0.88865],\n",
      "          Effect: [ate-q], [train: 0.11937], [test: 0.12132]\n",
      "********************************************************************************\n",
      "epoch: 505 / 800, time cost: 46.64 sec, \n",
      "          Loss: [Train: 2.15185], [Test: 2.37037],\n",
      "          Accuracy: [prop score:  0.95456], [q1: 0.72920], [q0: 0.90315],\n",
      "          Effect: [ate-q], [train: 0.13856], [test: 0.14058]\n",
      "********************************************************************************\n",
      "epoch: 506 / 800, time cost: 44.64 sec, \n",
      "          Loss: [Train: 2.15115], [Test: 2.37106],\n",
      "          Accuracy: [prop score:  0.95440], [q1: 0.69828], [q0: 0.88714],\n",
      "          Effect: [ate-q], [train: 0.12027], [test: 0.12224]\n",
      "********************************************************************************\n",
      "epoch: 507 / 800, time cost: 46.56 sec, \n",
      "          Loss: [Train: 2.15103], [Test: 2.37132],\n",
      "          Accuracy: [prop score:  0.95437], [q1: 0.69604], [q0: 0.90417],\n",
      "          Effect: [ate-q], [train: 0.12590], [test: 0.12790]\n",
      "********************************************************************************\n",
      "epoch: 508 / 800, time cost: 45.25 sec, \n",
      "          Loss: [Train: 2.15111], [Test: 2.37237],\n",
      "          Accuracy: [prop score:  0.95453], [q1: 0.71250], [q0: 0.89400],\n",
      "          Effect: [ate-q], [train: 0.12839], [test: 0.13043]\n",
      "********************************************************************************\n",
      "epoch: 509 / 800, time cost: 46.86 sec, \n",
      "          Loss: [Train: 2.15021], [Test: 2.37219],\n",
      "          Accuracy: [prop score:  0.95443], [q1: 0.72848], [q0: 0.90954],\n",
      "          Effect: [ate-q], [train: 0.14367], [test: 0.14571]\n",
      "********************************************************************************\n",
      "epoch: 510 / 800, time cost: 44.02 sec, \n",
      "          Loss: [Train: 2.14995], [Test: 2.37292],\n",
      "          Accuracy: [prop score:  0.95443], [q1: 0.69049], [q0: 0.89363],\n",
      "          Effect: [ate-q], [train: 0.11954], [test: 0.12150]\n",
      "********************************************************************************\n",
      "epoch: 511 / 800, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.14921], [Test: 2.37327],\n",
      "          Accuracy: [prop score:  0.95437], [q1: 0.69129], [q0: 0.87903],\n",
      "          Effect: [ate-q], [train: 0.11505], [test: 0.11699]\n",
      "********************************************************************************\n",
      "epoch: 512 / 800, time cost: 44.93 sec, \n",
      "          Loss: [Train: 2.14889], [Test: 2.37383],\n",
      "          Accuracy: [prop score:  0.95451], [q1: 0.71114], [q0: 0.89233],\n",
      "          Effect: [ate-q], [train: 0.12861], [test: 0.13062]\n",
      "********************************************************************************\n",
      "epoch: 513 / 800, time cost: 47.31 sec, \n",
      "          Loss: [Train: 2.14778], [Test: 2.37476],\n",
      "          Accuracy: [prop score:  0.95453], [q1: 0.75396], [q0: 0.90402],\n",
      "          Effect: [ate-q], [train: 0.15373], [test: 0.15584]\n",
      "********************************************************************************\n",
      "epoch: 514 / 800, time cost: 45.36 sec, \n",
      "          Loss: [Train: 2.14767], [Test: 2.37455],\n",
      "          Accuracy: [prop score:  0.95444], [q1: 0.70496], [q0: 0.88805],\n",
      "          Effect: [ate-q], [train: 0.12472], [test: 0.12668]\n",
      "********************************************************************************\n",
      "epoch: 515 / 800, time cost: 46.85 sec, \n",
      "          Loss: [Train: 2.14784], [Test: 2.37551],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.69410], [q0: 0.88542],\n",
      "          Effect: [ate-q], [train: 0.11992], [test: 0.12189]\n",
      "********************************************************************************\n",
      "epoch: 516 / 800, time cost: 44.70 sec, \n",
      "          Loss: [Train: 2.14749], [Test: 2.37539],\n",
      "          Accuracy: [prop score:  0.95457], [q1: 0.68254], [q0: 0.88950],\n",
      "          Effect: [ate-q], [train: 0.11718], [test: 0.11920]\n",
      "********************************************************************************\n",
      "epoch: 517 / 800, time cost: 47.12 sec, \n",
      "          Loss: [Train: 2.14658], [Test: 2.37602],\n",
      "          Accuracy: [prop score:  0.95459], [q1: 0.69707], [q0: 0.88729],\n",
      "          Effect: [ate-q], [train: 0.12217], [test: 0.12414]\n",
      "********************************************************************************\n",
      "epoch: 518 / 800, time cost: 45.02 sec, \n",
      "          Loss: [Train: 2.14616], [Test: 2.37594],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.70368], [q0: 0.88429],\n",
      "          Effect: [ate-q], [train: 0.12332], [test: 0.12534]\n",
      "********************************************************************************\n",
      "epoch: 519 / 800, time cost: 46.75 sec, \n",
      "          Loss: [Train: 2.14564], [Test: 2.37697],\n",
      "          Accuracy: [prop score:  0.95465], [q1: 0.67693], [q0: 0.90579],\n",
      "          Effect: [ate-q], [train: 0.12281], [test: 0.12473]\n",
      "********************************************************************************\n",
      "epoch: 520 / 800, time cost: 44.52 sec, \n",
      "          Loss: [Train: 2.14502], [Test: 2.37720],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.69017], [q0: 0.88857],\n",
      "          Effect: [ate-q], [train: 0.12043], [test: 0.12247]\n",
      "********************************************************************************\n",
      "epoch: 521 / 800, time cost: 46.61 sec, \n",
      "          Loss: [Train: 2.14476], [Test: 2.37719],\n",
      "          Accuracy: [prop score:  0.95461], [q1: 0.72328], [q0: 0.89254],\n",
      "          Effect: [ate-q], [train: 0.13615], [test: 0.13823]\n",
      "********************************************************************************\n",
      "epoch: 522 / 800, time cost: 45.06 sec, \n",
      "          Loss: [Train: 2.14405], [Test: 2.37803],\n",
      "          Accuracy: [prop score:  0.95464], [q1: 0.71004], [q0: 0.89470],\n",
      "          Effect: [ate-q], [train: 0.13216], [test: 0.13425]\n",
      "********************************************************************************\n",
      "epoch: 523 / 800, time cost: 47.13 sec, \n",
      "          Loss: [Train: 2.14349], [Test: 2.37855],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.65700], [q0: 0.89594],\n",
      "          Effect: [ate-q], [train: 0.11339], [test: 0.11533]\n",
      "********************************************************************************\n",
      "epoch: 524 / 800, time cost: 44.68 sec, \n",
      "          Loss: [Train: 2.14300], [Test: 2.37871],\n",
      "          Accuracy: [prop score:  0.95452], [q1: 0.71059], [q0: 0.87321],\n",
      "          Effect: [ate-q], [train: 0.12478], [test: 0.12675]\n",
      "********************************************************************************\n",
      "epoch: 525 / 800, time cost: 46.52 sec, \n",
      "          Loss: [Train: 2.14263], [Test: 2.37921],\n",
      "          Accuracy: [prop score:  0.95457], [q1: 0.68910], [q0: 0.87544],\n",
      "          Effect: [ate-q], [train: 0.11579], [test: 0.11778]\n",
      "********************************************************************************\n",
      "epoch: 526 / 800, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.14240], [Test: 2.38012],\n",
      "          Accuracy: [prop score:  0.95464], [q1: 0.71333], [q0: 0.86476],\n",
      "          Effect: [ate-q], [train: 0.12258], [test: 0.12466]\n",
      "********************************************************************************\n",
      "epoch: 527 / 800, time cost: 48.97 sec, \n",
      "          Loss: [Train: 2.14200], [Test: 2.38090],\n",
      "          Accuracy: [prop score:  0.95467], [q1: 0.68805], [q0: 0.86557],\n",
      "          Effect: [ate-q], [train: 0.11355], [test: 0.11558]\n",
      "********************************************************************************\n",
      "epoch: 528 / 800, time cost: 45.54 sec, \n",
      "          Loss: [Train: 2.14164], [Test: 2.38091],\n",
      "          Accuracy: [prop score:  0.95467], [q1: 0.70983], [q0: 0.87111],\n",
      "          Effect: [ate-q], [train: 0.12318], [test: 0.12521]\n",
      "********************************************************************************\n",
      "epoch: 529 / 800, time cost: 47.69 sec, \n",
      "          Loss: [Train: 2.14162], [Test: 2.38161],\n",
      "          Accuracy: [prop score:  0.95463], [q1: 0.68260], [q0: 0.88892],\n",
      "          Effect: [ate-q], [train: 0.12092], [test: 0.12293]\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for e in range(501, 801):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        run_loss += loss.item()\n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / 800, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
