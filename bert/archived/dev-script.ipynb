{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from data import LineByLineTextDataset\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from utils import DATA_PATH, make_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86803"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = WordLevelBertTokenizer(create_vocab(merged=True, uni_diag=True))    \n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_group = [str(i) for i in range(10)]\n",
    "\n",
    "vocab, size = {}, 0\n",
    "for group in user_group:\n",
    "    read = os.path.join(DATA_PATH, f'group_{group}.csv')\n",
    "\n",
    "    with open(read, 'r') as raw:\n",
    "        for line in raw:\n",
    "            if '2860253' in line:\n",
    "                print(line)\n",
    "                break\n",
    "            line = line.replace('\\n', '')\n",
    "            user, tokens = line.split(',')\n",
    "            tokens = tokens.strip()\n",
    "            token_list = tokens.split(' ')\n",
    "\n",
    "            for token in token_list:\n",
    "                if token not in ['[SEP]', 'document', '']:\n",
    "                    if token in vocab:\n",
    "                        # If a token is existed, don't do anything.\n",
    "                        pass\n",
    "                    else:\n",
    "                        # A new token: tokens value will start from 0\n",
    "                        vocab[token] = size\n",
    "                        size += 1\n",
    "\n",
    "for j, v in enumerate(['[UNK]', '[SEP]', '[CLS]']):\n",
    "    vocab[v] = size + j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_vocab, icd10_vocab = {}, {}\n",
    "for k in vocab:\n",
    "    if '10_' in k:\n",
    "        icd10_vocab[k] = 1\n",
    "    elif '9_' in k:\n",
    "        icd9_vocab[k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/turbo/lsa-regier/emr-data/vocabs/vocab_merged.json'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_write = os.path.join('/home/liutianc/emr/data/', 'diag_stat.json')\n",
    "proc_write = os.path.join('/home/liutianc/emr/data/', 'proc_stat.json')\n",
    "pharm_write = os.path.join('/home/liutianc/emr/data/', 'pharm_stat.json')\n",
    "\n",
    "with open(diag_write, 'r') as to_write:\n",
    "    diags = json.load(to_write)\n",
    "        \n",
    "with open(proc_write, 'r') as to_write:\n",
    "    procs = json.load(to_write)\n",
    "    \n",
    "with open(pharm_write, 'r') as to_write:\n",
    "    pharms = json.load(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "diag_tokens = {token for token in vocab if 'diag' in token}\n",
    "diag_tokens = set([token.replace('diag:', '') for token in diag_tokens])\n",
    "\n",
    "miss_pv = 0\n",
    "miss_key = []\n",
    "for key in set(diags).difference(diag_tokens_list):\n",
    "    miss_pv += diags[key]\n",
    "    miss_key.append(key)\n",
    "print(miss_pv)\n",
    "print(len(miss_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "proc_tokens = {token for token in vocab if 'proc' in token}\n",
    "proc_tokens = set([token.replace('proc:', '') for token in proc_tokens])\n",
    "\n",
    "miss_pv = 0\n",
    "miss_key = []\n",
    "for key in set(procs).difference(proc_tokens):\n",
    "    miss_pv += procs[key]\n",
    "    miss_key.append(key)\n",
    "print(miss_pv)\n",
    "print(len(miss_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "pharm_tokens = set({token for token in vocab if not 'proc' in token and not 'diag' in token})\n",
    "# proc_tokens = set([token.replace('proc:', '') for token in proc_tokens])\n",
    "\n",
    "miss_pv = 0\n",
    "miss_key = []\n",
    "for key in set(pharms).difference(pharm_tokens):\n",
    "    miss_pv += pharms[key]\n",
    "    miss_key.append(key)\n",
    "\n",
    "print(miss_pv)\n",
    "print(len(miss_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'icd:nan_diag:5601': 'icd:9_diag:5601',\n",
       " 'icd:nan_proc:0DJD8ZZ': 'icd:10_proc:0DJD8ZZ',\n",
       " 'icd:nan_diag:5733': 'icd:9_diag:5733',\n",
       " 'icd:nan_proc:0DP67UZ': 'icd:10_proc:0DP67UZ',\n",
       " 'icd:nan_proc:0DHA7UZ': 'icd:10_proc:0DHA7UZ',\n",
       " 'icd:nan_proc:B2151ZZ': 'icd:10_proc:B2151ZZ',\n",
       " 'icd:nan_proc:3E0H8GC': 'icd:10_proc:3E0H8GC',\n",
       " 'icd:nan_proc:B2111ZZ': 'icd:9_proc:B2111ZZ',\n",
       " 'icd:nan_proc:4A023N7': 'icd:10_proc:4A023N7',\n",
       " 'icd:nan_proc:0DB68ZX': 'icd:10_proc:0DB68ZX',\n",
       " 'icd:nan_proc:02HV33Z': 'icd:10_proc:02HV33Z',\n",
       " 'icd:nan_diag:2630': 'icd:9_diag:2630',\n",
       " 'icd:nan_diag:5770': 'icd:9_diag:5770'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid = {}\n",
    "valid = set({token for token in vocab if 'nan' not in token})\n",
    "for key in set({token for token in vocab if 'nan' in token}):\n",
    "    code = key.split('_')[1]\n",
    "    for token in valid:\n",
    "        if 'diag' in token or 'proc' in token:\n",
    "            if code == token.split('_')[1]:\n",
    "                invalid[key] = token\n",
    "#                 print(f'miss: {code}, valid: {token}')\n",
    "invalid        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '560499293410900,2018-01-15,icd:10_diag:K56690 icd:10_diag:K760 icd:10_diag:I10 icd:10_diag:K56699 icd:10_diag:R1084 icd:10_diag:E119 icd:10_diag:E785 icd:10_diag:E871 icd:10_diag:G8929 icd:10_diag:I10 icd:10_diag:I2510 icd:10_diag:K56600 icd:10_diag:M549 icd:10_diag:N151 icd:10_diag:N179 icd:10_diag:N319 icd:10_diag:Z794 icd:10_diag:Z7982 icd:10_diag:Z79899 icd:10_diag:Z8711 icd:10_diag:Z950 icd:10_diag:Z955 icd:10_diag:Z981 icd:10_diag:Z993 icd:nan_proc:02HV33Z icd:10_proc:02HV33Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in invalid:\n",
    "    test = test.replace(key, invalid[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set({token for token in vocab if 'nan' in token}))\n",
    "len(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LineByLineTextDataset(tokenizer=tokenizer, data_type='merged', max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "config = BertConfig(vocab_size=len(tokenizer), max_position_embeddings=max_length,\n",
    "                    num_attention_heads=4,\n",
    "                    num_hidden_layers=4,\n",
    "                    hidden_size=128,\n",
    "                    type_vocab_size=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./result-dev/MLM', overwrite_output_dir=True,\n",
    "                              num_train_epochs=1,\n",
    "                              per_device_train_batch_size=bsz,\n",
    "                              save_steps=10_000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=mlm_collator,\n",
    "                  train_dataset=dataset,\n",
    "                  prediction_loss_only=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Select diag data.\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,N9NFNVL9L8,J029,01,10,2,2015-12-17\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,339J9VJJ3R,2724,01,9,2,2013-11-19\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,NF9NVLNF3V,E782,02,10,2,2017-04-24\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,N38RF9RJLV,D125,01,10,2,2016-11-21\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200782112,39J998VRFO,2572,01,9,2,2011-11-15\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,39LR9FFVVJ,79981,01,9,2,2012-11-06\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,NLVFLV98N3,H6982,01,10,2,2018-02-26\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200782112,JF39FJO39F,72252,01,9,2,2010-06-11\n",
      "\n",
      "Patid,Clmid,Diag,Diag_Position,Icd_Flag,Loc_cd,Fst_Dt\n",
      "\n",
      "560499200160862,38RO383O38,2724,02,9,2,2014-11-06\n",
      "\n",
      "********************************************************************************************************************************************************************************************************\n",
      "Start: Select proc data.\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202035631,NOJ8R989VJ,10,2W3DX1Z,01,2016-06-24\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202033685,ONNLL9LLJL,10,03HY32Z,01,2018-11-20\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499201173714,39ORROF833,9,741,01,2012-02-03\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202293083,OV9RO3F9JO,9,4281,01,2010-05-28\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202147975,OVV3N9RVFJ,9,8659,01,2011-10-17\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202000262,33399J3FVR,9,9955,01,2014-01-09\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202056285,338NVRJVOV,9,1742,03,2013-12-11\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499202035631,NFJ3FLLLO8,10,0T2BX0Z,01,2017-03-08\n",
      "\n",
      "Patid,Clmid,Icd_Flag,Proc,Proc_Position,Fst_Dt\n",
      "\n",
      "560499201173714,3ROLF9LL9O,9,741,01,2015-05-19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "input_path = '/nfs/turbo/lsa-regier/OPTUMInsight_csv/'\n",
    "diags = [str(x) for x in Path(input_path).glob(\"**/diag_201[0-9].csv\")]\n",
    "procs = [str(x) for x in Path(input_path).glob(\"**/proc_201[0-9].csv\")]\n",
    "pharms = [str(x) for x in Path(input_path).glob(\"**/pharm_201[0-9].csv\")]\n",
    "\n",
    "diag_pattern = '{},' * 6 + '{}\\n'\n",
    "proc_pattern = '{},' * 5 + '{}\\n'\n",
    "pharm_pattern = '{},' * 5 + '{}\\n'\n",
    "\n",
    "print('Start: Select diag data.')\n",
    "for file in diags:\n",
    "    file_name = os.path.split(file)[1]\n",
    "\n",
    "#     print(f'Start: {file_name}.')\n",
    "    with open(file, newline='') as infile:\n",
    "        diagreader = csv.reader(infile)\n",
    "        row_num = 0\n",
    "        for row in diagreader:\n",
    "            row_num += 1 \n",
    "#             row = [cell.replace('.0', '').strip() for cell in row]\n",
    "            row = [cell.strip() for cell in row]\n",
    "            patid = row[0].split('.0')[0].strip()\n",
    "            claimid = row[2].split('.0')[0].strip()\n",
    "            Diag = row[3].split('.0')[0].strip()\n",
    "            Diag_Position = row[4].split('.0')[0].strip()\n",
    "            Icd_Flag = row[5].split('.0')[0].strip()\n",
    "            Loc_cd = row[6].split('.0')[0].strip()\n",
    "            Fst_Dt = row[10].strip()\n",
    "\n",
    "            select_row = diag_pattern.format(patid, claimid, Diag, Diag_Position, Icd_Flag, Loc_cd, Fst_Dt)\n",
    "            print(select_row)\n",
    "            \n",
    "            if row_num >= 2:\n",
    "                break\n",
    "\n",
    "print('*' * 200)\n",
    "print('Start: Select proc data.')\n",
    "for file in procs:\n",
    "    file_name = os.path.split(file)[1]\n",
    "\n",
    "#     print(f'Start: {file_name}.')\n",
    "    with open(file, newline='') as infile:\n",
    "        procreader = csv.reader(infile)\n",
    "\n",
    "        row_num = 0\n",
    "        for row in procreader:\n",
    "            row_num += 1\n",
    "            \n",
    "#                 row = [cell.replace('.0', '').strip() for cell in row]\n",
    "            row = [cell.strip() for cell in row]\n",
    "\n",
    "            patid = row[0].split('.0')[0].strip()\n",
    "            claimid = row[2].split('.0')[0].strip()\n",
    "            Icd_Flag = row[3].split('.0')[0].strip()\n",
    "            Proc = row[4].split('.0')[0].strip()\n",
    "            Proc_Position = row[5].split('.0')[0].strip()\n",
    "            Fst_Dt = row[8]\n",
    "            select_row = proc_pattern.format(patid, claimid, Icd_Flag, Proc, Proc_Position, Fst_Dt)\n",
    "            print(select_row)\n",
    "            if row_num >= 2:\n",
    "                break\n",
    "\n",
    "\n",
    "# logger.info('Start: Select pharm data.')\n",
    "# for file in pharms:\n",
    "#     file_name = os.path.split(file)[1]\n",
    "#     output_file = os.path.join(output_path, file_name)\n",
    "\n",
    "#     logger.info(f'Start: {file_name}.')\n",
    "#     with open(file, newline='') as infile:\n",
    "#         with open(output_file, 'w') as outfile:\n",
    "#             pharmreader = csv.reader(infile)\n",
    "\n",
    "#             for row in pharmreader:\n",
    "#                 row = [cell.strip() for cell in row]\n",
    "\n",
    "#                 patid = row[0].split('.0')[0].strip()\n",
    "#                 claimid = row[7].split('.0')[0].strip()\n",
    "#                 Fill_Date = row[14].split('.0')[0].strip()\n",
    "#                 Gnrc_Nm = row[19].split('.0')[0].strip()\n",
    "#                 Quantity = row[25].split('.0')[0].strip()\n",
    "#                 Rfl_Nbr = row[26].split('.0')[0].strip()\n",
    "\n",
    "#                 Gnrc_Nm = Gnrc_Nm.replace('\"', '').replace(',', '_').replace(' ', '_').strip()\n",
    "\n",
    "#                 select_row = pharm_pattern.format(patid, claimid, Fill_Date, Gnrc_Nm, Quantity, Rfl_Nbr)\n",
    "#                 outfile.write(select_row)\n",
    "\n",
    "#     logger.info(f'Finish: {file_name}.')\n",
    "# logger.info('Finish: Select pharm data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
