{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from data import CausalBertDataset, MLMDataset\n",
    "from causal_bert import CausalBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                            group=[9], max_length=512, min_length=10,\n",
    "                            truncate_method='first', device=device)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=64, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Causal-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 10, running loss: 0.48283.\n",
      "epoch: 2 / 10, running loss: 0.41434.\n",
      "epoch: 3 / 10, running loss: 0.40337.\n",
      "epoch: 4 / 10, running loss: 0.39869.\n",
      "epoch: 5 / 10, running loss: 0.39505.\n",
      "epoch: 6 / 10, running loss: 0.39134.\n",
      "epoch: 7 / 10, running loss: 0.38911.\n",
      "epoch: 8 / 10, running loss: 0.38652.\n",
      "epoch: 9 / 10, running loss: 0.38459.\n",
      "epoch: 10 / 10, running loss: 0.38243.\n"
     ]
    }
   ],
   "source": [
    "trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-2166633/'\n",
    "model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "token_embed = model.get_input_embeddings()\n",
    "\n",
    "causalBert = CausalBert(token_embed).to(device)\n",
    "optimizer = torch.optim.Adam(causalBert.parameters(), lr=5e-4)\n",
    "\n",
    "q_loss = nn.BCELoss()\n",
    "prop_score_loss = nn.BCELoss()\n",
    "\n",
    "epoch = 10\n",
    "running_loss = 0.\n",
    "for e in range(1, epoch + 1):\n",
    "    for idx, (tokens, treatment, response) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = causalBert(tokens)\n",
    "        s_loss = prop_score_loss(prop_score, treatment)\n",
    "        q1_loss = q_loss(q1[response==1], response[response==1])\n",
    "        q0_loss = q_loss(q0[response==0], response[response==0])\n",
    "        s_loss.backward(retain_graph=True)\n",
    "        q1_loss.backward(retain_graph=True)\n",
    "        q0_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += s_loss.item() + q1_loss.item() + q0_loss.item()\n",
    "\n",
    "    print(f'epoch: {e} / {epoch}, running loss: {round(running_loss / (idx + 1), 5)}.')\n",
    "    running_loss = 0.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
