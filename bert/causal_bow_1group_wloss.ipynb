{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from data import CausalBertDataset, MLMDataset\n",
    "from causal_bert import CausalBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 256\n",
<<<<<<< HEAD
    "epoch = 500"
=======
    "epoch = 50\n",
    "hidden_size = 64\n",
    "lr = 1e-5"
>>>>>>> 4ac631a77b28de01a83a4aca96454a5e1990a54a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def true_casual_effect(data_loader, effect='ate', estimation='q'):\n",
    "    assert effect == 'ate' and estimation == 'q', f'unallowed effect/estimation: {effect}/{estimation}'\n",
    "    \n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    Q1 = dataset.treatment * dataset.response + (1 - dataset.treatment) * dataset.pseudo_response\n",
    "    Q1 = Q1.cpu().data.numpy().squeeze()\n",
    "\n",
    "    Q0 = dataset.treatment * dataset.pseudo_response + (1 - dataset.treatment) * dataset.response\n",
    "    Q0 = Q0.cpu().data.numpy().squeeze()\n",
    "\n",
    "    treatment = dataset.treatment.cpu().data.numpy().squeeze()\n",
    "    prop_scores = dataset.prop_scores.cpu().data.numpy().squeeze()\n",
    "    \n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (treatment * (Q1 - Q0))\n",
    "            return phi.sum() / treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            return (Q1 - Q0).mean()\n",
    "        \n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            return phi / treatment.mean()\n",
    "        elif effect == 'ate': \n",
    "            return phi\n",
    "        \n",
    "def est_casual_effect(data_loader, model, effect='ate', estimation='q', evaluate=True, **kwargs):\n",
    "    # We use `real_treatment` here to emphasize the estimations use real instead of estimated treatment.\n",
    "    real_response, real_treatment, real_prop_scores = [], [], []\n",
    "    prop_scores, Q1, Q0 = [], [], []\n",
    "    \n",
    "    if evaluate:\n",
    "        g_loss = kwargs.get('g_loss')\n",
    "        q_loss = kwargs.get('q_loss')\n",
    "        g_loss_test, q1_loss_test, q0_loss_test  = [], [], []\n",
    "        \n",
    "    model.eval()\n",
    "    for idx, (tokens, treatment, response, real_prop_score) in enumerate(data_loader):\n",
    "        real_response.append(response.cpu().data.numpy().squeeze())\n",
    "        real_treatment.append(treatment.cpu().data.numpy().squeeze())\n",
    "        real_prop_scores.append(real_prop_score.cpu().data.numpy().squeeze())\n",
    "\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        prop_scores.append(prop_score.cpu().data.numpy().squeeze())\n",
    "        Q1.append(q1.cpu().data.numpy().squeeze())\n",
    "        Q0.append(q0.cpu().data.numpy().squeeze())\n",
    "        \n",
    "        # Evaulate loss\n",
    "        if evaluate:\n",
    "            g_loss_val  = g_loss(prop_score, treatment)\n",
    "            q1_loss_val = q_loss(q1[treatment==1], response[treatment==1])\n",
    "            q0_loss_val = q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "            g_loss_test.append(g_loss_val.item())\n",
    "            q1_loss_test.append(q1_loss_val.item())\n",
    "            q0_loss_test.append(q0_loss_val.item())\n",
    "    \n",
    "    g_loss = np.array(g_loss_test).mean() if evaluate else None\n",
    "    q1_loss = np.array(q1_loss_test).mean() if evaluate else None\n",
    "    q0_loss = np.array(q0_loss_test).mean() if evaluate else None\n",
    "\n",
    "    Q1 = np.concatenate(Q1, axis=0)\n",
    "    Q0 = np.concatenate(Q0, axis=0)\n",
    "    prop_scores = np.concatenate(prop_scores, axis=0)\n",
    "    \n",
    "    real_response = np.concatenate(real_response, axis=0)\n",
    "    real_treatment = np.concatenate(real_treatment, axis=0)\n",
    "    real_prop_scores = np.concatenate(real_prop_scores, axis=0)\n",
    "    \n",
    "    # Evaluate accuracy.\n",
    "    if evaluate:\n",
    "        dataset = data_loader.dataset\n",
    "        \n",
    "        real_q1_prob = sigmoid(dataset.alpha + dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        real_q0_prob = sigmoid(dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        thre = (real_q1_prob + real_q0_prob) / 2\n",
    "\n",
    "    # prop score: real and estimated must locate one the same side of 0.5.\n",
    "    prop_accu = (1. * (((real_prop_scores - .5) * (prop_scores - .5)) > 0.)).mean() if evaluate else None\n",
    "    # q: estimate is more close to corresponding real value than the other.\n",
    "    q1_accu = (1. * (dataset.alpha > 0) * (Q1 > thre)).mean() if evaluate else None\n",
    "    q0_accu = (1. * (dataset.alpha > 0) * (Q0 < thre)).mean() if evaluate else None\n",
    "\n",
    "\n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (real_treatment * (Q1 - Q0))\n",
    "            effect = phi.sum() / real_treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            effect = (Q1 - Q0).mean()\n",
    "\n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            effect = phi / real_treatment.mean()\n",
    "        elif effect == 'ate':\n",
    "            effect = phi\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return effect, g_loss, q1_loss, q0_loss, prop_accu, q1_accu, q0_accu\n",
    "\n",
    "def show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch):\n",
    "    train_loss_hist = np.array(train_loss_hist)\n",
    "    test_loss_hist = np.array(test_loss_hist)\n",
    "    est_effect = np.array(est_effect)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    lns1 = ax.plot(np.arange(epoch), test_loss_hist, label='Eval loss')\n",
    "    ax_r = plt.twinx()\n",
    "    lns2 = ax_r.plot(np.arange(epoch), est_effect, color='coral', label='Estimate ATE')\n",
    "    lns3 = ax_r.plot(np.arange(epoch), np.ones(epoch) * real, color='red', ls='--', label='Real ATE')\n",
    "    lns4 = ax_r.plot(np.arange(epoch), np.ones(epoch) * unadjust, color='green', ls='--', label='Unadjusted ATE')\n",
    "\n",
    "    lns = lns1+lns2+lns3+lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax_r.legend(lns, labs, loc=0)\n",
    "    ax.set_ylabel('Eval loss')\n",
    "    ax_r.set_ylabel('ATEs')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "alpha = 0.25\n",
    "beta = 1.\n",
    "c = 0.2\n",
    "i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Load training set in 107.79 sec\n",
      "Load validation set in 85.96 sec\n"
=======
      "Load training set in 80.11 sec\n",
      "Load validation set in 70.87 sec\n"
>>>>>>> 4ac631a77b28de01a83a4aca96454a5e1990a54a
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "                             group=list(range(1)), max_length=512, min_length=10,\n",
    "                             truncate_method='first', device=device, seed=1)\n",
    "\n",
    "print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "start = time.time()\n",
    "testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                            alpha=alpha, beta=beta, c=c, i=i, \n",
    "                            group=[9], max_length=512, min_length=10,\n",
    "                            truncate_method='first', device=device)\n",
    "\n",
    "print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [effect: ate], [estimation: q], [value: 0.06317]\n",
      "Unadjusted: [value: 0.0919]\n"
     ]
    }
   ],
   "source": [
    "real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "token_embed = model.get_input_embeddings()\n",
    "model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=hidden_size, prop_is_logit=True).to(device)\n",
    "\n",
    "pos_portion = trainset.treatment.mean()\n",
    "pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epoch_iter = len(train_loader)\n",
    "total_steps = epoch * epoch_iter\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 2, num_training_steps=total_steps)\n",
    "\n",
    "q_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCELoss()\n",
    "prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Please specify the effect and estimation we want to use here.\n",
    "effect = 'ate'\n",
    "estimation = 'q'\n",
    "\n",
    "effect = effect.lower()\n",
    "estimation = estimation.lower()\n",
    "assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 1 / 500, time cost: 90.02 sec, \n",
      "          Loss: [Train: 2.42559], [Test: 2.42827],\n",
      "          Effect: [ate-q], [train: 0.09067], [test: 0.09067]\n",
      "********************************************************************************\n",
      "epoch: 2 / 500, time cost: 90.05 sec, \n",
      "          Loss: [Train: 2.42161], [Test: 2.42441],\n",
      "          Effect: [ate-q], [train: 0.07691], [test: 0.07691]\n",
      "********************************************************************************\n",
      "epoch: 3 / 500, time cost: 90.57 sec, \n",
      "          Loss: [Train: 2.41663], [Test: 2.41964],\n",
      "          Effect: [ate-q], [train: 0.08008], [test: 0.08008]\n",
      "********************************************************************************\n",
      "epoch: 4 / 500, time cost: 83.90 sec, \n",
      "          Loss: [Train: 2.41099], [Test: 2.41492],\n",
      "          Effect: [ate-q], [train: 0.09079], [test: 0.09080]\n",
      "********************************************************************************\n",
      "epoch: 5 / 500, time cost: 95.79 sec, \n",
      "          Loss: [Train: 2.40546], [Test: 2.41091],\n",
      "          Effect: [ate-q], [train: 0.08733], [test: 0.08736]\n",
      "********************************************************************************\n",
      "epoch: 6 / 500, time cost: 91.25 sec, \n",
      "          Loss: [Train: 2.40090], [Test: 2.40844],\n",
      "          Effect: [ate-q], [train: 0.08172], [test: 0.08178]\n",
      "********************************************************************************\n",
      "epoch: 7 / 500, time cost: 95.47 sec, \n",
      "          Loss: [Train: 2.39699], [Test: 2.40638],\n",
      "          Effect: [ate-q], [train: 0.08855], [test: 0.08860]\n",
      "********************************************************************************\n",
      "epoch: 8 / 500, time cost: 89.31 sec, \n",
      "          Loss: [Train: 2.39354], [Test: 2.40445],\n",
      "          Effect: [ate-q], [train: 0.08923], [test: 0.08931]\n",
      "********************************************************************************\n",
      "epoch: 9 / 500, time cost: 92.76 sec, \n",
      "          Loss: [Train: 2.39095], [Test: 2.40361],\n",
      "          Effect: [ate-q], [train: 0.07782], [test: 0.07790]\n",
      "********************************************************************************\n",
      "epoch: 10 / 500, time cost: 89.08 sec, \n",
      "          Loss: [Train: 2.38796], [Test: 2.40320],\n",
      "          Effect: [ate-q], [train: 0.06273], [test: 0.06282]\n",
      "********************************************************************************\n",
      "epoch: 11 / 500, time cost: 89.12 sec, \n",
      "          Loss: [Train: 2.38511], [Test: 2.40185],\n",
      "          Effect: [ate-q], [train: 0.08287], [test: 0.08296]\n",
      "********************************************************************************\n",
      "epoch: 12 / 500, time cost: 87.54 sec, \n",
      "          Loss: [Train: 2.38215], [Test: 2.40073],\n",
      "          Effect: [ate-q], [train: 0.07959], [test: 0.07970]\n",
      "********************************************************************************\n",
      "epoch: 13 / 500, time cost: 84.57 sec, \n",
      "          Loss: [Train: 2.37970], [Test: 2.40161],\n",
      "          Effect: [ate-q], [train: 0.07039], [test: 0.07051]\n",
      "********************************************************************************\n",
      "epoch: 14 / 500, time cost: 84.18 sec, \n",
      "          Loss: [Train: 2.37676], [Test: 2.40039],\n",
      "          Effect: [ate-q], [train: 0.06790], [test: 0.06805]\n",
      "********************************************************************************\n",
      "epoch: 15 / 500, time cost: 85.88 sec, \n",
      "          Loss: [Train: 2.37341], [Test: 2.40018],\n",
      "          Effect: [ate-q], [train: 0.07065], [test: 0.07085]\n",
      "********************************************************************************\n",
      "epoch: 16 / 500, time cost: 82.16 sec, \n",
      "          Loss: [Train: 2.36979], [Test: 2.39961],\n",
      "          Effect: [ate-q], [train: 0.06905], [test: 0.06926]\n",
      "********************************************************************************\n",
      "epoch: 17 / 500, time cost: 85.01 sec, \n",
      "          Loss: [Train: 2.36514], [Test: 2.40054],\n",
      "          Effect: [ate-q], [train: 0.06210], [test: 0.06238]\n",
      "********************************************************************************\n",
      "epoch: 18 / 500, time cost: 81.74 sec, \n",
      "          Loss: [Train: 2.36010], [Test: 2.40090],\n",
      "          Effect: [ate-q], [train: 0.07533], [test: 0.07565]\n",
      "********************************************************************************\n",
      "epoch: 19 / 500, time cost: 85.28 sec, \n",
      "          Loss: [Train: 2.35413], [Test: 2.40230],\n",
      "          Effect: [ate-q], [train: 0.07712], [test: 0.07749]\n",
      "********************************************************************************\n",
      "epoch: 20 / 500, time cost: 90.82 sec, \n",
      "          Loss: [Train: 2.34694], [Test: 2.40748],\n",
      "          Effect: [ate-q], [train: 0.03909], [test: 0.03945]\n",
      "********************************************************************************\n",
      "epoch: 21 / 500, time cost: 84.62 sec, \n",
      "          Loss: [Train: 2.33942], [Test: 2.40881],\n",
      "          Effect: [ate-q], [train: 0.05519], [test: 0.05558]\n",
      "********************************************************************************\n",
      "epoch: 22 / 500, time cost: 93.59 sec, \n",
      "          Loss: [Train: 2.33163], [Test: 2.41593],\n",
      "          Effect: [ate-q], [train: 0.03273], [test: 0.03311]\n",
      "********************************************************************************\n",
      "epoch: 23 / 500, time cost: 92.99 sec, \n",
      "          Loss: [Train: 2.32388], [Test: 2.41993],\n",
      "          Effect: [ate-q], [train: 0.04764], [test: 0.04809]\n",
      "********************************************************************************\n",
      "epoch: 24 / 500, time cost: 90.16 sec, \n",
      "          Loss: [Train: 2.31611], [Test: 2.42833],\n",
      "          Effect: [ate-q], [train: 0.02905], [test: 0.02938]\n",
      "********************************************************************************\n",
      "epoch: 25 / 500, time cost: 90.95 sec, \n",
      "          Loss: [Train: 2.30894], [Test: 2.43492],\n",
      "          Effect: [ate-q], [train: 0.03714], [test: 0.03756]\n",
      "********************************************************************************\n",
      "epoch: 26 / 500, time cost: 90.35 sec, \n",
      "          Loss: [Train: 2.30201], [Test: 2.44078],\n",
      "          Effect: [ate-q], [train: 0.12003], [test: 0.12071]\n",
      "********************************************************************************\n",
      "epoch: 27 / 500, time cost: 88.19 sec, \n",
      "          Loss: [Train: 2.29491], [Test: 2.44652],\n",
      "          Effect: [ate-q], [train: 0.05138], [test: 0.05187]\n",
      "********************************************************************************\n",
      "epoch: 28 / 500, time cost: 57.47 sec, \n",
      "          Loss: [Train: 2.28915], [Test: 2.45610],\n",
      "          Effect: [ate-q], [train: 0.04226], [test: 0.04272]\n",
      "********************************************************************************\n",
      "epoch: 29 / 500, time cost: 44.43 sec, \n",
      "          Loss: [Train: 2.28309], [Test: 2.46341],\n",
      "          Effect: [ate-q], [train: 0.02924], [test: 0.02958]\n",
      "********************************************************************************\n",
      "epoch: 30 / 500, time cost: 44.24 sec, \n",
      "          Loss: [Train: 2.27642], [Test: 2.47184],\n",
      "          Effect: [ate-q], [train: 0.12220], [test: 0.12296]\n",
      "********************************************************************************\n",
      "epoch: 31 / 500, time cost: 44.41 sec, \n",
      "          Loss: [Train: 2.27080], [Test: 2.47387],\n",
      "          Effect: [ate-q], [train: 0.03586], [test: 0.03637]\n",
      "********************************************************************************\n",
      "epoch: 32 / 500, time cost: 44.19 sec, \n",
      "          Loss: [Train: 2.26471], [Test: 2.48024],\n",
      "          Effect: [ate-q], [train: 0.04348], [test: 0.04396]\n",
      "********************************************************************************\n",
      "epoch: 33 / 500, time cost: 44.37 sec, \n",
      "          Loss: [Train: 2.25917], [Test: 2.48659],\n",
      "          Effect: [ate-q], [train: 0.06152], [test: 0.06212]\n",
      "********************************************************************************\n",
      "epoch: 34 / 500, time cost: 44.34 sec, \n",
      "          Loss: [Train: 2.25434], [Test: 2.49589],\n",
      "          Effect: [ate-q], [train: 0.07173], [test: 0.07244]\n",
      "********************************************************************************\n",
      "epoch: 35 / 500, time cost: 43.53 sec, \n",
      "          Loss: [Train: 2.24725], [Test: 2.51105],\n",
      "          Effect: [ate-q], [train: 0.01492], [test: 0.01536]\n",
      "********************************************************************************\n",
      "epoch: 36 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 2.24188], [Test: 2.52206],\n",
      "          Effect: [ate-q], [train: 0.11773], [test: 0.11862]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 / 500, time cost: 44.00 sec, \n",
      "          Loss: [Train: 2.23581], [Test: 2.51571],\n",
      "          Effect: [ate-q], [train: 0.06386], [test: 0.06459]\n",
      "********************************************************************************\n",
      "epoch: 38 / 500, time cost: 44.27 sec, \n",
      "          Loss: [Train: 2.22954], [Test: 2.52494],\n",
      "          Effect: [ate-q], [train: 0.04683], [test: 0.04744]\n",
      "********************************************************************************\n",
      "epoch: 39 / 500, time cost: 44.27 sec, \n",
      "          Loss: [Train: 2.22379], [Test: 2.53482],\n",
      "          Effect: [ate-q], [train: 0.06892], [test: 0.06969]\n",
      "********************************************************************************\n",
      "epoch: 40 / 500, time cost: 44.68 sec, \n",
      "          Loss: [Train: 2.21771], [Test: 2.54229],\n",
      "          Effect: [ate-q], [train: 0.03222], [test: 0.03276]\n",
      "********************************************************************************\n",
      "epoch: 41 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 2.21247], [Test: 2.55023],\n",
      "          Effect: [ate-q], [train: 0.04024], [test: 0.04089]\n",
      "********************************************************************************\n",
      "epoch: 42 / 500, time cost: 46.80 sec, \n",
      "          Loss: [Train: 2.20664], [Test: 2.56321],\n",
      "          Effect: [ate-q], [train: 0.01764], [test: 0.01795]\n",
      "********************************************************************************\n",
      "epoch: 43 / 500, time cost: 43.85 sec, \n",
      "          Loss: [Train: 2.20103], [Test: 2.56577],\n",
      "          Effect: [ate-q], [train: 0.04272], [test: 0.04318]\n",
      "********************************************************************************\n",
      "epoch: 44 / 500, time cost: 43.65 sec, \n",
      "          Loss: [Train: 2.19519], [Test: 2.57253],\n",
      "          Effect: [ate-q], [train: 0.05016], [test: 0.05063]\n",
      "********************************************************************************\n",
      "epoch: 45 / 500, time cost: 43.94 sec, \n",
      "          Loss: [Train: 2.19092], [Test: 2.59001],\n",
      "          Effect: [ate-q], [train: 0.13516], [test: 0.13601]\n",
      "********************************************************************************\n",
      "epoch: 46 / 500, time cost: 43.86 sec, \n",
      "          Loss: [Train: 2.18487], [Test: 2.59764],\n",
      "          Effect: [ate-q], [train: 0.12363], [test: 0.12434]\n",
      "********************************************************************************\n",
      "epoch: 47 / 500, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.18034], [Test: 2.59741],\n",
      "          Effect: [ate-q], [train: 0.07813], [test: 0.07873]\n",
      "********************************************************************************\n",
      "epoch: 48 / 500, time cost: 44.04 sec, \n",
      "          Loss: [Train: 2.17378], [Test: 2.60674],\n",
      "          Effect: [ate-q], [train: 0.05603], [test: 0.05639]\n",
      "********************************************************************************\n",
      "epoch: 49 / 500, time cost: 43.93 sec, \n",
      "          Loss: [Train: 2.16898], [Test: 2.62316],\n",
      "          Effect: [ate-q], [train: 0.04519], [test: 0.04544]\n",
      "********************************************************************************\n",
      "epoch: 50 / 500, time cost: 44.45 sec, \n",
      "          Loss: [Train: 2.16419], [Test: 2.63050],\n",
      "          Effect: [ate-q], [train: 0.12037], [test: 0.12095]\n",
      "********************************************************************************\n",
      "epoch: 51 / 500, time cost: 44.56 sec, \n",
      "          Loss: [Train: 2.16036], [Test: 2.63634],\n",
      "          Effect: [ate-q], [train: 0.10922], [test: 0.10983]\n",
      "********************************************************************************\n",
      "epoch: 52 / 500, time cost: 44.32 sec, \n",
      "          Loss: [Train: 2.15557], [Test: 2.64233],\n",
      "          Effect: [ate-q], [train: 0.06413], [test: 0.06449]\n",
      "********************************************************************************\n",
      "epoch: 53 / 500, time cost: 44.29 sec, \n",
      "          Loss: [Train: 2.15037], [Test: 2.65350],\n",
      "          Effect: [ate-q], [train: 0.05083], [test: 0.05107]\n",
      "********************************************************************************\n",
      "epoch: 54 / 500, time cost: 44.56 sec, \n",
      "          Loss: [Train: 2.14696], [Test: 2.66031],\n",
      "          Effect: [ate-q], [train: 0.03775], [test: 0.03789]\n",
      "********************************************************************************\n",
      "epoch: 55 / 500, time cost: 44.65 sec, \n",
      "          Loss: [Train: 2.14319], [Test: 2.68119],\n",
      "          Effect: [ate-q], [train: 0.03184], [test: 0.03200]\n",
      "********************************************************************************\n",
      "epoch: 56 / 500, time cost: 44.77 sec, \n",
      "          Loss: [Train: 2.13958], [Test: 2.68426],\n",
      "          Effect: [ate-q], [train: 0.13676], [test: 0.13738]\n",
      "********************************************************************************\n",
      "epoch: 57 / 500, time cost: 43.95 sec, \n",
      "          Loss: [Train: 2.13601], [Test: 2.68457],\n",
      "          Effect: [ate-q], [train: 0.09178], [test: 0.09214]\n",
      "********************************************************************************\n",
      "epoch: 58 / 500, time cost: 46.35 sec, \n",
      "          Loss: [Train: 2.13187], [Test: 2.70033],\n",
      "          Effect: [ate-q], [train: 0.13819], [test: 0.13892]\n",
      "********************************************************************************\n",
      "epoch: 59 / 500, time cost: 43.54 sec, \n",
      "          Loss: [Train: 2.12756], [Test: 2.70203],\n",
      "          Effect: [ate-q], [train: 0.06021], [test: 0.06029]\n",
      "********************************************************************************\n",
      "epoch: 60 / 500, time cost: 43.82 sec, \n",
      "          Loss: [Train: 2.12405], [Test: 2.71453],\n",
      "          Effect: [ate-q], [train: 0.10584], [test: 0.10614]\n",
      "********************************************************************************\n",
      "epoch: 61 / 500, time cost: 43.93 sec, \n",
      "          Loss: [Train: 2.12095], [Test: 2.73287],\n",
      "          Effect: [ate-q], [train: -0.03194], [test: -0.03212]\n",
      "********************************************************************************\n",
      "epoch: 62 / 500, time cost: 44.01 sec, \n",
      "          Loss: [Train: 2.11650], [Test: 2.72969],\n",
      "          Effect: [ate-q], [train: 0.08671], [test: 0.08694]\n",
      "********************************************************************************\n",
      "epoch: 63 / 500, time cost: 43.98 sec, \n",
      "          Loss: [Train: 2.11242], [Test: 2.74129],\n",
      "          Effect: [ate-q], [train: 0.07108], [test: 0.07149]\n",
      "********************************************************************************\n",
      "epoch: 64 / 500, time cost: 43.81 sec, \n",
      "          Loss: [Train: 2.10869], [Test: 2.75146],\n",
      "          Effect: [ate-q], [train: 0.01616], [test: 0.01616]\n",
      "********************************************************************************\n",
      "epoch: 65 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 2.10556], [Test: 2.75649],\n",
      "          Effect: [ate-q], [train: 0.05882], [test: 0.05905]\n",
      "********************************************************************************\n",
      "epoch: 66 / 500, time cost: 43.73 sec, \n",
      "          Loss: [Train: 2.10366], [Test: 2.76582],\n",
      "          Effect: [ate-q], [train: 0.01361], [test: 0.01345]\n",
      "********************************************************************************\n",
      "epoch: 67 / 500, time cost: 44.37 sec, \n",
      "          Loss: [Train: 2.10013], [Test: 2.78029],\n",
      "          Effect: [ate-q], [train: 0.08407], [test: 0.08432]\n",
      "********************************************************************************\n",
      "epoch: 68 / 500, time cost: 44.09 sec, \n",
      "          Loss: [Train: 2.09631], [Test: 2.78424],\n",
      "          Effect: [ate-q], [train: 0.03193], [test: 0.03207]\n",
      "********************************************************************************\n",
      "epoch: 69 / 500, time cost: 43.87 sec, \n",
      "          Loss: [Train: 2.09404], [Test: 2.80039],\n",
      "          Effect: [ate-q], [train: 0.17068], [test: 0.17126]\n",
      "********************************************************************************\n",
      "epoch: 70 / 500, time cost: 44.08 sec, \n",
      "          Loss: [Train: 2.08960], [Test: 2.80473],\n",
      "          Effect: [ate-q], [train: 0.13004], [test: 0.13029]\n",
      "********************************************************************************\n",
      "epoch: 71 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 2.08748], [Test: 2.81276],\n",
      "          Effect: [ate-q], [train: 0.11281], [test: 0.11332]\n",
      "********************************************************************************\n",
      "epoch: 72 / 500, time cost: 44.02 sec, \n",
      "          Loss: [Train: 2.08301], [Test: 2.83687],\n",
      "          Effect: [ate-q], [train: 0.15328], [test: 0.15371]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 73 / 500, time cost: 44.35 sec, \n",
      "          Loss: [Train: 2.08145], [Test: 2.83693],\n",
      "          Effect: [ate-q], [train: 0.06996], [test: 0.07025]\n",
      "********************************************************************************\n",
      "epoch: 74 / 500, time cost: 44.33 sec, \n",
      "          Loss: [Train: 2.07872], [Test: 2.84432],\n",
      "          Effect: [ate-q], [train: 0.07357], [test: 0.07364]\n",
      "********************************************************************************\n",
      "epoch: 75 / 500, time cost: 44.39 sec, \n",
      "          Loss: [Train: 2.07481], [Test: 2.85719],\n",
      "          Effect: [ate-q], [train: 0.11663], [test: 0.11687]\n",
      "********************************************************************************\n",
      "epoch: 76 / 500, time cost: 44.68 sec, \n",
      "          Loss: [Train: 2.07203], [Test: 2.86637],\n",
      "          Effect: [ate-q], [train: 0.07590], [test: 0.07602]\n",
      "********************************************************************************\n",
      "epoch: 77 / 500, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.07120], [Test: 2.87398],\n",
      "          Effect: [ate-q], [train: 0.06812], [test: 0.06832]\n",
      "********************************************************************************\n",
      "epoch: 78 / 500, time cost: 44.62 sec, \n",
      "          Loss: [Train: 2.06781], [Test: 2.88730],\n",
      "          Effect: [ate-q], [train: 0.05723], [test: 0.05745]\n",
      "********************************************************************************\n",
      "epoch: 79 / 500, time cost: 44.50 sec, \n",
      "          Loss: [Train: 2.06640], [Test: 2.88734],\n",
      "          Effect: [ate-q], [train: 0.12759], [test: 0.12807]\n",
      "********************************************************************************\n",
      "epoch: 80 / 500, time cost: 44.14 sec, \n",
      "          Loss: [Train: 2.06349], [Test: 2.90225],\n",
      "          Effect: [ate-q], [train: 0.09190], [test: 0.09206]\n",
      "********************************************************************************\n",
      "epoch: 81 / 500, time cost: 44.21 sec, \n",
      "          Loss: [Train: 2.06160], [Test: 2.90594],\n",
      "          Effect: [ate-q], [train: 0.09027], [test: 0.09048]\n",
      "********************************************************************************\n",
      "epoch: 82 / 500, time cost: 46.65 sec, \n",
      "          Loss: [Train: 2.05919], [Test: 2.91453],\n",
      "          Effect: [ate-q], [train: 0.11125], [test: 0.11146]\n",
      "********************************************************************************\n",
      "epoch: 83 / 500, time cost: 43.81 sec, \n",
      "          Loss: [Train: 2.05679], [Test: 2.92420],\n",
      "          Effect: [ate-q], [train: 0.12481], [test: 0.12473]\n",
      "********************************************************************************\n",
      "epoch: 84 / 500, time cost: 44.52 sec, \n",
      "          Loss: [Train: 2.05351], [Test: 2.92928],\n",
      "          Effect: [ate-q], [train: 0.04425], [test: 0.04423]\n",
      "********************************************************************************\n",
      "epoch: 85 / 500, time cost: 44.64 sec, \n",
      "          Loss: [Train: 2.05094], [Test: 2.94546],\n",
      "          Effect: [ate-q], [train: 0.08545], [test: 0.08542]\n",
      "********************************************************************************\n",
      "epoch: 86 / 500, time cost: 44.62 sec, \n",
      "          Loss: [Train: 2.04766], [Test: 2.95191],\n",
      "          Effect: [ate-q], [train: 0.03184], [test: 0.03166]\n",
      "********************************************************************************\n",
      "epoch: 87 / 500, time cost: 44.45 sec, \n",
      "          Loss: [Train: 2.04743], [Test: 2.95945],\n",
      "          Effect: [ate-q], [train: 0.02154], [test: 0.02136]\n",
      "********************************************************************************\n",
      "epoch: 88 / 500, time cost: 44.49 sec, \n",
      "          Loss: [Train: 2.04536], [Test: 2.98188],\n",
      "          Effect: [ate-q], [train: 0.15787], [test: 0.15819]\n",
      "********************************************************************************\n",
      "epoch: 89 / 500, time cost: 44.57 sec, \n",
      "          Loss: [Train: 2.04212], [Test: 2.99160],\n",
      "          Effect: [ate-q], [train: 0.08337], [test: 0.08341]\n",
      "********************************************************************************\n",
      "epoch: 90 / 500, time cost: 44.30 sec, \n",
      "          Loss: [Train: 2.04017], [Test: 2.98598],\n",
      "          Effect: [ate-q], [train: 0.08149], [test: 0.08162]\n",
      "********************************************************************************\n",
      "epoch: 91 / 500, time cost: 44.39 sec, \n",
      "          Loss: [Train: 2.03809], [Test: 2.99853],\n",
      "          Effect: [ate-q], [train: 0.04775], [test: 0.04762]\n",
      "********************************************************************************\n",
      "epoch: 92 / 500, time cost: 44.60 sec, \n",
      "          Loss: [Train: 2.03594], [Test: 3.01218],\n",
      "          Effect: [ate-q], [train: 0.12527], [test: 0.12548]\n",
      "********************************************************************************\n",
      "epoch: 93 / 500, time cost: 46.38 sec, \n",
      "          Loss: [Train: 2.03396], [Test: 3.02084],\n",
      "          Effect: [ate-q], [train: 0.04480], [test: 0.04460]\n",
      "********************************************************************************\n",
      "epoch: 94 / 500, time cost: 44.28 sec, \n",
      "          Loss: [Train: 2.03226], [Test: 3.03295],\n",
      "          Effect: [ate-q], [train: -0.00992], [test: -0.01048]\n",
      "********************************************************************************\n",
      "epoch: 95 / 500, time cost: 44.32 sec, \n",
      "          Loss: [Train: 2.02991], [Test: 3.03069],\n",
      "          Effect: [ate-q], [train: 0.04749], [test: 0.04710]\n",
      "********************************************************************************\n",
      "epoch: 96 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 2.03134], [Test: 3.04094],\n",
      "          Effect: [ate-q], [train: 0.01094], [test: 0.01067]\n",
      "********************************************************************************\n",
      "epoch: 97 / 500, time cost: 44.12 sec, \n",
      "          Loss: [Train: 2.02560], [Test: 3.04400],\n",
      "          Effect: [ate-q], [train: 0.01994], [test: 0.01961]\n",
      "********************************************************************************\n",
      "epoch: 98 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 2.02208], [Test: 3.04999],\n",
      "          Effect: [ate-q], [train: 0.06456], [test: 0.06461]\n",
      "********************************************************************************\n",
      "epoch: 99 / 500, time cost: 44.61 sec, \n",
      "          Loss: [Train: 2.02268], [Test: 3.05740],\n",
      "          Effect: [ate-q], [train: 0.04887], [test: 0.04871]\n",
      "********************************************************************************\n",
      "epoch: 100 / 500, time cost: 44.15 sec, \n",
      "          Loss: [Train: 2.02094], [Test: 3.12620],\n",
      "          Effect: [ate-q], [train: 0.14668], [test: 0.14646]\n",
      "********************************************************************************\n",
      "epoch: 101 / 500, time cost: 44.03 sec, \n",
      "          Loss: [Train: 2.02046], [Test: 3.09557],\n",
      "          Effect: [ate-q], [train: -0.05010], [test: -0.05060]\n",
      "********************************************************************************\n",
      "epoch: 102 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 2.01559], [Test: 3.09550],\n",
      "          Effect: [ate-q], [train: 0.00968], [test: 0.00907]\n",
      "********************************************************************************\n",
      "epoch: 103 / 500, time cost: 44.41 sec, \n",
      "          Loss: [Train: 2.01557], [Test: 3.11758],\n",
      "          Effect: [ate-q], [train: 0.02592], [test: 0.02557]\n",
      "********************************************************************************\n",
      "epoch: 104 / 500, time cost: 43.95 sec, \n",
      "          Loss: [Train: 2.01318], [Test: 3.11121],\n",
      "          Effect: [ate-q], [train: 0.03413], [test: 0.03373]\n",
      "********************************************************************************\n",
      "epoch: 105 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 2.01046], [Test: 3.12117],\n",
      "          Effect: [ate-q], [train: 0.01621], [test: 0.01555]\n",
      "********************************************************************************\n",
      "epoch: 106 / 500, time cost: 44.03 sec, \n",
      "          Loss: [Train: 2.01227], [Test: 3.12496],\n",
      "          Effect: [ate-q], [train: 0.05678], [test: 0.05619]\n",
      "********************************************************************************\n",
      "epoch: 107 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 2.00968], [Test: 3.17186],\n",
      "          Effect: [ate-q], [train: 0.02497], [test: 0.02418]\n",
      "********************************************************************************\n",
      "epoch: 108 / 500, time cost: 44.34 sec, \n",
      "          Loss: [Train: 2.00975], [Test: 3.15355],\n",
      "          Effect: [ate-q], [train: 0.15124], [test: 0.15093]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 109 / 500, time cost: 44.02 sec, \n",
      "          Loss: [Train: 2.00395], [Test: 3.15628],\n",
      "          Effect: [ate-q], [train: 0.12003], [test: 0.11979]\n",
      "********************************************************************************\n",
      "epoch: 110 / 500, time cost: 43.61 sec, \n",
      "          Loss: [Train: 2.00536], [Test: 3.17134],\n",
      "          Effect: [ate-q], [train: 0.12165], [test: 0.12135]\n",
      "********************************************************************************\n",
      "epoch: 111 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 2.00371], [Test: 3.17768],\n",
      "          Effect: [ate-q], [train: 0.12005], [test: 0.11939]\n",
      "********************************************************************************\n",
      "epoch: 112 / 500, time cost: 46.53 sec, \n",
      "          Loss: [Train: 2.00001], [Test: 3.19848],\n",
      "          Effect: [ate-q], [train: -0.02166], [test: -0.02210]\n",
      "********************************************************************************\n",
      "epoch: 113 / 500, time cost: 44.38 sec, \n",
      "          Loss: [Train: 1.99879], [Test: 3.18150],\n",
      "          Effect: [ate-q], [train: 0.09917], [test: 0.09865]\n",
      "********************************************************************************\n",
      "epoch: 114 / 500, time cost: 44.04 sec, \n",
      "          Loss: [Train: 1.99841], [Test: 3.18754],\n",
      "          Effect: [ate-q], [train: 0.08797], [test: 0.08726]\n",
      "********************************************************************************\n",
      "epoch: 115 / 500, time cost: 44.01 sec, \n",
      "          Loss: [Train: 1.99802], [Test: 3.19894],\n",
      "          Effect: [ate-q], [train: 0.05586], [test: 0.05551]\n",
      "********************************************************************************\n",
      "epoch: 116 / 500, time cost: 43.95 sec, \n",
      "          Loss: [Train: 1.99418], [Test: 3.20585],\n",
      "          Effect: [ate-q], [train: 0.08254], [test: 0.08180]\n",
      "********************************************************************************\n",
      "epoch: 117 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 1.99305], [Test: 3.21584],\n",
      "          Effect: [ate-q], [train: 0.03232], [test: 0.03164]\n",
      "********************************************************************************\n",
      "epoch: 118 / 500, time cost: 44.37 sec, \n",
      "          Loss: [Train: 1.99326], [Test: 3.22372],\n",
      "          Effect: [ate-q], [train: 0.13381], [test: 0.13320]\n",
      "********************************************************************************\n",
      "epoch: 119 / 500, time cost: 44.30 sec, \n",
      "          Loss: [Train: 1.99157], [Test: 3.23674],\n",
      "          Effect: [ate-q], [train: 0.11461], [test: 0.11374]\n",
      "********************************************************************************\n",
      "epoch: 120 / 500, time cost: 44.55 sec, \n",
      "          Loss: [Train: 1.98907], [Test: 3.25663],\n",
      "          Effect: [ate-q], [train: 0.14744], [test: 0.14675]\n",
      "********************************************************************************\n",
      "epoch: 121 / 500, time cost: 44.24 sec, \n",
      "          Loss: [Train: 1.98949], [Test: 3.24522],\n",
      "          Effect: [ate-q], [train: 0.07286], [test: 0.07186]\n",
      "********************************************************************************\n",
      "epoch: 122 / 500, time cost: 44.08 sec, \n",
      "          Loss: [Train: 1.98748], [Test: 3.26281],\n",
      "          Effect: [ate-q], [train: -0.00034], [test: -0.00135]\n",
      "********************************************************************************\n",
      "epoch: 123 / 500, time cost: 44.32 sec, \n",
      "          Loss: [Train: 1.98426], [Test: 3.29250],\n",
      "          Effect: [ate-q], [train: -0.09851], [test: -0.09940]\n",
      "********************************************************************************\n",
      "epoch: 124 / 500, time cost: 44.00 sec, \n",
      "          Loss: [Train: 1.98343], [Test: 3.27795],\n",
      "          Effect: [ate-q], [train: 0.13039], [test: 0.12959]\n",
      "********************************************************************************\n",
      "epoch: 125 / 500, time cost: 44.21 sec, \n",
      "          Loss: [Train: 1.98202], [Test: 3.27576],\n",
      "          Effect: [ate-q], [train: 0.07551], [test: 0.07474]\n",
      "********************************************************************************\n",
      "epoch: 126 / 500, time cost: 43.13 sec, \n",
      "          Loss: [Train: 1.98061], [Test: 3.29892],\n",
      "          Effect: [ate-q], [train: 0.12794], [test: 0.12725]\n",
      "********************************************************************************\n",
      "epoch: 127 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 1.98036], [Test: 3.30482],\n",
      "          Effect: [ate-q], [train: 0.01304], [test: 0.01218]\n",
      "********************************************************************************\n",
      "epoch: 128 / 500, time cost: 46.71 sec, \n",
      "          Loss: [Train: 1.97858], [Test: 3.30162],\n",
      "          Effect: [ate-q], [train: 0.09582], [test: 0.09492]\n",
      "********************************************************************************\n",
      "epoch: 129 / 500, time cost: 43.82 sec, \n",
      "          Loss: [Train: 1.97779], [Test: 3.32207],\n",
      "          Effect: [ate-q], [train: -0.01999], [test: -0.02123]\n",
      "********************************************************************************\n",
      "epoch: 130 / 500, time cost: 43.76 sec, \n",
      "          Loss: [Train: 1.97641], [Test: 3.31915],\n",
      "          Effect: [ate-q], [train: 0.06888], [test: 0.06786]\n",
      "********************************************************************************\n",
      "epoch: 131 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 1.97524], [Test: 3.34286],\n",
      "          Effect: [ate-q], [train: 0.04540], [test: 0.04459]\n",
      "********************************************************************************\n",
      "epoch: 132 / 500, time cost: 43.86 sec, \n",
      "          Loss: [Train: 1.97410], [Test: 3.32626],\n",
      "          Effect: [ate-q], [train: 0.02929], [test: 0.02832]\n",
      "********************************************************************************\n",
      "epoch: 133 / 500, time cost: 43.76 sec, \n",
      "          Loss: [Train: 1.97164], [Test: 3.34351],\n",
      "          Effect: [ate-q], [train: -0.04916], [test: -0.05001]\n",
      "********************************************************************************\n",
      "epoch: 134 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 1.97176], [Test: 3.35004],\n",
      "          Effect: [ate-q], [train: 0.03429], [test: 0.03331]\n",
      "********************************************************************************\n",
      "epoch: 135 / 500, time cost: 43.85 sec, \n",
      "          Loss: [Train: 1.96889], [Test: 3.33973],\n",
      "          Effect: [ate-q], [train: 0.06767], [test: 0.06634]\n",
      "********************************************************************************\n",
      "epoch: 136 / 500, time cost: 43.86 sec, \n",
      "          Loss: [Train: 1.96887], [Test: 3.36616],\n",
      "          Effect: [ate-q], [train: -0.01385], [test: -0.01519]\n",
      "********************************************************************************\n",
      "epoch: 137 / 500, time cost: 43.93 sec, \n",
      "          Loss: [Train: 1.96914], [Test: 3.37605],\n",
      "          Effect: [ate-q], [train: 0.14336], [test: 0.14207]\n",
      "********************************************************************************\n",
      "epoch: 138 / 500, time cost: 43.66 sec, \n",
      "          Loss: [Train: 1.96684], [Test: 3.36672],\n",
      "          Effect: [ate-q], [train: 0.11261], [test: 0.11138]\n",
      "********************************************************************************\n",
      "epoch: 139 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 1.96519], [Test: 3.40048],\n",
      "          Effect: [ate-q], [train: 0.15425], [test: 0.15306]\n",
      "********************************************************************************\n",
      "epoch: 140 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 1.96283], [Test: 3.39324],\n",
      "          Effect: [ate-q], [train: 0.08492], [test: 0.08372]\n",
      "********************************************************************************\n",
      "epoch: 141 / 500, time cost: 44.52 sec, \n",
      "          Loss: [Train: 1.96220], [Test: 3.39807],\n",
      "          Effect: [ate-q], [train: 0.10112], [test: 0.10005]\n",
      "********************************************************************************\n",
      "epoch: 142 / 500, time cost: 44.09 sec, \n",
      "          Loss: [Train: 1.96043], [Test: 3.40561],\n",
      "          Effect: [ate-q], [train: -0.00751], [test: -0.00864]\n",
      "********************************************************************************\n",
      "epoch: 143 / 500, time cost: 44.42 sec, \n",
      "          Loss: [Train: 1.96007], [Test: 3.41129],\n",
      "          Effect: [ate-q], [train: 0.05022], [test: 0.04883]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 144 / 500, time cost: 44.19 sec, \n",
      "          Loss: [Train: 1.96055], [Test: 3.42895],\n",
      "          Effect: [ate-q], [train: 0.08794], [test: 0.08668]\n",
      "********************************************************************************\n",
      "epoch: 145 / 500, time cost: 44.31 sec, \n",
      "          Loss: [Train: 1.95796], [Test: 3.44180],\n",
      "          Effect: [ate-q], [train: 0.01555], [test: 0.01417]\n",
      "********************************************************************************\n",
      "epoch: 146 / 500, time cost: 44.51 sec, \n",
      "          Loss: [Train: 1.95688], [Test: 3.44253],\n",
      "          Effect: [ate-q], [train: 0.04170], [test: 0.04050]\n",
      "********************************************************************************\n",
      "epoch: 147 / 500, time cost: 46.73 sec, \n",
      "          Loss: [Train: 1.95697], [Test: 3.44543],\n",
      "          Effect: [ate-q], [train: 0.08528], [test: 0.08395]\n",
      "********************************************************************************\n",
      "epoch: 148 / 500, time cost: 43.88 sec, \n",
      "          Loss: [Train: 1.95466], [Test: 3.45511],\n",
      "          Effect: [ate-q], [train: 0.11893], [test: 0.11777]\n",
      "********************************************************************************\n",
      "epoch: 149 / 500, time cost: 44.84 sec, \n",
      "          Loss: [Train: 1.95314], [Test: 3.48054],\n",
      "          Effect: [ate-q], [train: -0.08329], [test: -0.08445]\n",
      "********************************************************************************\n",
      "epoch: 150 / 500, time cost: 43.99 sec, \n",
      "          Loss: [Train: 1.95376], [Test: 3.46052],\n",
      "          Effect: [ate-q], [train: 0.05621], [test: 0.05512]\n",
      "********************************************************************************\n",
      "epoch: 151 / 500, time cost: 43.79 sec, \n",
      "          Loss: [Train: 1.95299], [Test: 3.46946],\n",
      "          Effect: [ate-q], [train: 0.07028], [test: 0.06918]\n",
      "********************************************************************************\n",
      "epoch: 152 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 1.95039], [Test: 3.46560],\n",
      "          Effect: [ate-q], [train: 0.08364], [test: 0.08222]\n",
      "********************************************************************************\n",
      "epoch: 153 / 500, time cost: 44.16 sec, \n",
      "          Loss: [Train: 1.95095], [Test: 3.49244],\n",
      "          Effect: [ate-q], [train: 0.06601], [test: 0.06479]\n",
      "********************************************************************************\n",
      "epoch: 154 / 500, time cost: 43.96 sec, \n",
      "          Loss: [Train: 1.94968], [Test: 3.48685],\n",
      "          Effect: [ate-q], [train: 0.05969], [test: 0.05846]\n",
      "********************************************************************************\n",
      "epoch: 155 / 500, time cost: 44.28 sec, \n",
      "          Loss: [Train: 1.94778], [Test: 3.51343],\n",
      "          Effect: [ate-q], [train: 0.12975], [test: 0.12826]\n",
      "********************************************************************************\n",
      "epoch: 156 / 500, time cost: 44.30 sec, \n",
      "          Loss: [Train: 1.94568], [Test: 3.51936],\n",
      "          Effect: [ate-q], [train: -0.04683], [test: -0.04806]\n",
      "********************************************************************************\n",
      "epoch: 157 / 500, time cost: 43.97 sec, \n",
      "          Loss: [Train: 1.94629], [Test: 3.51031],\n",
      "          Effect: [ate-q], [train: 0.06854], [test: 0.06725]\n",
      "********************************************************************************\n",
      "epoch: 158 / 500, time cost: 47.22 sec, \n",
      "          Loss: [Train: 1.94563], [Test: 3.52225],\n",
      "          Effect: [ate-q], [train: 0.14113], [test: 0.13962]\n",
      "********************************************************************************\n",
      "epoch: 159 / 500, time cost: 46.61 sec, \n",
      "          Loss: [Train: 1.94313], [Test: 3.53043],\n",
      "          Effect: [ate-q], [train: 0.10736], [test: 0.10619]\n",
      "********************************************************************************\n",
      "epoch: 160 / 500, time cost: 43.94 sec, \n",
      "          Loss: [Train: 1.94190], [Test: 3.55419],\n",
      "          Effect: [ate-q], [train: 0.01541], [test: 0.01402]\n",
      "********************************************************************************\n",
      "epoch: 161 / 500, time cost: 63.50 sec, \n",
      "          Loss: [Train: 1.94194], [Test: 3.57567],\n",
      "          Effect: [ate-q], [train: 0.02811], [test: 0.02673]\n",
      "********************************************************************************\n",
      "epoch: 162 / 500, time cost: 44.16 sec, \n",
      "          Loss: [Train: 1.94220], [Test: 3.56781],\n",
      "          Effect: [ate-q], [train: -0.06255], [test: -0.06395]\n",
      "********************************************************************************\n",
      "epoch: 163 / 500, time cost: 46.43 sec, \n",
      "          Loss: [Train: 1.94034], [Test: 3.55950],\n",
      "          Effect: [ate-q], [train: 0.09612], [test: 0.09453]\n",
      "********************************************************************************\n",
      "epoch: 164 / 500, time cost: 44.31 sec, \n",
      "          Loss: [Train: 1.94098], [Test: 3.61099],\n",
      "          Effect: [ate-q], [train: 0.05632], [test: 0.05484]\n",
      "********************************************************************************\n",
      "epoch: 165 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 1.93751], [Test: 3.57094],\n",
      "          Effect: [ate-q], [train: 0.00961], [test: 0.00836]\n",
      "********************************************************************************\n",
      "epoch: 166 / 500, time cost: 43.99 sec, \n",
      "          Loss: [Train: 1.93689], [Test: 3.57691],\n",
      "          Effect: [ate-q], [train: 0.04355], [test: 0.04224]\n",
      "********************************************************************************\n",
      "epoch: 167 / 500, time cost: 43.88 sec, \n",
      "          Loss: [Train: 1.93570], [Test: 3.59705],\n",
      "          Effect: [ate-q], [train: 0.13414], [test: 0.13265]\n",
      "********************************************************************************\n",
      "epoch: 168 / 500, time cost: 44.22 sec, \n",
      "          Loss: [Train: 1.93549], [Test: 3.59820],\n",
      "          Effect: [ate-q], [train: 0.09806], [test: 0.09687]\n",
      "********************************************************************************\n",
      "epoch: 169 / 500, time cost: 44.08 sec, \n",
      "          Loss: [Train: 1.93409], [Test: 3.63946],\n",
      "          Effect: [ate-q], [train: 0.11695], [test: 0.11551]\n",
      "********************************************************************************\n",
      "epoch: 170 / 500, time cost: 44.32 sec, \n",
      "          Loss: [Train: 1.93548], [Test: 3.62376],\n",
      "          Effect: [ate-q], [train: -0.01733], [test: -0.01877]\n",
      "********************************************************************************\n",
      "epoch: 171 / 500, time cost: 44.02 sec, \n",
      "          Loss: [Train: 1.93291], [Test: 3.62037],\n",
      "          Effect: [ate-q], [train: 0.01240], [test: 0.01092]\n",
      "********************************************************************************\n",
      "epoch: 172 / 500, time cost: 43.18 sec, \n",
      "          Loss: [Train: 1.93120], [Test: 3.64561],\n",
      "          Effect: [ate-q], [train: 0.06977], [test: 0.06836]\n",
      "********************************************************************************\n",
      "epoch: 173 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 1.93238], [Test: 3.63481],\n",
      "          Effect: [ate-q], [train: 0.00548], [test: 0.00390]\n",
      "********************************************************************************\n",
      "epoch: 174 / 500, time cost: 42.95 sec, \n",
      "          Loss: [Train: 1.93081], [Test: 3.64797],\n",
      "          Effect: [ate-q], [train: 0.06663], [test: 0.06520]\n",
      "********************************************************************************\n",
      "epoch: 175 / 500, time cost: 44.48 sec, \n",
      "          Loss: [Train: 1.93036], [Test: 3.64245],\n",
      "          Effect: [ate-q], [train: 0.02394], [test: 0.02222]\n",
      "********************************************************************************\n",
      "epoch: 176 / 500, time cost: 44.21 sec, \n",
      "          Loss: [Train: 1.92941], [Test: 3.66243],\n",
      "          Effect: [ate-q], [train: 0.05351], [test: 0.05212]\n",
      "********************************************************************************\n",
      "epoch: 177 / 500, time cost: 44.50 sec, \n",
      "          Loss: [Train: 1.92841], [Test: 3.66694],\n",
      "          Effect: [ate-q], [train: 0.11062], [test: 0.10864]\n",
      "********************************************************************************\n",
      "epoch: 178 / 500, time cost: 43.72 sec, \n",
      "          Loss: [Train: 1.92609], [Test: 3.66639],\n",
      "          Effect: [ate-q], [train: 0.04264], [test: 0.04084]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 179 / 500, time cost: 44.17 sec, \n",
      "          Loss: [Train: 1.92822], [Test: 3.66900],\n",
      "          Effect: [ate-q], [train: 0.05246], [test: 0.05118]\n",
      "********************************************************************************\n",
      "epoch: 180 / 500, time cost: 44.19 sec, \n",
      "          Loss: [Train: 1.92492], [Test: 3.66636],\n",
      "          Effect: [ate-q], [train: 0.05777], [test: 0.05610]\n",
      "********************************************************************************\n",
      "epoch: 181 / 500, time cost: 44.32 sec, \n",
      "          Loss: [Train: 1.92436], [Test: 3.68246],\n",
      "          Effect: [ate-q], [train: 0.02858], [test: 0.02689]\n",
      "********************************************************************************\n",
      "epoch: 182 / 500, time cost: 46.68 sec, \n",
      "          Loss: [Train: 1.92469], [Test: 3.70074],\n",
      "          Effect: [ate-q], [train: -0.00409], [test: -0.00570]\n",
      "********************************************************************************\n",
      "epoch: 183 / 500, time cost: 44.60 sec, \n",
      "          Loss: [Train: 1.92277], [Test: 3.70614],\n",
      "          Effect: [ate-q], [train: 0.03729], [test: 0.03559]\n",
      "********************************************************************************\n",
      "epoch: 184 / 500, time cost: 44.58 sec, \n",
      "          Loss: [Train: 1.92366], [Test: 3.70202],\n",
      "          Effect: [ate-q], [train: 0.05532], [test: 0.05375]\n",
      "********************************************************************************\n",
      "epoch: 185 / 500, time cost: 44.29 sec, \n",
      "          Loss: [Train: 1.91967], [Test: 3.72236],\n",
      "          Effect: [ate-q], [train: 0.09753], [test: 0.09581]\n",
      "********************************************************************************\n",
      "epoch: 186 / 500, time cost: 44.31 sec, \n",
      "          Loss: [Train: 1.92031], [Test: 3.72260],\n",
      "          Effect: [ate-q], [train: 0.10899], [test: 0.10750]\n",
      "********************************************************************************\n",
      "epoch: 187 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 1.92040], [Test: 3.73158],\n",
      "          Effect: [ate-q], [train: 0.10924], [test: 0.10736]\n",
      "********************************************************************************\n",
      "epoch: 188 / 500, time cost: 44.59 sec, \n",
      "          Loss: [Train: 1.92052], [Test: 3.74370],\n",
      "          Effect: [ate-q], [train: 0.13058], [test: 0.12883]\n",
      "********************************************************************************\n",
      "epoch: 189 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.91698], [Test: 3.74456],\n",
      "          Effect: [ate-q], [train: 0.01791], [test: 0.01653]\n",
      "********************************************************************************\n",
      "epoch: 190 / 500, time cost: 44.37 sec, \n",
      "          Loss: [Train: 1.91829], [Test: 3.75503],\n",
      "          Effect: [ate-q], [train: 0.03365], [test: 0.03199]\n",
      "********************************************************************************\n",
      "epoch: 191 / 500, time cost: 43.91 sec, \n",
      "          Loss: [Train: 1.91759], [Test: 3.75809],\n",
      "          Effect: [ate-q], [train: 0.11920], [test: 0.11722]\n",
      "********************************************************************************\n",
      "epoch: 192 / 500, time cost: 44.45 sec, \n",
      "          Loss: [Train: 1.91511], [Test: 3.77094],\n",
      "          Effect: [ate-q], [train: 0.12853], [test: 0.12695]\n",
      "********************************************************************************\n",
      "epoch: 193 / 500, time cost: 44.80 sec, \n",
      "          Loss: [Train: 1.91561], [Test: 3.77576],\n",
      "          Effect: [ate-q], [train: 0.06581], [test: 0.06411]\n",
      "********************************************************************************\n",
      "epoch: 194 / 500, time cost: 44.71 sec, \n",
      "          Loss: [Train: 1.91417], [Test: 3.78255],\n",
      "          Effect: [ate-q], [train: 0.12254], [test: 0.12085]\n",
      "********************************************************************************\n",
      "epoch: 195 / 500, time cost: 43.88 sec, \n",
      "          Loss: [Train: 1.91311], [Test: 3.80000],\n",
      "          Effect: [ate-q], [train: 0.12295], [test: 0.12106]\n",
      "********************************************************************************\n",
      "epoch: 196 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 1.91311], [Test: 3.80293],\n",
      "          Effect: [ate-q], [train: 0.05243], [test: 0.05050]\n",
      "********************************************************************************\n",
      "epoch: 197 / 500, time cost: 43.72 sec, \n",
      "          Loss: [Train: 1.91100], [Test: 3.79569],\n",
      "          Effect: [ate-q], [train: 0.02804], [test: 0.02680]\n",
      "********************************************************************************\n",
      "epoch: 198 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 1.91142], [Test: 3.80290],\n",
      "          Effect: [ate-q], [train: -0.00299], [test: -0.00460]\n",
      "********************************************************************************\n",
      "epoch: 199 / 500, time cost: 43.81 sec, \n",
      "          Loss: [Train: 1.91149], [Test: 3.80519],\n",
      "          Effect: [ate-q], [train: 0.06162], [test: 0.05973]\n",
      "********************************************************************************\n",
      "epoch: 200 / 500, time cost: 43.30 sec, \n",
      "          Loss: [Train: 1.90937], [Test: 3.82857],\n",
      "          Effect: [ate-q], [train: 0.14205], [test: 0.13997]\n",
      "********************************************************************************\n",
      "epoch: 201 / 500, time cost: 44.01 sec, \n",
      "          Loss: [Train: 1.90839], [Test: 3.81466],\n",
      "          Effect: [ate-q], [train: 0.05062], [test: 0.04864]\n",
      "********************************************************************************\n",
      "epoch: 202 / 500, time cost: 43.96 sec, \n",
      "          Loss: [Train: 1.90949], [Test: 3.84184],\n",
      "          Effect: [ate-q], [train: 0.03216], [test: 0.03063]\n",
      "********************************************************************************\n",
      "epoch: 203 / 500, time cost: 43.87 sec, \n",
      "          Loss: [Train: 1.90882], [Test: 3.83839],\n",
      "          Effect: [ate-q], [train: 0.00460], [test: 0.00331]\n",
      "********************************************************************************\n",
      "epoch: 204 / 500, time cost: 43.85 sec, \n",
      "          Loss: [Train: 1.90635], [Test: 3.88317],\n",
      "          Effect: [ate-q], [train: 0.22958], [test: 0.22764]\n",
      "********************************************************************************\n",
      "epoch: 205 / 500, time cost: 43.92 sec, \n",
      "          Loss: [Train: 1.90508], [Test: 3.86211],\n",
      "          Effect: [ate-q], [train: 0.05234], [test: 0.05050]\n",
      "********************************************************************************\n",
      "epoch: 206 / 500, time cost: 44.19 sec, \n",
      "          Loss: [Train: 1.90432], [Test: 3.86863],\n",
      "          Effect: [ate-q], [train: 0.07302], [test: 0.07127]\n",
      "********************************************************************************\n",
      "epoch: 207 / 500, time cost: 44.37 sec, \n",
      "          Loss: [Train: 1.90294], [Test: 3.86938],\n",
      "          Effect: [ate-q], [train: 0.08155], [test: 0.07953]\n",
      "********************************************************************************\n",
      "epoch: 208 / 500, time cost: 43.73 sec, \n",
      "          Loss: [Train: 1.90224], [Test: 3.87038],\n",
      "          Effect: [ate-q], [train: 0.03127], [test: 0.03000]\n",
      "********************************************************************************\n",
      "epoch: 209 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.90199], [Test: 3.86151],\n",
      "          Effect: [ate-q], [train: 0.08070], [test: 0.07884]\n",
      "********************************************************************************\n",
      "epoch: 210 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.90339], [Test: 3.89029],\n",
      "          Effect: [ate-q], [train: -0.03755], [test: -0.03907]\n",
      "********************************************************************************\n",
      "epoch: 211 / 500, time cost: 44.00 sec, \n",
      "          Loss: [Train: 1.90113], [Test: 3.88064],\n",
      "          Effect: [ate-q], [train: 0.03521], [test: 0.03333]\n",
      "********************************************************************************\n",
      "epoch: 212 / 500, time cost: 43.98 sec, \n",
      "          Loss: [Train: 1.90069], [Test: 3.89487],\n",
      "          Effect: [ate-q], [train: 0.03871], [test: 0.03691]\n",
      "********************************************************************************\n",
      "epoch: 213 / 500, time cost: 43.89 sec, \n",
      "          Loss: [Train: 1.90019], [Test: 3.91824],\n",
      "          Effect: [ate-q], [train: -0.06255], [test: -0.06418]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 214 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.89902], [Test: 3.90910],\n",
      "          Effect: [ate-q], [train: 0.02568], [test: 0.02398]\n",
      "********************************************************************************\n",
      "epoch: 215 / 500, time cost: 43.98 sec, \n",
      "          Loss: [Train: 1.90309], [Test: 3.89980],\n",
      "          Effect: [ate-q], [train: 0.05814], [test: 0.05641]\n",
      "********************************************************************************\n",
      "epoch: 216 / 500, time cost: 43.34 sec, \n",
      "          Loss: [Train: 1.89573], [Test: 3.91574],\n",
      "          Effect: [ate-q], [train: -0.00396], [test: -0.00569]\n",
      "********************************************************************************\n",
      "epoch: 217 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 1.89862], [Test: 3.95780],\n",
      "          Effect: [ate-q], [train: 0.04142], [test: 0.03990]\n",
      "********************************************************************************\n",
      "epoch: 218 / 500, time cost: 43.49 sec, \n",
      "          Loss: [Train: 1.89641], [Test: 3.94710],\n",
      "          Effect: [ate-q], [train: 0.14103], [test: 0.13903]\n",
      "********************************************************************************\n",
      "epoch: 219 / 500, time cost: 93.58 sec, \n",
      "          Loss: [Train: 1.89610], [Test: 3.92059],\n",
      "          Effect: [ate-q], [train: 0.02299], [test: 0.02097]\n",
      "********************************************************************************\n",
      "epoch: 220 / 500, time cost: 94.58 sec, \n",
      "          Loss: [Train: 1.89619], [Test: 3.93356],\n",
      "          Effect: [ate-q], [train: 0.10288], [test: 0.10078]\n",
      "********************************************************************************\n",
      "epoch: 221 / 500, time cost: 90.67 sec, \n",
      "          Loss: [Train: 1.89422], [Test: 3.96393],\n",
      "          Effect: [ate-q], [train: 0.03982], [test: 0.03802]\n",
      "********************************************************************************\n",
      "epoch: 222 / 500, time cost: 91.78 sec, \n",
      "          Loss: [Train: 1.89398], [Test: 3.94709],\n",
      "          Effect: [ate-q], [train: 0.10259], [test: 0.10058]\n",
      "********************************************************************************\n",
      "epoch: 223 / 500, time cost: 96.32 sec, \n",
      "          Loss: [Train: 1.89376], [Test: 3.95107],\n",
      "          Effect: [ate-q], [train: 0.08451], [test: 0.08264]\n",
      "********************************************************************************\n",
      "epoch: 224 / 500, time cost: 96.32 sec, \n",
      "          Loss: [Train: 1.89127], [Test: 3.95282],\n",
      "          Effect: [ate-q], [train: 0.00324], [test: 0.00126]\n",
      "********************************************************************************\n",
      "epoch: 225 / 500, time cost: 87.68 sec, \n",
      "          Loss: [Train: 1.89237], [Test: 3.99256],\n",
      "          Effect: [ate-q], [train: 0.08535], [test: 0.08334]\n",
      "********************************************************************************\n",
      "epoch: 226 / 500, time cost: 93.97 sec, \n",
      "          Loss: [Train: 1.89179], [Test: 3.96223],\n",
      "          Effect: [ate-q], [train: 0.03302], [test: 0.03143]\n",
      "********************************************************************************\n",
      "epoch: 227 / 500, time cost: 93.52 sec, \n",
      "          Loss: [Train: 1.89287], [Test: 3.99172],\n",
      "          Effect: [ate-q], [train: 0.07389], [test: 0.07233]\n",
      "********************************************************************************\n",
      "epoch: 228 / 500, time cost: 88.39 sec, \n",
      "          Loss: [Train: 1.88921], [Test: 3.99925],\n",
      "          Effect: [ate-q], [train: 0.09797], [test: 0.09643]\n",
      "********************************************************************************\n",
      "epoch: 229 / 500, time cost: 89.63 sec, \n",
      "          Loss: [Train: 1.88866], [Test: 3.99889],\n",
      "          Effect: [ate-q], [train: 0.13001], [test: 0.12778]\n",
      "********************************************************************************\n",
      "epoch: 230 / 500, time cost: 94.29 sec, \n",
      "          Loss: [Train: 1.88754], [Test: 4.01852],\n",
      "          Effect: [ate-q], [train: -0.06499], [test: -0.06705]\n",
      "********************************************************************************\n",
      "epoch: 231 / 500, time cost: 91.77 sec, \n",
      "          Loss: [Train: 1.88629], [Test: 4.00829],\n",
      "          Effect: [ate-q], [train: 0.07770], [test: 0.07550]\n",
      "********************************************************************************\n",
      "epoch: 232 / 500, time cost: 95.33 sec, \n",
      "          Loss: [Train: 1.88680], [Test: 4.06682],\n",
      "          Effect: [ate-q], [train: 0.15168], [test: 0.14948]\n",
      "********************************************************************************\n",
      "epoch: 233 / 500, time cost: 90.09 sec, \n",
      "          Loss: [Train: 1.88767], [Test: 4.02539],\n",
      "          Effect: [ate-q], [train: -0.03331], [test: -0.03508]\n",
      "********************************************************************************\n",
      "epoch: 234 / 500, time cost: 96.25 sec, \n",
      "          Loss: [Train: 1.88636], [Test: 4.01801],\n",
      "          Effect: [ate-q], [train: 0.00165], [test: 0.00006]\n",
      "********************************************************************************\n",
      "epoch: 235 / 500, time cost: 87.10 sec, \n",
      "          Loss: [Train: 1.88621], [Test: 4.01932],\n",
      "          Effect: [ate-q], [train: 0.09804], [test: 0.09574]\n",
      "********************************************************************************\n",
      "epoch: 236 / 500, time cost: 93.41 sec, \n",
      "          Loss: [Train: 1.88488], [Test: 4.01686],\n",
      "          Effect: [ate-q], [train: 0.05180], [test: 0.04997]\n",
      "********************************************************************************\n",
      "epoch: 237 / 500, time cost: 91.79 sec, \n",
      "          Loss: [Train: 1.88330], [Test: 4.02848],\n",
      "          Effect: [ate-q], [train: 0.05880], [test: 0.05676]\n",
      "********************************************************************************\n",
      "epoch: 238 / 500, time cost: 88.77 sec, \n",
      "          Loss: [Train: 1.88363], [Test: 4.02748],\n",
      "          Effect: [ate-q], [train: 0.02537], [test: 0.02336]\n",
      "********************************************************************************\n",
      "epoch: 239 / 500, time cost: 90.53 sec, \n",
      "          Loss: [Train: 1.88143], [Test: 4.05673],\n",
      "          Effect: [ate-q], [train: 0.12778], [test: 0.12551]\n",
      "********************************************************************************\n",
      "epoch: 240 / 500, time cost: 96.99 sec, \n",
      "          Loss: [Train: 1.88205], [Test: 4.10225],\n",
      "          Effect: [ate-q], [train: 0.09889], [test: 0.09703]\n",
      "********************************************************************************\n",
      "epoch: 241 / 500, time cost: 90.89 sec, \n",
      "          Loss: [Train: 1.87996], [Test: 4.06354],\n",
      "          Effect: [ate-q], [train: 0.03645], [test: 0.03454]\n",
      "********************************************************************************\n",
      "epoch: 242 / 500, time cost: 91.60 sec, \n",
      "          Loss: [Train: 1.88176], [Test: 4.08781],\n",
      "          Effect: [ate-q], [train: 0.10086], [test: 0.09906]\n",
      "********************************************************************************\n",
      "epoch: 243 / 500, time cost: 92.81 sec, \n",
      "          Loss: [Train: 1.88131], [Test: 4.05083],\n",
      "          Effect: [ate-q], [train: 0.09749], [test: 0.09559]\n",
      "********************************************************************************\n",
      "epoch: 244 / 500, time cost: 86.48 sec, \n",
      "          Loss: [Train: 1.87930], [Test: 4.09782],\n",
      "          Effect: [ate-q], [train: 0.07443], [test: 0.07226]\n",
      "********************************************************************************\n",
      "epoch: 245 / 500, time cost: 88.43 sec, \n",
      "          Loss: [Train: 1.87796], [Test: 4.08358],\n",
      "          Effect: [ate-q], [train: 0.11399], [test: 0.11171]\n",
      "********************************************************************************\n",
      "epoch: 246 / 500, time cost: 90.98 sec, \n",
      "          Loss: [Train: 1.87895], [Test: 4.10384],\n",
      "          Effect: [ate-q], [train: 0.16618], [test: 0.16376]\n",
      "********************************************************************************\n",
      "epoch: 247 / 500, time cost: 89.18 sec, \n",
      "          Loss: [Train: 1.87750], [Test: 4.11251],\n",
      "          Effect: [ate-q], [train: 0.02977], [test: 0.02788]\n",
      "********************************************************************************\n",
      "epoch: 248 / 500, time cost: 91.32 sec, \n",
      "          Loss: [Train: 1.87807], [Test: 4.11856],\n",
      "          Effect: [ate-q], [train: 0.06950], [test: 0.06779]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 249 / 500, time cost: 86.38 sec, \n",
      "          Loss: [Train: 1.87542], [Test: 4.11659],\n",
      "          Effect: [ate-q], [train: 0.07380], [test: 0.07172]\n",
      "********************************************************************************\n",
      "epoch: 250 / 500, time cost: 95.73 sec, \n",
      "          Loss: [Train: 1.87496], [Test: 4.13105],\n",
      "          Effect: [ate-q], [train: 0.05640], [test: 0.05445]\n",
      "********************************************************************************\n",
      "epoch: 251 / 500, time cost: 96.97 sec, \n",
      "          Loss: [Train: 1.87491], [Test: 4.13568],\n",
      "          Effect: [ate-q], [train: 0.04641], [test: 0.04443]\n",
      "********************************************************************************\n",
      "epoch: 252 / 500, time cost: 96.08 sec, \n",
      "          Loss: [Train: 1.87489], [Test: 4.12742],\n",
      "          Effect: [ate-q], [train: 0.04529], [test: 0.04320]\n",
      "********************************************************************************\n",
      "epoch: 253 / 500, time cost: 89.53 sec, \n",
      "          Loss: [Train: 1.87407], [Test: 4.15335],\n",
      "          Effect: [ate-q], [train: 0.05800], [test: 0.05623]\n",
      "********************************************************************************\n",
      "epoch: 254 / 500, time cost: 89.82 sec, \n",
      "          Loss: [Train: 1.87443], [Test: 4.14120],\n",
      "          Effect: [ate-q], [train: 0.05235], [test: 0.05019]\n",
      "********************************************************************************\n",
      "epoch: 255 / 500, time cost: 90.55 sec, \n",
      "          Loss: [Train: 1.87188], [Test: 4.14348],\n",
      "          Effect: [ate-q], [train: 0.06546], [test: 0.06343]\n",
      "********************************************************************************\n",
      "epoch: 256 / 500, time cost: 88.40 sec, \n",
      "          Loss: [Train: 1.87091], [Test: 4.18366],\n",
      "          Effect: [ate-q], [train: 0.16206], [test: 0.15968]\n",
      "********************************************************************************\n",
      "epoch: 257 / 500, time cost: 88.69 sec, \n",
      "          Loss: [Train: 1.87122], [Test: 4.15044],\n",
      "          Effect: [ate-q], [train: 0.01917], [test: 0.01744]\n",
      "********************************************************************************\n",
      "epoch: 258 / 500, time cost: 95.90 sec, \n",
      "          Loss: [Train: 1.87074], [Test: 4.19569],\n",
      "          Effect: [ate-q], [train: 0.10815], [test: 0.10617]\n",
      "********************************************************************************\n",
      "epoch: 259 / 500, time cost: 86.01 sec, \n",
      "          Loss: [Train: 1.87025], [Test: 4.16809],\n",
      "          Effect: [ate-q], [train: -0.03777], [test: -0.03943]\n",
      "********************************************************************************\n",
      "epoch: 260 / 500, time cost: 94.84 sec, \n",
      "          Loss: [Train: 1.87047], [Test: 4.15782],\n",
      "          Effect: [ate-q], [train: 0.00814], [test: 0.00656]\n",
      "********************************************************************************\n",
      "epoch: 261 / 500, time cost: 89.61 sec, \n",
      "          Loss: [Train: 1.86874], [Test: 4.18553],\n",
      "          Effect: [ate-q], [train: -0.03148], [test: -0.03319]\n",
      "********************************************************************************\n",
      "epoch: 262 / 500, time cost: 88.56 sec, \n",
      "          Loss: [Train: 1.86788], [Test: 4.18312],\n",
      "          Effect: [ate-q], [train: 0.12626], [test: 0.12400]\n",
      "********************************************************************************\n",
      "epoch: 263 / 500, time cost: 85.47 sec, \n",
      "          Loss: [Train: 1.86972], [Test: 4.23103],\n",
      "          Effect: [ate-q], [train: 0.11229], [test: 0.11017]\n",
      "********************************************************************************\n",
      "epoch: 264 / 500, time cost: 94.56 sec, \n",
      "          Loss: [Train: 1.86757], [Test: 4.21984],\n",
      "          Effect: [ate-q], [train: 0.05317], [test: 0.05135]\n",
      "********************************************************************************\n",
      "epoch: 265 / 500, time cost: 88.59 sec, \n",
      "          Loss: [Train: 1.86523], [Test: 4.19995],\n",
      "          Effect: [ate-q], [train: 0.06787], [test: 0.06563]\n",
      "********************************************************************************\n",
      "epoch: 266 / 500, time cost: 91.13 sec, \n",
      "          Loss: [Train: 1.86673], [Test: 4.24113],\n",
      "          Effect: [ate-q], [train: 0.13053], [test: 0.12830]\n",
      "********************************************************************************\n",
      "epoch: 267 / 500, time cost: 86.05 sec, \n",
      "          Loss: [Train: 1.86789], [Test: 4.21693],\n",
      "          Effect: [ate-q], [train: 0.06859], [test: 0.06673]\n",
      "********************************************************************************\n",
      "epoch: 268 / 500, time cost: 94.65 sec, \n",
      "          Loss: [Train: 1.86436], [Test: 4.24752],\n",
      "          Effect: [ate-q], [train: 0.06438], [test: 0.06250]\n",
      "********************************************************************************\n",
      "epoch: 269 / 500, time cost: 44.64 sec, \n",
      "          Loss: [Train: 1.86616], [Test: 4.20647],\n",
      "          Effect: [ate-q], [train: 0.03785], [test: 0.03604]\n",
      "********************************************************************************\n",
      "epoch: 270 / 500, time cost: 43.67 sec, \n",
      "          Loss: [Train: 1.86344], [Test: 4.23652],\n",
      "          Effect: [ate-q], [train: 0.11498], [test: 0.11312]\n",
      "********************************************************************************\n",
      "epoch: 271 / 500, time cost: 43.50 sec, \n",
      "          Loss: [Train: 1.86439], [Test: 4.22205],\n",
      "          Effect: [ate-q], [train: 0.03841], [test: 0.03680]\n",
      "********************************************************************************\n",
      "epoch: 272 / 500, time cost: 43.69 sec, \n",
      "          Loss: [Train: 1.86236], [Test: 4.25606],\n",
      "          Effect: [ate-q], [train: 0.08069], [test: 0.07898]\n",
      "********************************************************************************\n",
      "epoch: 273 / 500, time cost: 43.79 sec, \n",
      "          Loss: [Train: 1.86282], [Test: 4.28321],\n",
      "          Effect: [ate-q], [train: 0.00998], [test: 0.00839]\n",
      "********************************************************************************\n",
      "epoch: 274 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 1.86135], [Test: 4.23798],\n",
      "          Effect: [ate-q], [train: 0.04665], [test: 0.04484]\n",
      "********************************************************************************\n",
      "epoch: 275 / 500, time cost: 44.14 sec, \n",
      "          Loss: [Train: 1.86065], [Test: 4.25472],\n",
      "          Effect: [ate-q], [train: 0.04561], [test: 0.04356]\n",
      "********************************************************************************\n",
      "epoch: 276 / 500, time cost: 44.21 sec, \n",
      "          Loss: [Train: 1.86049], [Test: 4.26950],\n",
      "          Effect: [ate-q], [train: 0.15511], [test: 0.15269]\n",
      "********************************************************************************\n",
      "epoch: 277 / 500, time cost: 44.19 sec, \n",
      "          Loss: [Train: 1.86073], [Test: 4.28315],\n",
      "          Effect: [ate-q], [train: 0.01189], [test: 0.01011]\n",
      "********************************************************************************\n",
      "epoch: 278 / 500, time cost: 44.04 sec, \n",
      "          Loss: [Train: 1.86050], [Test: 4.29938],\n",
      "          Effect: [ate-q], [train: 0.06783], [test: 0.06603]\n",
      "********************************************************************************\n",
      "epoch: 279 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 1.85927], [Test: 4.31695],\n",
      "          Effect: [ate-q], [train: -0.01083], [test: -0.01236]\n",
      "********************************************************************************\n",
      "epoch: 280 / 500, time cost: 44.17 sec, \n",
      "          Loss: [Train: 1.85709], [Test: 4.29543],\n",
      "          Effect: [ate-q], [train: 0.06858], [test: 0.06665]\n",
      "********************************************************************************\n",
      "epoch: 281 / 500, time cost: 43.68 sec, \n",
      "          Loss: [Train: 1.85760], [Test: 4.30213],\n",
      "          Effect: [ate-q], [train: 0.04215], [test: 0.04038]\n",
      "********************************************************************************\n",
      "epoch: 282 / 500, time cost: 43.42 sec, \n",
      "          Loss: [Train: 1.85726], [Test: 4.31050],\n",
      "          Effect: [ate-q], [train: 0.13050], [test: 0.12807]\n",
      "********************************************************************************\n",
      "epoch: 283 / 500, time cost: 43.86 sec, \n",
      "          Loss: [Train: 1.85566], [Test: 4.29725],\n",
      "          Effect: [ate-q], [train: 0.06901], [test: 0.06662]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 284 / 500, time cost: 43.89 sec, \n",
      "          Loss: [Train: 1.85524], [Test: 4.30867],\n",
      "          Effect: [ate-q], [train: 0.07271], [test: 0.07067]\n",
      "********************************************************************************\n",
      "epoch: 285 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 1.85404], [Test: 4.32592],\n",
      "          Effect: [ate-q], [train: 0.02001], [test: 0.01828]\n",
      "********************************************************************************\n",
      "epoch: 286 / 500, time cost: 43.97 sec, \n",
      "          Loss: [Train: 1.85492], [Test: 4.34514],\n",
      "          Effect: [ate-q], [train: 0.02628], [test: 0.02429]\n",
      "********************************************************************************\n",
      "epoch: 287 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 1.85308], [Test: 4.33900],\n",
      "          Effect: [ate-q], [train: 0.08202], [test: 0.08003]\n",
      "********************************************************************************\n",
      "epoch: 288 / 500, time cost: 43.89 sec, \n",
      "          Loss: [Train: 1.85431], [Test: 4.36746],\n",
      "          Effect: [ate-q], [train: 0.15438], [test: 0.15205]\n",
      "********************************************************************************\n",
      "epoch: 289 / 500, time cost: 43.93 sec, \n",
      "          Loss: [Train: 1.85439], [Test: 4.34543],\n",
      "          Effect: [ate-q], [train: 0.11843], [test: 0.11596]\n",
      "********************************************************************************\n",
      "epoch: 290 / 500, time cost: 43.74 sec, \n",
      "          Loss: [Train: 1.85379], [Test: 4.37692],\n",
      "          Effect: [ate-q], [train: 0.09687], [test: 0.09482]\n",
      "********************************************************************************\n",
      "epoch: 291 / 500, time cost: 43.76 sec, \n",
      "          Loss: [Train: 1.85108], [Test: 4.38510],\n",
      "          Effect: [ate-q], [train: 0.03173], [test: 0.03003]\n",
      "********************************************************************************\n",
      "epoch: 292 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 1.85087], [Test: 4.37169],\n",
      "          Effect: [ate-q], [train: -0.03397], [test: -0.03527]\n",
      "********************************************************************************\n",
      "epoch: 293 / 500, time cost: 44.00 sec, \n",
      "          Loss: [Train: 1.85055], [Test: 4.36886],\n",
      "          Effect: [ate-q], [train: -0.00352], [test: -0.00487]\n",
      "********************************************************************************\n",
      "epoch: 294 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.85010], [Test: 4.37940],\n",
      "          Effect: [ate-q], [train: 0.04535], [test: 0.04348]\n",
      "********************************************************************************\n",
      "epoch: 295 / 500, time cost: 44.48 sec, \n",
      "          Loss: [Train: 1.84850], [Test: 4.37552],\n",
      "          Effect: [ate-q], [train: 0.04585], [test: 0.04406]\n",
      "********************************************************************************\n",
      "epoch: 296 / 500, time cost: 44.41 sec, \n",
      "          Loss: [Train: 1.84880], [Test: 4.37648],\n",
      "          Effect: [ate-q], [train: 0.04242], [test: 0.04071]\n",
      "********************************************************************************\n",
      "epoch: 297 / 500, time cost: 43.60 sec, \n",
      "          Loss: [Train: 1.84813], [Test: 4.40330],\n",
      "          Effect: [ate-q], [train: 0.04448], [test: 0.04277]\n",
      "********************************************************************************\n",
      "epoch: 298 / 500, time cost: 44.14 sec, \n",
      "          Loss: [Train: 1.85023], [Test: 4.38313],\n",
      "          Effect: [ate-q], [train: 0.08255], [test: 0.08033]\n",
      "********************************************************************************\n",
      "epoch: 299 / 500, time cost: 44.48 sec, \n",
      "          Loss: [Train: 1.84682], [Test: 4.40615],\n",
      "          Effect: [ate-q], [train: 0.05698], [test: 0.05512]\n",
      "********************************************************************************\n",
      "epoch: 300 / 500, time cost: 43.16 sec, \n",
      "          Loss: [Train: 1.84790], [Test: 4.40863],\n",
      "          Effect: [ate-q], [train: 0.06160], [test: 0.05976]\n",
      "********************************************************************************\n",
      "epoch: 301 / 500, time cost: 43.73 sec, \n",
      "          Loss: [Train: 1.84614], [Test: 4.42427],\n",
      "          Effect: [ate-q], [train: 0.04956], [test: 0.04763]\n",
      "********************************************************************************\n",
      "epoch: 302 / 500, time cost: 43.62 sec, \n",
      "          Loss: [Train: 1.84591], [Test: 4.42206],\n",
      "          Effect: [ate-q], [train: 0.03916], [test: 0.03750]\n",
      "********************************************************************************\n",
      "epoch: 303 / 500, time cost: 45.97 sec, \n",
      "          Loss: [Train: 1.84510], [Test: 4.45057],\n",
      "          Effect: [ate-q], [train: -0.02239], [test: -0.02397]\n",
      "********************************************************************************\n",
      "epoch: 304 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.84376], [Test: 4.42643],\n",
      "          Effect: [ate-q], [train: 0.05336], [test: 0.05161]\n",
      "********************************************************************************\n",
      "epoch: 305 / 500, time cost: 43.41 sec, \n",
      "          Loss: [Train: 1.84494], [Test: 4.43796],\n",
      "          Effect: [ate-q], [train: 0.01818], [test: 0.01667]\n",
      "********************************************************************************\n",
      "epoch: 306 / 500, time cost: 43.23 sec, \n",
      "          Loss: [Train: 1.84478], [Test: 4.48664],\n",
      "          Effect: [ate-q], [train: 0.04580], [test: 0.04416]\n",
      "********************************************************************************\n",
      "epoch: 307 / 500, time cost: 43.74 sec, \n",
      "          Loss: [Train: 1.84386], [Test: 4.45474],\n",
      "          Effect: [ate-q], [train: 0.00768], [test: 0.00577]\n",
      "********************************************************************************\n",
      "epoch: 308 / 500, time cost: 43.53 sec, \n",
      "          Loss: [Train: 1.84191], [Test: 4.46717],\n",
      "          Effect: [ate-q], [train: 0.08074], [test: 0.07870]\n",
      "********************************************************************************\n",
      "epoch: 309 / 500, time cost: 43.19 sec, \n",
      "          Loss: [Train: 1.84294], [Test: 4.44025],\n",
      "          Effect: [ate-q], [train: 0.04644], [test: 0.04459]\n",
      "********************************************************************************\n",
      "epoch: 310 / 500, time cost: 43.48 sec, \n",
      "          Loss: [Train: 1.84117], [Test: 4.47080],\n",
      "          Effect: [ate-q], [train: 0.04756], [test: 0.04565]\n",
      "********************************************************************************\n",
      "epoch: 311 / 500, time cost: 43.50 sec, \n",
      "          Loss: [Train: 1.84085], [Test: 4.46527],\n",
      "          Effect: [ate-q], [train: -0.02261], [test: -0.02440]\n",
      "********************************************************************************\n",
      "epoch: 312 / 500, time cost: 43.52 sec, \n",
      "          Loss: [Train: 1.84067], [Test: 4.47648],\n",
      "          Effect: [ate-q], [train: 0.07033], [test: 0.06840]\n",
      "********************************************************************************\n",
      "epoch: 313 / 500, time cost: 43.71 sec, \n",
      "          Loss: [Train: 1.83999], [Test: 4.51391],\n",
      "          Effect: [ate-q], [train: 0.17964], [test: 0.17729]\n",
      "********************************************************************************\n",
      "epoch: 314 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 1.84003], [Test: 4.47800],\n",
      "          Effect: [ate-q], [train: 0.08916], [test: 0.08742]\n",
      "********************************************************************************\n",
      "epoch: 315 / 500, time cost: 44.15 sec, \n",
      "          Loss: [Train: 1.83897], [Test: 4.50051],\n",
      "          Effect: [ate-q], [train: 0.07162], [test: 0.06953]\n",
      "********************************************************************************\n",
      "epoch: 316 / 500, time cost: 44.03 sec, \n",
      "          Loss: [Train: 1.83845], [Test: 4.49015],\n",
      "          Effect: [ate-q], [train: 0.10934], [test: 0.10717]\n",
      "********************************************************************************\n",
      "epoch: 317 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 1.83828], [Test: 4.52019],\n",
      "          Effect: [ate-q], [train: 0.04306], [test: 0.04144]\n",
      "********************************************************************************\n",
      "epoch: 318 / 500, time cost: 43.83 sec, \n",
      "          Loss: [Train: 1.83660], [Test: 4.50805],\n",
      "          Effect: [ate-q], [train: 0.10759], [test: 0.10516]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 319 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 1.83615], [Test: 4.54624],\n",
      "          Effect: [ate-q], [train: 0.05917], [test: 0.05733]\n",
      "********************************************************************************\n",
      "epoch: 320 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 1.83729], [Test: 4.50086],\n",
      "          Effect: [ate-q], [train: 0.04689], [test: 0.04498]\n",
      "********************************************************************************\n",
      "epoch: 321 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.83589], [Test: 4.52485],\n",
      "          Effect: [ate-q], [train: 0.02397], [test: 0.02203]\n",
      "********************************************************************************\n",
      "epoch: 322 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 1.83627], [Test: 4.55035],\n",
      "          Effect: [ate-q], [train: 0.05057], [test: 0.04868]\n",
      "********************************************************************************\n",
      "epoch: 323 / 500, time cost: 43.98 sec, \n",
      "          Loss: [Train: 1.83427], [Test: 4.55008],\n",
      "          Effect: [ate-q], [train: 0.04448], [test: 0.04252]\n",
      "********************************************************************************\n",
      "epoch: 324 / 500, time cost: 43.59 sec, \n",
      "          Loss: [Train: 1.83496], [Test: 4.55231],\n",
      "          Effect: [ate-q], [train: 0.07816], [test: 0.07608]\n",
      "********************************************************************************\n",
      "epoch: 325 / 500, time cost: 43.55 sec, \n",
      "          Loss: [Train: 1.83359], [Test: 4.55310],\n",
      "          Effect: [ate-q], [train: 0.08782], [test: 0.08546]\n",
      "********************************************************************************\n",
      "epoch: 326 / 500, time cost: 43.45 sec, \n",
      "          Loss: [Train: 1.83401], [Test: 4.54470],\n",
      "          Effect: [ate-q], [train: 0.06842], [test: 0.06625]\n",
      "********************************************************************************\n",
      "epoch: 327 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 1.83361], [Test: 4.56242],\n",
      "          Effect: [ate-q], [train: 0.07762], [test: 0.07551]\n",
      "********************************************************************************\n",
      "epoch: 328 / 500, time cost: 43.97 sec, \n",
      "          Loss: [Train: 1.83174], [Test: 4.55173],\n",
      "          Effect: [ate-q], [train: 0.06500], [test: 0.06291]\n",
      "********************************************************************************\n",
      "epoch: 329 / 500, time cost: 43.83 sec, \n",
      "          Loss: [Train: 1.83287], [Test: 4.57474],\n",
      "          Effect: [ate-q], [train: 0.02418], [test: 0.02257]\n",
      "********************************************************************************\n",
      "epoch: 330 / 500, time cost: 44.08 sec, \n",
      "          Loss: [Train: 1.83195], [Test: 4.58209],\n",
      "          Effect: [ate-q], [train: 0.00978], [test: 0.00804]\n",
      "********************************************************************************\n",
      "epoch: 331 / 500, time cost: 44.01 sec, \n",
      "          Loss: [Train: 1.83007], [Test: 4.59415],\n",
      "          Effect: [ate-q], [train: 0.09354], [test: 0.09126]\n",
      "********************************************************************************\n",
      "epoch: 332 / 500, time cost: 43.88 sec, \n",
      "          Loss: [Train: 1.83011], [Test: 4.61498],\n",
      "          Effect: [ate-q], [train: 0.06268], [test: 0.06050]\n",
      "********************************************************************************\n",
      "epoch: 333 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 1.83032], [Test: 4.58464],\n",
      "          Effect: [ate-q], [train: -0.01078], [test: -0.01232]\n",
      "********************************************************************************\n",
      "epoch: 334 / 500, time cost: 43.69 sec, \n",
      "          Loss: [Train: 1.82951], [Test: 4.59094],\n",
      "          Effect: [ate-q], [train: 0.04792], [test: 0.04584]\n",
      "********************************************************************************\n",
      "epoch: 335 / 500, time cost: 43.84 sec, \n",
      "          Loss: [Train: 1.83031], [Test: 4.62852],\n",
      "          Effect: [ate-q], [train: 0.08762], [test: 0.08527]\n",
      "********************************************************************************\n",
      "epoch: 336 / 500, time cost: 43.89 sec, \n",
      "          Loss: [Train: 1.82647], [Test: 4.60538],\n",
      "          Effect: [ate-q], [train: 0.09199], [test: 0.08983]\n",
      "********************************************************************************\n",
      "epoch: 337 / 500, time cost: 43.78 sec, \n",
      "          Loss: [Train: 1.82964], [Test: 4.60240],\n",
      "          Effect: [ate-q], [train: 0.04359], [test: 0.04152]\n",
      "********************************************************************************\n",
      "epoch: 338 / 500, time cost: 46.09 sec, \n",
      "          Loss: [Train: 1.82650], [Test: 4.61781],\n",
      "          Effect: [ate-q], [train: 0.07345], [test: 0.07131]\n",
      "********************************************************************************\n",
      "epoch: 339 / 500, time cost: 44.23 sec, \n",
      "          Loss: [Train: 1.82686], [Test: 4.62697],\n",
      "          Effect: [ate-q], [train: 0.02145], [test: 0.02007]\n",
      "********************************************************************************\n",
      "epoch: 340 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 1.82629], [Test: 4.62161],\n",
      "          Effect: [ate-q], [train: 0.11461], [test: 0.11245]\n",
      "********************************************************************************\n",
      "epoch: 341 / 500, time cost: 43.54 sec, \n",
      "          Loss: [Train: 1.82616], [Test: 4.60956],\n",
      "          Effect: [ate-q], [train: 0.04057], [test: 0.03839]\n",
      "********************************************************************************\n",
      "epoch: 342 / 500, time cost: 43.86 sec, \n",
      "          Loss: [Train: 1.82443], [Test: 4.64715],\n",
      "          Effect: [ate-q], [train: 0.10926], [test: 0.10683]\n",
      "********************************************************************************\n",
      "epoch: 343 / 500, time cost: 43.72 sec, \n",
      "          Loss: [Train: 1.82494], [Test: 4.62767],\n",
      "          Effect: [ate-q], [train: 0.03541], [test: 0.03336]\n",
      "********************************************************************************\n",
      "epoch: 344 / 500, time cost: 43.66 sec, \n",
      "          Loss: [Train: 1.82389], [Test: 4.64675],\n",
      "          Effect: [ate-q], [train: 0.02345], [test: 0.02161]\n",
      "********************************************************************************\n",
      "epoch: 345 / 500, time cost: 43.75 sec, \n",
      "          Loss: [Train: 1.82207], [Test: 4.66682],\n",
      "          Effect: [ate-q], [train: 0.13730], [test: 0.13478]\n",
      "********************************************************************************\n",
      "epoch: 346 / 500, time cost: 43.82 sec, \n",
      "          Loss: [Train: 1.82389], [Test: 4.67906],\n",
      "          Effect: [ate-q], [train: 0.06935], [test: 0.06732]\n",
      "********************************************************************************\n",
      "epoch: 347 / 500, time cost: 44.73 sec, \n",
      "          Loss: [Train: 1.82173], [Test: 4.65701],\n",
      "          Effect: [ate-q], [train: 0.00464], [test: 0.00294]\n",
      "********************************************************************************\n",
      "epoch: 348 / 500, time cost: 45.15 sec, \n",
      "          Loss: [Train: 1.82224], [Test: 4.66746],\n",
      "          Effect: [ate-q], [train: 0.07962], [test: 0.07737]\n",
      "********************************************************************************\n",
      "epoch: 349 / 500, time cost: 43.57 sec, \n",
      "          Loss: [Train: 1.82338], [Test: 4.69051],\n",
      "          Effect: [ate-q], [train: 0.04285], [test: 0.04090]\n",
      "********************************************************************************\n",
      "epoch: 350 / 500, time cost: 43.43 sec, \n",
      "          Loss: [Train: 1.82069], [Test: 4.67617],\n",
      "          Effect: [ate-q], [train: 0.07357], [test: 0.07148]\n",
      "********************************************************************************\n",
      "epoch: 351 / 500, time cost: 43.82 sec, \n",
      "          Loss: [Train: 1.82172], [Test: 4.67661],\n",
      "          Effect: [ate-q], [train: 0.01070], [test: 0.00879]\n",
      "********************************************************************************\n",
      "epoch: 352 / 500, time cost: 44.03 sec, \n",
      "          Loss: [Train: 1.82070], [Test: 4.68835],\n",
      "          Effect: [ate-q], [train: -0.01251], [test: -0.01418]\n",
      "********************************************************************************\n",
      "epoch: 353 / 500, time cost: 43.88 sec, \n",
      "          Loss: [Train: 1.81945], [Test: 4.71751],\n",
      "          Effect: [ate-q], [train: 0.10633], [test: 0.10392]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 354 / 500, time cost: 43.95 sec, \n",
      "          Loss: [Train: 1.81993], [Test: 4.70704],\n",
      "          Effect: [ate-q], [train: 0.01116], [test: 0.00944]\n",
      "********************************************************************************\n",
      "epoch: 355 / 500, time cost: 44.22 sec, \n",
      "          Loss: [Train: 1.81872], [Test: 4.69454],\n",
      "          Effect: [ate-q], [train: 0.02080], [test: 0.01890]\n",
      "********************************************************************************\n",
      "epoch: 356 / 500, time cost: 44.12 sec, \n",
      "          Loss: [Train: 1.81912], [Test: 4.71147],\n",
      "          Effect: [ate-q], [train: 0.03991], [test: 0.03793]\n",
      "********************************************************************************\n",
      "epoch: 357 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 1.81854], [Test: 4.74068],\n",
      "          Effect: [ate-q], [train: 0.07655], [test: 0.07426]\n",
      "********************************************************************************\n",
      "epoch: 358 / 500, time cost: 43.92 sec, \n",
      "          Loss: [Train: 1.81709], [Test: 4.74574],\n",
      "          Effect: [ate-q], [train: 0.05190], [test: 0.04983]\n",
      "********************************************************************************\n",
      "epoch: 359 / 500, time cost: 43.87 sec, \n",
      "          Loss: [Train: 1.81715], [Test: 4.75047],\n",
      "          Effect: [ate-q], [train: 0.10949], [test: 0.10696]\n",
      "********************************************************************************\n",
      "epoch: 360 / 500, time cost: 44.21 sec, \n",
      "          Loss: [Train: 1.81765], [Test: 4.78748],\n",
      "          Effect: [ate-q], [train: 0.12701], [test: 0.12463]\n",
      "********************************************************************************\n",
      "epoch: 361 / 500, time cost: 46.58 sec, \n",
      "          Loss: [Train: 1.81640], [Test: 4.72961],\n",
      "          Effect: [ate-q], [train: 0.05276], [test: 0.05067]\n",
      "********************************************************************************\n",
      "epoch: 362 / 500, time cost: 47.63 sec, \n",
      "          Loss: [Train: 1.81666], [Test: 4.76067],\n",
      "          Effect: [ate-q], [train: 0.06380], [test: 0.06160]\n",
      "********************************************************************************\n",
      "epoch: 363 / 500, time cost: 43.96 sec, \n",
      "          Loss: [Train: 1.81575], [Test: 4.74468],\n",
      "          Effect: [ate-q], [train: 0.01602], [test: 0.01422]\n",
      "********************************************************************************\n",
      "epoch: 364 / 500, time cost: 67.64 sec, \n",
      "          Loss: [Train: 1.81533], [Test: 4.74642],\n",
      "          Effect: [ate-q], [train: 0.03834], [test: 0.03633]\n",
      "********************************************************************************\n",
      "epoch: 365 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 1.81485], [Test: 4.76683],\n",
      "          Effect: [ate-q], [train: 0.03265], [test: 0.03083]\n",
      "********************************************************************************\n",
      "epoch: 366 / 500, time cost: 43.67 sec, \n",
      "          Loss: [Train: 1.81468], [Test: 4.74494],\n",
      "          Effect: [ate-q], [train: -0.02511], [test: -0.02673]\n",
      "********************************************************************************\n",
      "epoch: 367 / 500, time cost: 50.55 sec, \n",
      "          Loss: [Train: 1.81283], [Test: 4.76044],\n",
      "          Effect: [ate-q], [train: 0.04265], [test: 0.04060]\n",
      "********************************************************************************\n",
      "epoch: 368 / 500, time cost: 71.84 sec, \n",
      "          Loss: [Train: 1.81324], [Test: 4.76999],\n",
      "          Effect: [ate-q], [train: 0.03855], [test: 0.03630]\n",
      "********************************************************************************\n",
      "epoch: 369 / 500, time cost: 91.91 sec, \n",
      "          Loss: [Train: 1.81277], [Test: 4.77352],\n",
      "          Effect: [ate-q], [train: 0.03195], [test: 0.02990]\n",
      "********************************************************************************\n",
      "epoch: 370 / 500, time cost: 93.45 sec, \n",
      "          Loss: [Train: 1.81174], [Test: 4.80581],\n",
      "          Effect: [ate-q], [train: 0.09981], [test: 0.09758]\n",
      "********************************************************************************\n",
      "epoch: 371 / 500, time cost: 89.15 sec, \n",
      "          Loss: [Train: 1.81201], [Test: 4.79273],\n",
      "          Effect: [ate-q], [train: 0.05497], [test: 0.05285]\n",
      "********************************************************************************\n",
      "epoch: 372 / 500, time cost: 96.81 sec, \n",
      "          Loss: [Train: 1.81090], [Test: 4.79749],\n",
      "          Effect: [ate-q], [train: 0.02369], [test: 0.02153]\n",
      "********************************************************************************\n",
      "epoch: 373 / 500, time cost: 93.47 sec, \n",
      "          Loss: [Train: 1.81146], [Test: 4.80779],\n",
      "          Effect: [ate-q], [train: 0.03043], [test: 0.02840]\n",
      "********************************************************************************\n",
      "epoch: 374 / 500, time cost: 89.02 sec, \n",
      "          Loss: [Train: 1.81095], [Test: 4.82229],\n",
      "          Effect: [ate-q], [train: 0.01572], [test: 0.01375]\n",
      "********************************************************************************\n",
      "epoch: 375 / 500, time cost: 98.11 sec, \n",
      "          Loss: [Train: 1.81036], [Test: 4.81893],\n",
      "          Effect: [ate-q], [train: 0.01195], [test: 0.01004]\n",
      "********************************************************************************\n",
      "epoch: 376 / 500, time cost: 83.21 sec, \n",
      "          Loss: [Train: 1.80917], [Test: 4.85129],\n",
      "          Effect: [ate-q], [train: 0.09787], [test: 0.09546]\n",
      "********************************************************************************\n",
      "epoch: 377 / 500, time cost: 96.23 sec, \n",
      "          Loss: [Train: 1.80929], [Test: 4.84939],\n",
      "          Effect: [ate-q], [train: 0.11308], [test: 0.11031]\n",
      "********************************************************************************\n",
      "epoch: 378 / 500, time cost: 88.34 sec, \n",
      "          Loss: [Train: 1.80910], [Test: 4.83131],\n",
      "          Effect: [ate-q], [train: 0.08411], [test: 0.08170]\n",
      "********************************************************************************\n",
      "epoch: 379 / 500, time cost: 85.82 sec, \n",
      "          Loss: [Train: 1.80982], [Test: 4.82004],\n",
      "          Effect: [ate-q], [train: 0.04620], [test: 0.04396]\n",
      "********************************************************************************\n",
      "epoch: 380 / 500, time cost: 94.72 sec, \n",
      "          Loss: [Train: 1.80708], [Test: 4.82214],\n",
      "          Effect: [ate-q], [train: 0.07687], [test: 0.07437]\n",
      "********************************************************************************\n",
      "epoch: 381 / 500, time cost: 88.79 sec, \n",
      "          Loss: [Train: 1.80706], [Test: 4.85831],\n",
      "          Effect: [ate-q], [train: 0.08306], [test: 0.08082]\n",
      "********************************************************************************\n",
      "epoch: 382 / 500, time cost: 85.65 sec, \n",
      "          Loss: [Train: 1.80636], [Test: 4.84975],\n",
      "          Effect: [ate-q], [train: 0.05604], [test: 0.05395]\n",
      "********************************************************************************\n",
      "epoch: 383 / 500, time cost: 91.68 sec, \n",
      "          Loss: [Train: 1.80668], [Test: 4.85290],\n",
      "          Effect: [ate-q], [train: 0.06994], [test: 0.06775]\n",
      "********************************************************************************\n",
      "epoch: 384 / 500, time cost: 88.33 sec, \n",
      "          Loss: [Train: 1.80659], [Test: 4.87149],\n",
      "          Effect: [ate-q], [train: 0.09696], [test: 0.09449]\n",
      "********************************************************************************\n",
      "epoch: 385 / 500, time cost: 88.14 sec, \n",
      "          Loss: [Train: 1.80571], [Test: 4.85080],\n",
      "          Effect: [ate-q], [train: 0.06045], [test: 0.05831]\n",
      "********************************************************************************\n",
      "epoch: 386 / 500, time cost: 95.22 sec, \n",
      "          Loss: [Train: 1.80543], [Test: 4.86299],\n",
      "          Effect: [ate-q], [train: 0.07204], [test: 0.06971]\n",
      "********************************************************************************\n",
      "epoch: 387 / 500, time cost: 94.09 sec, \n",
      "          Loss: [Train: 1.80550], [Test: 4.92092],\n",
      "          Effect: [ate-q], [train: 0.09763], [test: 0.09531]\n",
      "********************************************************************************\n",
      "epoch: 388 / 500, time cost: 85.56 sec, \n",
      "          Loss: [Train: 1.80524], [Test: 4.86864],\n",
      "          Effect: [ate-q], [train: 0.04711], [test: 0.04508]\n",
      "********************************************************************************\n",
      "epoch: 389 / 500, time cost: 90.77 sec, \n",
      "          Loss: [Train: 1.80516], [Test: 4.89977],\n",
      "          Effect: [ate-q], [train: 0.07384], [test: 0.07141]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 390 / 500, time cost: 92.76 sec, \n",
      "          Loss: [Train: 1.80324], [Test: 4.87712],\n",
      "          Effect: [ate-q], [train: 0.06431], [test: 0.06212]\n",
      "********************************************************************************\n",
      "epoch: 391 / 500, time cost: 90.04 sec, \n",
      "          Loss: [Train: 1.80331], [Test: 4.89075],\n",
      "          Effect: [ate-q], [train: 0.05227], [test: 0.05017]\n",
      "********************************************************************************\n",
      "epoch: 392 / 500, time cost: 95.20 sec, \n",
      "          Loss: [Train: 1.80402], [Test: 4.88219],\n",
      "          Effect: [ate-q], [train: 0.07566], [test: 0.07326]\n",
      "********************************************************************************\n",
      "epoch: 393 / 500, time cost: 85.20 sec, \n",
      "          Loss: [Train: 1.80256], [Test: 4.89987],\n",
      "          Effect: [ate-q], [train: 0.06095], [test: 0.05863]\n",
      "********************************************************************************\n",
      "epoch: 394 / 500, time cost: 93.53 sec, \n",
      "          Loss: [Train: 1.80266], [Test: 4.90209],\n",
      "          Effect: [ate-q], [train: 0.02338], [test: 0.02142]\n",
      "********************************************************************************\n",
      "epoch: 395 / 500, time cost: 92.54 sec, \n",
      "          Loss: [Train: 1.80229], [Test: 4.93979],\n",
      "          Effect: [ate-q], [train: 0.12825], [test: 0.12555]\n",
      "********************************************************************************\n",
      "epoch: 396 / 500, time cost: 94.33 sec, \n",
      "          Loss: [Train: 1.80243], [Test: 4.92993],\n",
      "          Effect: [ate-q], [train: 0.09499], [test: 0.09239]\n",
      "********************************************************************************\n",
      "epoch: 397 / 500, time cost: 101.91 sec, \n",
      "          Loss: [Train: 1.80164], [Test: 4.91855],\n",
      "          Effect: [ate-q], [train: 0.06452], [test: 0.06237]\n",
      "********************************************************************************\n",
      "epoch: 398 / 500, time cost: 94.79 sec, \n",
      "          Loss: [Train: 1.79975], [Test: 4.92235],\n",
      "          Effect: [ate-q], [train: 0.04910], [test: 0.04725]\n",
      "********************************************************************************\n",
      "epoch: 399 / 500, time cost: 92.46 sec, \n",
      "          Loss: [Train: 1.80040], [Test: 4.93431],\n",
      "          Effect: [ate-q], [train: 0.10322], [test: 0.10054]\n",
      "********************************************************************************\n",
      "epoch: 400 / 500, time cost: 96.13 sec, \n",
      "          Loss: [Train: 1.80011], [Test: 4.95391],\n",
      "          Effect: [ate-q], [train: -0.00305], [test: -0.00490]\n",
      "********************************************************************************\n",
      "epoch: 401 / 500, time cost: 88.26 sec, \n",
      "          Loss: [Train: 1.79975], [Test: 4.93893],\n",
      "          Effect: [ate-q], [train: 0.04798], [test: 0.04586]\n",
      "********************************************************************************\n",
      "epoch: 402 / 500, time cost: 92.15 sec, \n",
      "          Loss: [Train: 1.79942], [Test: 4.95385],\n",
      "          Effect: [ate-q], [train: 0.11512], [test: 0.11249]\n",
      "********************************************************************************\n",
      "epoch: 403 / 500, time cost: 91.07 sec, \n",
      "          Loss: [Train: 1.79909], [Test: 4.95164],\n",
      "          Effect: [ate-q], [train: 0.08396], [test: 0.08156]\n",
      "********************************************************************************\n",
      "epoch: 404 / 500, time cost: 90.63 sec, \n",
      "          Loss: [Train: 1.79809], [Test: 4.93627],\n",
      "          Effect: [ate-q], [train: -0.00136], [test: -0.00306]\n",
      "********************************************************************************\n",
      "epoch: 405 / 500, time cost: 92.36 sec, \n",
      "          Loss: [Train: 1.79728], [Test: 4.95366],\n",
      "          Effect: [ate-q], [train: 0.02925], [test: 0.02713]\n",
      "********************************************************************************\n",
      "epoch: 406 / 500, time cost: 94.51 sec, \n",
      "          Loss: [Train: 1.79814], [Test: 4.97851],\n",
      "          Effect: [ate-q], [train: 0.09876], [test: 0.09634]\n",
      "********************************************************************************\n",
      "epoch: 407 / 500, time cost: 90.99 sec, \n",
      "          Loss: [Train: 1.79788], [Test: 4.96882],\n",
      "          Effect: [ate-q], [train: 0.03699], [test: 0.03479]\n",
      "********************************************************************************\n",
      "epoch: 408 / 500, time cost: 89.70 sec, \n",
      "          Loss: [Train: 1.79639], [Test: 5.00128],\n",
      "          Effect: [ate-q], [train: 0.10918], [test: 0.10657]\n",
      "********************************************************************************\n",
      "epoch: 409 / 500, time cost: 97.02 sec, \n",
      "          Loss: [Train: 1.79664], [Test: 4.99034],\n",
      "          Effect: [ate-q], [train: 0.11299], [test: 0.11042]\n",
      "********************************************************************************\n",
      "epoch: 410 / 500, time cost: 85.25 sec, \n",
      "          Loss: [Train: 1.79616], [Test: 4.98383],\n",
      "          Effect: [ate-q], [train: 0.00463], [test: 0.00257]\n",
      "********************************************************************************\n",
      "epoch: 411 / 500, time cost: 89.71 sec, \n",
      "          Loss: [Train: 1.79572], [Test: 4.98023],\n",
      "          Effect: [ate-q], [train: 0.09712], [test: 0.09470]\n",
      "********************************************************************************\n",
      "epoch: 412 / 500, time cost: 95.37 sec, \n",
      "          Loss: [Train: 1.79533], [Test: 4.99625],\n",
      "          Effect: [ate-q], [train: 0.04412], [test: 0.04189]\n",
      "********************************************************************************\n",
      "epoch: 413 / 500, time cost: 88.49 sec, \n",
      "          Loss: [Train: 1.79441], [Test: 4.98287],\n",
      "          Effect: [ate-q], [train: 0.06861], [test: 0.06641]\n",
      "********************************************************************************\n",
      "epoch: 414 / 500, time cost: 91.38 sec, \n",
      "          Loss: [Train: 1.79398], [Test: 4.98616],\n",
      "          Effect: [ate-q], [train: 0.04689], [test: 0.04439]\n",
      "********************************************************************************\n",
      "epoch: 415 / 500, time cost: 97.84 sec, \n",
      "          Loss: [Train: 1.79495], [Test: 4.99866],\n",
      "          Effect: [ate-q], [train: 0.05266], [test: 0.05034]\n",
      "********************************************************************************\n",
      "epoch: 416 / 500, time cost: 87.62 sec, \n",
      "          Loss: [Train: 1.79327], [Test: 4.99705],\n",
      "          Effect: [ate-q], [train: 0.03139], [test: 0.02906]\n",
      "********************************************************************************\n",
      "epoch: 417 / 500, time cost: 84.90 sec, \n",
      "          Loss: [Train: 1.79289], [Test: 5.00041],\n",
      "          Effect: [ate-q], [train: 0.05735], [test: 0.05502]\n",
      "********************************************************************************\n",
      "epoch: 418 / 500, time cost: 49.26 sec, \n",
      "          Loss: [Train: 1.79261], [Test: 4.99761],\n",
      "          Effect: [ate-q], [train: 0.02958], [test: 0.02746]\n",
      "********************************************************************************\n",
      "epoch: 419 / 500, time cost: 44.18 sec, \n",
      "          Loss: [Train: 1.79238], [Test: 5.00939],\n",
      "          Effect: [ate-q], [train: 0.05893], [test: 0.05656]\n",
      "********************************************************************************\n",
      "epoch: 420 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.79271], [Test: 5.00761],\n",
      "          Effect: [ate-q], [train: 0.06761], [test: 0.06540]\n",
      "********************************************************************************\n",
      "epoch: 421 / 500, time cost: 44.10 sec, \n",
      "          Loss: [Train: 1.79214], [Test: 5.04072],\n",
      "          Effect: [ate-q], [train: 0.09344], [test: 0.09094]\n",
      "********************************************************************************\n",
      "epoch: 422 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 1.79158], [Test: 5.02218],\n",
      "          Effect: [ate-q], [train: 0.03773], [test: 0.03551]\n",
      "********************************************************************************\n",
      "epoch: 423 / 500, time cost: 44.30 sec, \n",
      "          Loss: [Train: 1.79204], [Test: 5.02430],\n",
      "          Effect: [ate-q], [train: 0.05738], [test: 0.05516]\n",
      "********************************************************************************\n",
      "epoch: 424 / 500, time cost: 44.15 sec, \n",
      "          Loss: [Train: 1.79050], [Test: 5.03089],\n",
      "          Effect: [ate-q], [train: 0.10053], [test: 0.09802]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 425 / 500, time cost: 43.46 sec, \n",
      "          Loss: [Train: 1.79088], [Test: 5.05128],\n",
      "          Effect: [ate-q], [train: 0.01097], [test: 0.00893]\n",
      "********************************************************************************\n",
      "epoch: 426 / 500, time cost: 43.29 sec, \n",
      "          Loss: [Train: 1.79056], [Test: 5.04362],\n",
      "          Effect: [ate-q], [train: 0.03676], [test: 0.03471]\n",
      "********************************************************************************\n",
      "epoch: 427 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 1.78960], [Test: 5.04373],\n",
      "          Effect: [ate-q], [train: 0.11779], [test: 0.11503]\n",
      "********************************************************************************\n",
      "epoch: 428 / 500, time cost: 43.50 sec, \n",
      "          Loss: [Train: 1.78954], [Test: 5.03613],\n",
      "          Effect: [ate-q], [train: 0.06291], [test: 0.06046]\n",
      "********************************************************************************\n",
      "epoch: 429 / 500, time cost: 42.76 sec, \n",
      "          Loss: [Train: 1.78910], [Test: 5.05468],\n",
      "          Effect: [ate-q], [train: 0.06671], [test: 0.06430]\n",
      "********************************************************************************\n",
      "epoch: 430 / 500, time cost: 43.80 sec, \n",
      "          Loss: [Train: 1.79009], [Test: 5.04810],\n",
      "          Effect: [ate-q], [train: 0.02483], [test: 0.02270]\n",
      "********************************************************************************\n",
      "epoch: 431 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 1.78853], [Test: 5.05280],\n",
      "          Effect: [ate-q], [train: 0.08872], [test: 0.08597]\n",
      "********************************************************************************\n",
      "epoch: 432 / 500, time cost: 45.30 sec, \n",
      "          Loss: [Train: 1.78831], [Test: 5.07206],\n",
      "          Effect: [ate-q], [train: 0.11999], [test: 0.11752]\n",
      "********************************************************************************\n",
      "epoch: 433 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.78719], [Test: 5.06151],\n",
      "          Effect: [ate-q], [train: 0.05008], [test: 0.04781]\n",
      "********************************************************************************\n",
      "epoch: 434 / 500, time cost: 43.59 sec, \n",
      "          Loss: [Train: 1.78721], [Test: 5.08215],\n",
      "          Effect: [ate-q], [train: 0.07686], [test: 0.07422]\n",
      "********************************************************************************\n",
      "epoch: 435 / 500, time cost: 44.07 sec, \n",
      "          Loss: [Train: 1.78742], [Test: 5.08459],\n",
      "          Effect: [ate-q], [train: 0.02872], [test: 0.02669]\n",
      "********************************************************************************\n",
      "epoch: 436 / 500, time cost: 43.42 sec, \n",
      "          Loss: [Train: 1.78717], [Test: 5.06994],\n",
      "          Effect: [ate-q], [train: 0.06219], [test: 0.05968]\n",
      "********************************************************************************\n",
      "epoch: 437 / 500, time cost: 43.33 sec, \n",
      "          Loss: [Train: 1.78622], [Test: 5.08208],\n",
      "          Effect: [ate-q], [train: 0.05410], [test: 0.05186]\n",
      "********************************************************************************\n",
      "epoch: 438 / 500, time cost: 43.61 sec, \n",
      "          Loss: [Train: 1.78643], [Test: 5.10301],\n",
      "          Effect: [ate-q], [train: 0.06393], [test: 0.06167]\n",
      "********************************************************************************\n",
      "epoch: 439 / 500, time cost: 43.62 sec, \n",
      "          Loss: [Train: 1.78559], [Test: 5.07631],\n",
      "          Effect: [ate-q], [train: 0.07862], [test: 0.07621]\n",
      "********************************************************************************\n",
      "epoch: 440 / 500, time cost: 43.68 sec, \n",
      "          Loss: [Train: 1.78529], [Test: 5.12040],\n",
      "          Effect: [ate-q], [train: 0.04848], [test: 0.04635]\n",
      "********************************************************************************\n",
      "epoch: 441 / 500, time cost: 43.67 sec, \n",
      "          Loss: [Train: 1.78544], [Test: 5.10901],\n",
      "          Effect: [ate-q], [train: 0.08115], [test: 0.07860]\n",
      "********************************************************************************\n",
      "epoch: 442 / 500, time cost: 43.61 sec, \n",
      "          Loss: [Train: 1.78492], [Test: 5.09198],\n",
      "          Effect: [ate-q], [train: 0.06662], [test: 0.06410]\n",
      "********************************************************************************\n",
      "epoch: 443 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 1.78473], [Test: 5.09834],\n",
      "          Effect: [ate-q], [train: 0.07796], [test: 0.07534]\n",
      "********************************************************************************\n",
      "epoch: 444 / 500, time cost: 43.84 sec, \n",
      "          Loss: [Train: 1.78531], [Test: 5.09712],\n",
      "          Effect: [ate-q], [train: 0.06199], [test: 0.05967]\n",
      "********************************************************************************\n",
      "epoch: 445 / 500, time cost: 44.18 sec, \n",
      "          Loss: [Train: 1.78513], [Test: 5.09384],\n",
      "          Effect: [ate-q], [train: 0.08823], [test: 0.08561]\n",
      "********************************************************************************\n",
      "epoch: 446 / 500, time cost: 44.32 sec, \n",
      "          Loss: [Train: 1.78340], [Test: 5.11560],\n",
      "          Effect: [ate-q], [train: 0.06148], [test: 0.05921]\n",
      "********************************************************************************\n",
      "epoch: 447 / 500, time cost: 44.23 sec, \n",
      "          Loss: [Train: 1.78414], [Test: 5.11222],\n",
      "          Effect: [ate-q], [train: 0.08680], [test: 0.08431]\n",
      "********************************************************************************\n",
      "epoch: 448 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.78311], [Test: 5.09876],\n",
      "          Effect: [ate-q], [train: 0.06586], [test: 0.06355]\n",
      "********************************************************************************\n",
      "epoch: 449 / 500, time cost: 44.25 sec, \n",
      "          Loss: [Train: 1.78264], [Test: 5.10389],\n",
      "          Effect: [ate-q], [train: 0.06268], [test: 0.06038]\n",
      "********************************************************************************\n",
      "epoch: 450 / 500, time cost: 43.96 sec, \n",
      "          Loss: [Train: 1.78292], [Test: 5.13442],\n",
      "          Effect: [ate-q], [train: 0.10463], [test: 0.10192]\n",
      "********************************************************************************\n",
      "epoch: 451 / 500, time cost: 43.90 sec, \n",
      "          Loss: [Train: 1.78190], [Test: 5.16686],\n",
      "          Effect: [ate-q], [train: 0.12141], [test: 0.11857]\n",
      "********************************************************************************\n",
      "epoch: 452 / 500, time cost: 44.04 sec, \n",
      "          Loss: [Train: 1.78183], [Test: 5.13658],\n",
      "          Effect: [ate-q], [train: 0.07165], [test: 0.06912]\n",
      "********************************************************************************\n",
      "epoch: 453 / 500, time cost: 43.78 sec, \n",
      "          Loss: [Train: 1.78209], [Test: 5.13722],\n",
      "          Effect: [ate-q], [train: 0.06959], [test: 0.06725]\n",
      "********************************************************************************\n",
      "epoch: 454 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.78170], [Test: 5.13664],\n",
      "          Effect: [ate-q], [train: 0.05349], [test: 0.05120]\n",
      "********************************************************************************\n",
      "epoch: 455 / 500, time cost: 44.25 sec, \n",
      "          Loss: [Train: 1.78083], [Test: 5.13210],\n",
      "          Effect: [ate-q], [train: 0.05979], [test: 0.05748]\n",
      "********************************************************************************\n",
      "epoch: 456 / 500, time cost: 44.22 sec, \n",
      "          Loss: [Train: 1.78143], [Test: 5.15272],\n",
      "          Effect: [ate-q], [train: 0.08063], [test: 0.07792]\n",
      "********************************************************************************\n",
      "epoch: 457 / 500, time cost: 44.29 sec, \n",
      "          Loss: [Train: 1.78050], [Test: 5.14180],\n",
      "          Effect: [ate-q], [train: 0.06924], [test: 0.06686]\n",
      "********************************************************************************\n",
      "epoch: 458 / 500, time cost: 43.60 sec, \n",
      "          Loss: [Train: 1.78032], [Test: 5.14183],\n",
      "          Effect: [ate-q], [train: 0.04716], [test: 0.04491]\n",
      "********************************************************************************\n",
      "epoch: 459 / 500, time cost: 44.06 sec, \n",
      "          Loss: [Train: 1.77978], [Test: 5.14039],\n",
      "          Effect: [ate-q], [train: 0.05383], [test: 0.05140]\n",
      "********************************************************************************\n",
      "epoch: 460 / 500, time cost: 43.92 sec, \n",
      "          Loss: [Train: 1.78000], [Test: 5.16235],\n",
      "          Effect: [ate-q], [train: 0.07193], [test: 0.06944]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 461 / 500, time cost: 43.92 sec, \n",
      "          Loss: [Train: 1.77976], [Test: 5.15272],\n",
      "          Effect: [ate-q], [train: 0.03579], [test: 0.03339]\n",
      "********************************************************************************\n",
      "epoch: 462 / 500, time cost: 45.51 sec, \n",
      "          Loss: [Train: 1.77901], [Test: 5.16640],\n",
      "          Effect: [ate-q], [train: 0.07145], [test: 0.06896]\n",
      "********************************************************************************\n",
      "epoch: 463 / 500, time cost: 44.00 sec, \n",
      "          Loss: [Train: 1.77840], [Test: 5.15285],\n",
      "          Effect: [ate-q], [train: 0.06098], [test: 0.05869]\n",
      "********************************************************************************\n",
      "epoch: 464 / 500, time cost: 43.75 sec, \n",
      "          Loss: [Train: 1.77866], [Test: 5.17088],\n",
      "          Effect: [ate-q], [train: 0.07835], [test: 0.07588]\n",
      "********************************************************************************\n",
      "epoch: 465 / 500, time cost: 44.31 sec, \n",
      "          Loss: [Train: 1.77866], [Test: 5.15819],\n",
      "          Effect: [ate-q], [train: 0.06577], [test: 0.06314]\n",
      "********************************************************************************\n",
      "epoch: 466 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 1.77838], [Test: 5.19561],\n",
      "          Effect: [ate-q], [train: 0.04599], [test: 0.04367]\n",
      "********************************************************************************\n",
      "epoch: 467 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 1.77832], [Test: 5.17221],\n",
      "          Effect: [ate-q], [train: 0.03905], [test: 0.03681]\n",
      "********************************************************************************\n",
      "epoch: 468 / 500, time cost: 44.39 sec, \n",
      "          Loss: [Train: 1.77820], [Test: 5.18110],\n",
      "          Effect: [ate-q], [train: 0.06064], [test: 0.05816]\n",
      "********************************************************************************\n",
      "epoch: 469 / 500, time cost: 44.36 sec, \n",
      "          Loss: [Train: 1.77787], [Test: 5.17765],\n",
      "          Effect: [ate-q], [train: 0.06840], [test: 0.06587]\n",
      "********************************************************************************\n",
      "epoch: 470 / 500, time cost: 44.28 sec, \n",
      "          Loss: [Train: 1.77723], [Test: 5.19158],\n",
      "          Effect: [ate-q], [train: 0.07739], [test: 0.07496]\n",
      "********************************************************************************\n",
      "epoch: 471 / 500, time cost: 44.03 sec, \n",
      "          Loss: [Train: 1.77742], [Test: 5.19020],\n",
      "          Effect: [ate-q], [train: 0.06713], [test: 0.06492]\n",
      "********************************************************************************\n",
      "epoch: 472 / 500, time cost: 43.64 sec, \n",
      "          Loss: [Train: 1.77730], [Test: 5.18061],\n",
      "          Effect: [ate-q], [train: 0.05733], [test: 0.05488]\n",
      "********************************************************************************\n",
      "epoch: 473 / 500, time cost: 43.62 sec, \n",
      "          Loss: [Train: 1.77652], [Test: 5.18485],\n",
      "          Effect: [ate-q], [train: 0.05089], [test: 0.04896]\n",
      "********************************************************************************\n",
      "epoch: 474 / 500, time cost: 43.88 sec, \n",
      "          Loss: [Train: 1.77634], [Test: 5.18834],\n",
      "          Effect: [ate-q], [train: 0.06803], [test: 0.06560]\n",
      "********************************************************************************\n",
      "epoch: 475 / 500, time cost: 43.46 sec, \n",
      "          Loss: [Train: 1.77627], [Test: 5.19143],\n",
      "          Effect: [ate-q], [train: 0.05884], [test: 0.05644]\n",
      "********************************************************************************\n",
      "epoch: 476 / 500, time cost: 43.71 sec, \n",
      "          Loss: [Train: 1.77625], [Test: 5.19403],\n",
      "          Effect: [ate-q], [train: 0.05607], [test: 0.05383]\n",
      "********************************************************************************\n",
      "epoch: 477 / 500, time cost: 43.79 sec, \n",
      "          Loss: [Train: 1.77537], [Test: 5.20336],\n",
      "          Effect: [ate-q], [train: 0.08977], [test: 0.08726]\n",
      "********************************************************************************\n",
      "epoch: 478 / 500, time cost: 45.79 sec, \n",
      "          Loss: [Train: 1.77581], [Test: 5.19656],\n",
      "          Effect: [ate-q], [train: 0.05092], [test: 0.04870]\n",
      "********************************************************************************\n",
      "epoch: 479 / 500, time cost: 42.50 sec, \n",
      "          Loss: [Train: 1.77594], [Test: 5.19143],\n",
      "          Effect: [ate-q], [train: 0.06005], [test: 0.05757]\n",
      "********************************************************************************\n",
      "epoch: 480 / 500, time cost: 42.92 sec, \n",
      "          Loss: [Train: 1.77578], [Test: 5.19100],\n",
      "          Effect: [ate-q], [train: 0.06136], [test: 0.05919]\n",
      "********************************************************************************\n",
      "epoch: 481 / 500, time cost: 42.95 sec, \n",
      "          Loss: [Train: 1.77462], [Test: 5.21246],\n",
      "          Effect: [ate-q], [train: 0.08558], [test: 0.08302]\n",
      "********************************************************************************\n",
      "epoch: 482 / 500, time cost: 42.98 sec, \n",
      "          Loss: [Train: 1.77415], [Test: 5.21816],\n",
      "          Effect: [ate-q], [train: 0.04980], [test: 0.04746]\n",
      "********************************************************************************\n",
      "epoch: 483 / 500, time cost: 42.97 sec, \n",
      "          Loss: [Train: 1.77425], [Test: 5.19245],\n",
      "          Effect: [ate-q], [train: 0.03349], [test: 0.03129]\n",
      "********************************************************************************\n",
      "epoch: 484 / 500, time cost: 43.87 sec, \n",
      "          Loss: [Train: 1.77434], [Test: 5.20049],\n",
      "          Effect: [ate-q], [train: 0.06524], [test: 0.06277]\n",
      "********************************************************************************\n",
      "epoch: 485 / 500, time cost: 44.13 sec, \n",
      "          Loss: [Train: 1.77409], [Test: 5.20872],\n",
      "          Effect: [ate-q], [train: 0.05718], [test: 0.05482]\n",
      "********************************************************************************\n",
      "epoch: 486 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.77383], [Test: 5.20668],\n",
      "          Effect: [ate-q], [train: 0.06872], [test: 0.06627]\n",
      "********************************************************************************\n",
      "epoch: 487 / 500, time cost: 43.99 sec, \n",
      "          Loss: [Train: 1.77386], [Test: 5.21945],\n",
      "          Effect: [ate-q], [train: 0.03571], [test: 0.03349]\n",
      "********************************************************************************\n",
      "epoch: 488 / 500, time cost: 43.93 sec, \n",
      "          Loss: [Train: 1.77353], [Test: 5.20484],\n",
      "          Effect: [ate-q], [train: 0.04956], [test: 0.04723]\n",
      "********************************************************************************\n",
      "epoch: 489 / 500, time cost: 43.26 sec, \n",
      "          Loss: [Train: 1.77352], [Test: 5.21048],\n",
      "          Effect: [ate-q], [train: 0.04515], [test: 0.04280]\n",
      "********************************************************************************\n",
      "epoch: 490 / 500, time cost: 43.85 sec, \n",
      "          Loss: [Train: 1.77343], [Test: 5.20223],\n",
      "          Effect: [ate-q], [train: 0.04407], [test: 0.04166]\n",
      "********************************************************************************\n",
      "epoch: 491 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 1.77270], [Test: 5.21179],\n",
      "          Effect: [ate-q], [train: 0.06734], [test: 0.06482]\n",
      "********************************************************************************\n",
      "epoch: 492 / 500, time cost: 44.11 sec, \n",
      "          Loss: [Train: 1.77292], [Test: 5.21170],\n",
      "          Effect: [ate-q], [train: 0.07045], [test: 0.06781]\n",
      "********************************************************************************\n",
      "epoch: 493 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 1.77253], [Test: 5.20516],\n",
      "          Effect: [ate-q], [train: 0.06232], [test: 0.05984]\n",
      "********************************************************************************\n",
      "epoch: 494 / 500, time cost: 44.36 sec, \n",
      "          Loss: [Train: 1.77216], [Test: 5.21626],\n",
      "          Effect: [ate-q], [train: 0.05383], [test: 0.05141]\n",
      "********************************************************************************\n",
      "epoch: 495 / 500, time cost: 44.39 sec, \n",
      "          Loss: [Train: 1.77203], [Test: 5.21365],\n",
      "          Effect: [ate-q], [train: 0.06779], [test: 0.06532]\n",
      "********************************************************************************\n",
      "epoch: 496 / 500, time cost: 44.36 sec, \n",
      "          Loss: [Train: 1.77196], [Test: 5.21091],\n",
      "          Effect: [ate-q], [train: 0.05052], [test: 0.04832]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 497 / 500, time cost: 46.52 sec, \n",
      "          Loss: [Train: 1.77209], [Test: 5.21107],\n",
      "          Effect: [ate-q], [train: 0.05779], [test: 0.05531]\n",
      "********************************************************************************\n",
      "epoch: 498 / 500, time cost: 44.39 sec, \n",
      "          Loss: [Train: 1.77145], [Test: 5.21877],\n",
      "          Effect: [ate-q], [train: 0.06486], [test: 0.06232]\n",
      "********************************************************************************\n",
      "epoch: 499 / 500, time cost: 44.52 sec, \n",
      "          Loss: [Train: 1.77159], [Test: 5.20751],\n",
      "          Effect: [ate-q], [train: 0.05786], [test: 0.05550]\n",
      "********************************************************************************\n",
      "epoch: 500 / 500, time cost: 44.38 sec, \n",
      "          Loss: [Train: 1.77112], [Test: 5.20197],\n",
      "          Effect: [ate-q], [train: 0.06041], [test: 0.05805]\n",
      "********************************************************************************\n",
      "Finish training...\n"
     ]
    }
   ],
   "source": [
    "rs_loss, rq1_loss, rq0_loss = [0.] * 3\n",
    "\n",
    "train_loss_hist, test_loss_hist, est_effect = [], [], []\n",
    "for e in range(1, epoch + 1):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        \n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, eval_loss=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test = est_casual_effect(test_loader, model, effect, estimation, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "alpha = 0.25\n",
    "beta = 5.\n",
    "c = 0.2\n",
    "i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training set in 91.56 sec\n",
      "Load validation set in 81.79 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "                             group=list(range(1)), max_length=512, min_length=10,\n",
    "                             truncate_method='first', device=device, seed=1)\n",
    "\n",
    "print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "start = time.time()\n",
    "testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                            alpha=alpha, beta=beta, c=c, i=i, \n",
    "                            group=[9], max_length=512, min_length=10,\n",
    "                            truncate_method='first', device=device)\n",
    "\n",
    "print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [effect: ate], [estimation: q], [value: 0.06024]\n",
      "Unadjusted: [value: 0.1403]\n"
     ]
    }
   ],
   "source": [
    "real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "token_embed = model.get_input_embeddings()\n",
    "model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=64, prop_is_logit=True).to(device)\n",
    "\n",
    "pos_portion = trainset.treatment.mean()\n",
    "pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epoch_iter = len(train_loader)\n",
    "total_steps = epoch * epoch_iter\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "q_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCELoss()\n",
    "prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Please specify the effect and estimation we want to use here.\n",
    "effect = 'ate'\n",
    "estimation = 'q'\n",
    "\n",
    "effect = effect.lower()\n",
    "estimation = estimation.lower()\n",
    "assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 500, time cost: 39.43 sec, \n",
      "          Loss: [Train: 2.39916], [Test: 2.40127],\n",
      "          Effect: [ate-q], [train: 0.13556], [test: 0.13556]\n",
      "********************************************************************************\n",
      "epoch: 2 / 500, time cost: 41.28 sec, \n",
      "          Loss: [Train: 2.39191], [Test: 2.39453],\n",
      "          Effect: [ate-q], [train: 0.12172], [test: 0.12171]\n",
      "********************************************************************************\n",
      "epoch: 3 / 500, time cost: 43.24 sec, \n",
      "          Loss: [Train: 2.38099], [Test: 2.38135],\n",
      "          Effect: [ate-q], [train: 0.12695], [test: 0.12697]\n",
      "********************************************************************************\n",
      "epoch: 4 / 500, time cost: 41.21 sec, \n",
      "          Loss: [Train: 2.36449], [Test: 2.36782],\n",
      "          Effect: [ate-q], [train: 0.12168], [test: 0.12183]\n",
      "********************************************************************************\n",
      "epoch: 5 / 500, time cost: 40.97 sec, \n",
      "          Loss: [Train: 2.35144], [Test: 2.36130],\n",
      "          Effect: [ate-q], [train: 0.11843], [test: 0.11863]\n",
      "********************************************************************************\n",
      "epoch: 6 / 500, time cost: 41.80 sec, \n",
      "          Loss: [Train: 2.34358], [Test: 2.35733],\n",
      "          Effect: [ate-q], [train: 0.11090], [test: 0.11115]\n",
      "********************************************************************************\n",
      "epoch: 7 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 2.33624], [Test: 2.35398],\n",
      "          Effect: [ate-q], [train: 0.09792], [test: 0.09818]\n",
      "********************************************************************************\n",
      "epoch: 8 / 500, time cost: 42.23 sec, \n",
      "          Loss: [Train: 2.32920], [Test: 2.35132],\n",
      "          Effect: [ate-q], [train: 0.12421], [test: 0.12456]\n",
      "********************************************************************************\n",
      "epoch: 9 / 500, time cost: 42.28 sec, \n",
      "          Loss: [Train: 2.32335], [Test: 2.34914],\n",
      "          Effect: [ate-q], [train: 0.09344], [test: 0.09382]\n",
      "********************************************************************************\n",
      "epoch: 10 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 2.31633], [Test: 2.35005],\n",
      "          Effect: [ate-q], [train: 0.06375], [test: 0.06420]\n",
      "********************************************************************************\n",
      "epoch: 11 / 500, time cost: 42.00 sec, \n",
      "          Loss: [Train: 2.30960], [Test: 2.34890],\n",
      "          Effect: [ate-q], [train: 0.12739], [test: 0.12792]\n",
      "********************************************************************************\n",
      "epoch: 12 / 500, time cost: 41.95 sec, \n",
      "          Loss: [Train: 2.30224], [Test: 2.34564],\n",
      "          Effect: [ate-q], [train: 0.09480], [test: 0.09545]\n",
      "********************************************************************************\n",
      "epoch: 13 / 500, time cost: 44.04 sec, \n",
      "          Loss: [Train: 2.29534], [Test: 2.34742],\n",
      "          Effect: [ate-q], [train: 0.10149], [test: 0.10228]\n",
      "********************************************************************************\n",
      "epoch: 14 / 500, time cost: 42.29 sec, \n",
      "          Loss: [Train: 2.28822], [Test: 2.35221],\n",
      "          Effect: [ate-q], [train: 0.12127], [test: 0.12222]\n",
      "********************************************************************************\n",
      "epoch: 15 / 500, time cost: 41.01 sec, \n",
      "          Loss: [Train: 2.28093], [Test: 2.34994],\n",
      "          Effect: [ate-q], [train: 0.06675], [test: 0.06774]\n",
      "********************************************************************************\n",
      "epoch: 16 / 500, time cost: 40.57 sec, \n",
      "          Loss: [Train: 2.27334], [Test: 2.35063],\n",
      "          Effect: [ate-q], [train: 0.10499], [test: 0.10613]\n",
      "********************************************************************************\n",
      "epoch: 17 / 500, time cost: 40.55 sec, \n",
      "          Loss: [Train: 2.26533], [Test: 2.35322],\n",
      "          Effect: [ate-q], [train: 0.09714], [test: 0.09845]\n",
      "********************************************************************************\n",
      "epoch: 18 / 500, time cost: 42.55 sec, \n",
      "          Loss: [Train: 2.25771], [Test: 2.35783],\n",
      "          Effect: [ate-q], [train: 0.05080], [test: 0.05204]\n",
      "********************************************************************************\n",
      "epoch: 19 / 500, time cost: 40.93 sec, \n",
      "          Loss: [Train: 2.25084], [Test: 2.35822],\n",
      "          Effect: [ate-q], [train: 0.07323], [test: 0.07468]\n",
      "********************************************************************************\n",
      "epoch: 20 / 500, time cost: 40.82 sec, \n",
      "          Loss: [Train: 2.24260], [Test: 2.36644],\n",
      "          Effect: [ate-q], [train: 0.03395], [test: 0.03548]\n",
      "********************************************************************************\n",
      "epoch: 21 / 500, time cost: 40.87 sec, \n",
      "          Loss: [Train: 2.23512], [Test: 2.36678],\n",
      "          Effect: [ate-q], [train: 0.06876], [test: 0.07047]\n",
      "********************************************************************************\n",
      "epoch: 22 / 500, time cost: 41.63 sec, \n",
      "          Loss: [Train: 2.22825], [Test: 2.37589],\n",
      "          Effect: [ate-q], [train: 0.03357], [test: 0.03527]\n",
      "********************************************************************************\n",
      "epoch: 23 / 500, time cost: 40.96 sec, \n",
      "          Loss: [Train: 2.22153], [Test: 2.37912],\n",
      "          Effect: [ate-q], [train: 0.04027], [test: 0.04204]\n",
      "********************************************************************************\n",
      "epoch: 24 / 500, time cost: 42.79 sec, \n",
      "          Loss: [Train: 2.21536], [Test: 2.38330],\n",
      "          Effect: [ate-q], [train: 0.03764], [test: 0.03945]\n",
      "********************************************************************************\n",
      "epoch: 25 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 2.20887], [Test: 2.38829],\n",
      "          Effect: [ate-q], [train: 0.04659], [test: 0.04853]\n",
      "********************************************************************************\n",
      "epoch: 26 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 2.20182], [Test: 2.39125],\n",
      "          Effect: [ate-q], [train: 0.11370], [test: 0.11610]\n",
      "********************************************************************************\n",
      "epoch: 27 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 2.19495], [Test: 2.39720],\n",
      "          Effect: [ate-q], [train: 0.05149], [test: 0.05366]\n",
      "********************************************************************************\n",
      "epoch: 28 / 500, time cost: 40.60 sec, \n",
      "          Loss: [Train: 2.18931], [Test: 2.40653],\n",
      "          Effect: [ate-q], [train: 0.06184], [test: 0.06415]\n",
      "********************************************************************************\n",
      "epoch: 29 / 500, time cost: 41.57 sec, \n",
      "          Loss: [Train: 2.18385], [Test: 2.41641],\n",
      "          Effect: [ate-q], [train: 0.01882], [test: 0.02093]\n",
      "********************************************************************************\n",
      "epoch: 30 / 500, time cost: 40.77 sec, \n",
      "          Loss: [Train: 2.17680], [Test: 2.42065],\n",
      "          Effect: [ate-q], [train: 0.13792], [test: 0.14058]\n",
      "********************************************************************************\n",
      "epoch: 31 / 500, time cost: 40.99 sec, \n",
      "          Loss: [Train: 2.17160], [Test: 2.42441],\n",
      "          Effect: [ate-q], [train: 0.00807], [test: 0.01029]\n",
      "********************************************************************************\n",
      "epoch: 32 / 500, time cost: 40.38 sec, \n",
      "          Loss: [Train: 2.16809], [Test: 2.42736],\n",
      "          Effect: [ate-q], [train: 0.03161], [test: 0.03381]\n",
      "********************************************************************************\n",
      "epoch: 33 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 2.16234], [Test: 2.43175],\n",
      "          Effect: [ate-q], [train: 0.07776], [test: 0.08024]\n",
      "********************************************************************************\n",
      "epoch: 34 / 500, time cost: 41.85 sec, \n",
      "          Loss: [Train: 2.15867], [Test: 2.43853],\n",
      "          Effect: [ate-q], [train: 0.05906], [test: 0.06154]\n",
      "********************************************************************************\n",
      "epoch: 35 / 500, time cost: 43.91 sec, \n",
      "          Loss: [Train: 2.15110], [Test: 2.45503],\n",
      "          Effect: [ate-q], [train: 0.01483], [test: 0.01703]\n",
      "********************************************************************************\n",
      "epoch: 36 / 500, time cost: 40.71 sec, \n",
      "          Loss: [Train: 2.14638], [Test: 2.45642],\n",
      "          Effect: [ate-q], [train: 0.09366], [test: 0.09633]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 / 500, time cost: 41.13 sec, \n",
      "          Loss: [Train: 2.14270], [Test: 2.45302],\n",
      "          Effect: [ate-q], [train: 0.04506], [test: 0.04740]\n",
      "********************************************************************************\n",
      "epoch: 38 / 500, time cost: 40.99 sec, \n",
      "          Loss: [Train: 2.13605], [Test: 2.45841],\n",
      "          Effect: [ate-q], [train: 0.03331], [test: 0.03561]\n",
      "********************************************************************************\n",
      "epoch: 39 / 500, time cost: 41.02 sec, \n",
      "          Loss: [Train: 2.13124], [Test: 2.47031],\n",
      "          Effect: [ate-q], [train: 0.09667], [test: 0.09926]\n",
      "********************************************************************************\n",
      "epoch: 40 / 500, time cost: 41.90 sec, \n",
      "          Loss: [Train: 2.12503], [Test: 2.46910],\n",
      "          Effect: [ate-q], [train: 0.04519], [test: 0.04748]\n",
      "********************************************************************************\n",
      "epoch: 41 / 500, time cost: 41.77 sec, \n",
      "          Loss: [Train: 2.11989], [Test: 2.47723],\n",
      "          Effect: [ate-q], [train: 0.02170], [test: 0.02399]\n",
      "********************************************************************************\n",
      "epoch: 42 / 500, time cost: 41.39 sec, \n",
      "          Loss: [Train: 2.11549], [Test: 2.49065],\n",
      "          Effect: [ate-q], [train: 0.00504], [test: 0.00685]\n",
      "********************************************************************************\n",
      "epoch: 43 / 500, time cost: 41.74 sec, \n",
      "          Loss: [Train: 2.11004], [Test: 2.48984],\n",
      "          Effect: [ate-q], [train: 0.02464], [test: 0.02676]\n",
      "********************************************************************************\n",
      "epoch: 44 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 2.10459], [Test: 2.49280],\n",
      "          Effect: [ate-q], [train: 0.03834], [test: 0.04038]\n",
      "********************************************************************************\n",
      "epoch: 45 / 500, time cost: 40.71 sec, \n",
      "          Loss: [Train: 2.09966], [Test: 2.50121],\n",
      "          Effect: [ate-q], [train: 0.10817], [test: 0.11060]\n",
      "********************************************************************************\n",
      "epoch: 46 / 500, time cost: 40.92 sec, \n",
      "          Loss: [Train: 2.09510], [Test: 2.50063],\n",
      "          Effect: [ate-q], [train: 0.06953], [test: 0.07192]\n",
      "********************************************************************************\n",
      "epoch: 47 / 500, time cost: 40.85 sec, \n",
      "          Loss: [Train: 2.09001], [Test: 2.50589],\n",
      "          Effect: [ate-q], [train: 0.04504], [test: 0.04729]\n",
      "********************************************************************************\n",
      "epoch: 48 / 500, time cost: 40.92 sec, \n",
      "          Loss: [Train: 2.08292], [Test: 2.51147],\n",
      "          Effect: [ate-q], [train: 0.07315], [test: 0.07542]\n",
      "********************************************************************************\n",
      "epoch: 49 / 500, time cost: 43.22 sec, \n",
      "          Loss: [Train: 2.07802], [Test: 2.52021],\n",
      "          Effect: [ate-q], [train: 0.04447], [test: 0.04653]\n",
      "********************************************************************************\n",
      "epoch: 50 / 500, time cost: 41.01 sec, \n",
      "          Loss: [Train: 2.07393], [Test: 2.52982],\n",
      "          Effect: [ate-q], [train: 0.11408], [test: 0.11637]\n",
      "********************************************************************************\n",
      "epoch: 51 / 500, time cost: 41.00 sec, \n",
      "          Loss: [Train: 2.06998], [Test: 2.52505],\n",
      "          Effect: [ate-q], [train: 0.10148], [test: 0.10396]\n",
      "********************************************************************************\n",
      "epoch: 52 / 500, time cost: 40.96 sec, \n",
      "          Loss: [Train: 2.06588], [Test: 2.53300],\n",
      "          Effect: [ate-q], [train: 0.06136], [test: 0.06349]\n",
      "********************************************************************************\n",
      "epoch: 53 / 500, time cost: 40.97 sec, \n",
      "          Loss: [Train: 2.06134], [Test: 2.54518],\n",
      "          Effect: [ate-q], [train: 0.04059], [test: 0.04267]\n",
      "********************************************************************************\n",
      "epoch: 54 / 500, time cost: 41.15 sec, \n",
      "          Loss: [Train: 2.05504], [Test: 2.54582],\n",
      "          Effect: [ate-q], [train: 0.02045], [test: 0.02258]\n",
      "********************************************************************************\n",
      "epoch: 55 / 500, time cost: 41.18 sec, \n",
      "          Loss: [Train: 2.05445], [Test: 2.55652],\n",
      "          Effect: [ate-q], [train: 0.04803], [test: 0.05022]\n",
      "********************************************************************************\n",
      "epoch: 56 / 500, time cost: 41.08 sec, \n",
      "          Loss: [Train: 2.05093], [Test: 2.55947],\n",
      "          Effect: [ate-q], [train: 0.13276], [test: 0.13521]\n",
      "********************************************************************************\n",
      "epoch: 57 / 500, time cost: 41.01 sec, \n",
      "          Loss: [Train: 2.04816], [Test: 2.56055],\n",
      "          Effect: [ate-q], [train: 0.07227], [test: 0.07452]\n",
      "********************************************************************************\n",
      "epoch: 58 / 500, time cost: 41.01 sec, \n",
      "          Loss: [Train: 2.04358], [Test: 2.56959],\n",
      "          Effect: [ate-q], [train: 0.12386], [test: 0.12645]\n",
      "********************************************************************************\n",
      "epoch: 59 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 2.03853], [Test: 2.57310],\n",
      "          Effect: [ate-q], [train: 0.05692], [test: 0.05901]\n",
      "********************************************************************************\n",
      "epoch: 60 / 500, time cost: 41.04 sec, \n",
      "          Loss: [Train: 2.03430], [Test: 2.58150],\n",
      "          Effect: [ate-q], [train: 0.05028], [test: 0.05223]\n",
      "********************************************************************************\n",
      "epoch: 61 / 500, time cost: 41.10 sec, \n",
      "          Loss: [Train: 2.03180], [Test: 2.59816],\n",
      "          Effect: [ate-q], [train: -0.04359], [test: -0.04190]\n",
      "********************************************************************************\n",
      "epoch: 62 / 500, time cost: 41.03 sec, \n",
      "          Loss: [Train: 2.02882], [Test: 2.58681],\n",
      "          Effect: [ate-q], [train: 0.04313], [test: 0.04512]\n",
      "********************************************************************************\n",
      "epoch: 63 / 500, time cost: 43.33 sec, \n",
      "          Loss: [Train: 2.02428], [Test: 2.59620],\n",
      "          Effect: [ate-q], [train: 0.04276], [test: 0.04499]\n",
      "********************************************************************************\n",
      "epoch: 64 / 500, time cost: 41.42 sec, \n",
      "          Loss: [Train: 2.01980], [Test: 2.60886],\n",
      "          Effect: [ate-q], [train: 0.01193], [test: 0.01390]\n",
      "********************************************************************************\n",
      "epoch: 65 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 2.01767], [Test: 2.60516],\n",
      "          Effect: [ate-q], [train: 0.05478], [test: 0.05710]\n",
      "********************************************************************************\n",
      "epoch: 66 / 500, time cost: 41.93 sec, \n",
      "          Loss: [Train: 2.01506], [Test: 2.61623],\n",
      "          Effect: [ate-q], [train: 0.00454], [test: 0.00637]\n",
      "********************************************************************************\n",
      "epoch: 67 / 500, time cost: 41.87 sec, \n",
      "          Loss: [Train: 2.01188], [Test: 2.62512],\n",
      "          Effect: [ate-q], [train: 0.09501], [test: 0.09730]\n",
      "********************************************************************************\n",
      "epoch: 68 / 500, time cost: 41.80 sec, \n",
      "          Loss: [Train: 2.00877], [Test: 2.62655],\n",
      "          Effect: [ate-q], [train: 0.03359], [test: 0.03558]\n",
      "********************************************************************************\n",
      "epoch: 69 / 500, time cost: 41.71 sec, \n",
      "          Loss: [Train: 2.00716], [Test: 2.63396],\n",
      "          Effect: [ate-q], [train: 0.13552], [test: 0.13824]\n",
      "********************************************************************************\n",
      "epoch: 70 / 500, time cost: 41.79 sec, \n",
      "          Loss: [Train: 2.00244], [Test: 2.63626],\n",
      "          Effect: [ate-q], [train: 0.11434], [test: 0.11667]\n",
      "********************************************************************************\n",
      "epoch: 71 / 500, time cost: 41.87 sec, \n",
      "          Loss: [Train: 1.99974], [Test: 2.64203],\n",
      "          Effect: [ate-q], [train: 0.09863], [test: 0.10115]\n",
      "********************************************************************************\n",
      "epoch: 72 / 500, time cost: 42.50 sec, \n",
      "          Loss: [Train: 1.99545], [Test: 2.65251],\n",
      "          Effect: [ate-q], [train: 0.12107], [test: 0.12347]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 73 / 500, time cost: 42.47 sec, \n",
      "          Loss: [Train: 1.99430], [Test: 2.66295],\n",
      "          Effect: [ate-q], [train: 0.01575], [test: 0.01782]\n",
      "********************************************************************************\n",
      "epoch: 74 / 500, time cost: 41.88 sec, \n",
      "          Loss: [Train: 1.99158], [Test: 2.67345],\n",
      "          Effect: [ate-q], [train: -0.00493], [test: -0.00320]\n",
      "********************************************************************************\n",
      "epoch: 75 / 500, time cost: 42.08 sec, \n",
      "          Loss: [Train: 1.98814], [Test: 2.67436],\n",
      "          Effect: [ate-q], [train: 0.08518], [test: 0.08742]\n",
      "********************************************************************************\n",
      "epoch: 76 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 1.98693], [Test: 2.67081],\n",
      "          Effect: [ate-q], [train: 0.05710], [test: 0.05941]\n",
      "********************************************************************************\n",
      "epoch: 77 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.98544], [Test: 2.68866],\n",
      "          Effect: [ate-q], [train: 0.06120], [test: 0.06361]\n",
      "********************************************************************************\n",
      "epoch: 78 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.98380], [Test: 2.69121],\n",
      "          Effect: [ate-q], [train: 0.03047], [test: 0.03274]\n",
      "********************************************************************************\n",
      "epoch: 79 / 500, time cost: 40.87 sec, \n",
      "          Loss: [Train: 1.98115], [Test: 2.69993],\n",
      "          Effect: [ate-q], [train: 0.11983], [test: 0.12237]\n",
      "********************************************************************************\n",
      "epoch: 80 / 500, time cost: 40.88 sec, \n",
      "          Loss: [Train: 1.97844], [Test: 2.70496],\n",
      "          Effect: [ate-q], [train: 0.03443], [test: 0.03657]\n",
      "********************************************************************************\n",
      "epoch: 81 / 500, time cost: 41.25 sec, \n",
      "          Loss: [Train: 1.97663], [Test: 2.71495],\n",
      "          Effect: [ate-q], [train: 0.08724], [test: 0.08965]\n",
      "********************************************************************************\n",
      "epoch: 82 / 500, time cost: 41.17 sec, \n",
      "          Loss: [Train: 1.97450], [Test: 2.71694],\n",
      "          Effect: [ate-q], [train: 0.07659], [test: 0.07885]\n",
      "********************************************************************************\n",
      "epoch: 83 / 500, time cost: 41.12 sec, \n",
      "          Loss: [Train: 1.97354], [Test: 2.71747],\n",
      "          Effect: [ate-q], [train: 0.10267], [test: 0.10504]\n",
      "********************************************************************************\n",
      "epoch: 84 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 1.96937], [Test: 2.73445],\n",
      "          Effect: [ate-q], [train: 0.01885], [test: 0.02085]\n",
      "********************************************************************************\n",
      "epoch: 85 / 500, time cost: 41.92 sec, \n",
      "          Loss: [Train: 1.96706], [Test: 2.73638],\n",
      "          Effect: [ate-q], [train: 0.01295], [test: 0.01491]\n",
      "********************************************************************************\n",
      "epoch: 86 / 500, time cost: 40.55 sec, \n",
      "          Loss: [Train: 1.96452], [Test: 2.74548],\n",
      "          Effect: [ate-q], [train: 0.01406], [test: 0.01600]\n",
      "********************************************************************************\n",
      "epoch: 87 / 500, time cost: 42.31 sec, \n",
      "          Loss: [Train: 1.96390], [Test: 2.76321],\n",
      "          Effect: [ate-q], [train: -0.00093], [test: 0.00101]\n",
      "********************************************************************************\n",
      "epoch: 88 / 500, time cost: 42.38 sec, \n",
      "          Loss: [Train: 1.96145], [Test: 2.75748],\n",
      "          Effect: [ate-q], [train: 0.12138], [test: 0.12378]\n",
      "********************************************************************************\n",
      "epoch: 89 / 500, time cost: 42.38 sec, \n",
      "          Loss: [Train: 1.95915], [Test: 2.76686],\n",
      "          Effect: [ate-q], [train: 0.02382], [test: 0.02595]\n",
      "********************************************************************************\n",
      "epoch: 90 / 500, time cost: 42.50 sec, \n",
      "          Loss: [Train: 1.95683], [Test: 2.77568],\n",
      "          Effect: [ate-q], [train: 0.04103], [test: 0.04329]\n",
      "********************************************************************************\n",
      "epoch: 91 / 500, time cost: 44.53 sec, \n",
      "          Loss: [Train: 1.95449], [Test: 2.77246],\n",
      "          Effect: [ate-q], [train: 0.01794], [test: 0.02021]\n",
      "********************************************************************************\n",
      "epoch: 92 / 500, time cost: 42.54 sec, \n",
      "          Loss: [Train: 1.95341], [Test: 2.78579],\n",
      "          Effect: [ate-q], [train: 0.08108], [test: 0.08347]\n",
      "********************************************************************************\n",
      "epoch: 93 / 500, time cost: 41.91 sec, \n",
      "          Loss: [Train: 1.95193], [Test: 2.79339],\n",
      "          Effect: [ate-q], [train: 0.01708], [test: 0.01903]\n",
      "********************************************************************************\n",
      "epoch: 94 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.95002], [Test: 2.79957],\n",
      "          Effect: [ate-q], [train: 0.02397], [test: 0.02578]\n",
      "********************************************************************************\n",
      "epoch: 95 / 500, time cost: 42.06 sec, \n",
      "          Loss: [Train: 1.94781], [Test: 2.80124],\n",
      "          Effect: [ate-q], [train: 0.01511], [test: 0.01695]\n",
      "********************************************************************************\n",
      "epoch: 96 / 500, time cost: 42.05 sec, \n",
      "          Loss: [Train: 1.94774], [Test: 2.80991],\n",
      "          Effect: [ate-q], [train: 0.01385], [test: 0.01594]\n",
      "********************************************************************************\n",
      "epoch: 97 / 500, time cost: 41.98 sec, \n",
      "          Loss: [Train: 1.94377], [Test: 2.81478],\n",
      "          Effect: [ate-q], [train: 0.01249], [test: 0.01434]\n",
      "********************************************************************************\n",
      "epoch: 98 / 500, time cost: 42.31 sec, \n",
      "          Loss: [Train: 1.94082], [Test: 2.81420],\n",
      "          Effect: [ate-q], [train: 0.07233], [test: 0.07468]\n",
      "********************************************************************************\n",
      "epoch: 99 / 500, time cost: 42.47 sec, \n",
      "          Loss: [Train: 1.94146], [Test: 2.83008],\n",
      "          Effect: [ate-q], [train: 0.01102], [test: 0.01329]\n",
      "********************************************************************************\n",
      "epoch: 100 / 500, time cost: 42.13 sec, \n",
      "          Loss: [Train: 1.94056], [Test: 2.83672],\n",
      "          Effect: [ate-q], [train: 0.09716], [test: 0.09928]\n",
      "********************************************************************************\n",
      "epoch: 101 / 500, time cost: 41.26 sec, \n",
      "          Loss: [Train: 1.93924], [Test: 2.84816],\n",
      "          Effect: [ate-q], [train: -0.04475], [test: -0.04299]\n",
      "********************************************************************************\n",
      "epoch: 102 / 500, time cost: 42.53 sec, \n",
      "          Loss: [Train: 1.93450], [Test: 2.86067],\n",
      "          Effect: [ate-q], [train: 0.00772], [test: 0.00929]\n",
      "********************************************************************************\n",
      "epoch: 103 / 500, time cost: 42.30 sec, \n",
      "          Loss: [Train: 1.93492], [Test: 2.85988],\n",
      "          Effect: [ate-q], [train: 0.01642], [test: 0.01825]\n",
      "********************************************************************************\n",
      "epoch: 104 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.93192], [Test: 2.86065],\n",
      "          Effect: [ate-q], [train: -0.00690], [test: -0.00495]\n",
      "********************************************************************************\n",
      "epoch: 105 / 500, time cost: 44.30 sec, \n",
      "          Loss: [Train: 1.92964], [Test: 2.86099],\n",
      "          Effect: [ate-q], [train: 0.02417], [test: 0.02609]\n",
      "********************************************************************************\n",
      "epoch: 106 / 500, time cost: 42.47 sec, \n",
      "          Loss: [Train: 1.93196], [Test: 2.86515],\n",
      "          Effect: [ate-q], [train: 0.03716], [test: 0.03884]\n",
      "********************************************************************************\n",
      "epoch: 107 / 500, time cost: 42.30 sec, \n",
      "          Loss: [Train: 1.92912], [Test: 2.88596],\n",
      "          Effect: [ate-q], [train: 0.05539], [test: 0.05709]\n",
      "********************************************************************************\n",
      "epoch: 108 / 500, time cost: 42.19 sec, \n",
      "          Loss: [Train: 1.92809], [Test: 2.87189],\n",
      "          Effect: [ate-q], [train: 0.10444], [test: 0.10651]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 109 / 500, time cost: 42.23 sec, \n",
      "          Loss: [Train: 1.92265], [Test: 2.88265],\n",
      "          Effect: [ate-q], [train: 0.11026], [test: 0.11244]\n",
      "********************************************************************************\n",
      "epoch: 110 / 500, time cost: 42.10 sec, \n",
      "          Loss: [Train: 1.92418], [Test: 2.88760],\n",
      "          Effect: [ate-q], [train: 0.11449], [test: 0.11673]\n",
      "********************************************************************************\n",
      "epoch: 111 / 500, time cost: 42.27 sec, \n",
      "          Loss: [Train: 1.92236], [Test: 2.89937],\n",
      "          Effect: [ate-q], [train: 0.09301], [test: 0.09487]\n",
      "********************************************************************************\n",
      "epoch: 112 / 500, time cost: 42.33 sec, \n",
      "          Loss: [Train: 1.92000], [Test: 2.92863],\n",
      "          Effect: [ate-q], [train: -0.06940], [test: -0.06765]\n",
      "********************************************************************************\n",
      "epoch: 113 / 500, time cost: 41.81 sec, \n",
      "          Loss: [Train: 1.91801], [Test: 2.89658],\n",
      "          Effect: [ate-q], [train: 0.06172], [test: 0.06363]\n",
      "********************************************************************************\n",
      "epoch: 114 / 500, time cost: 41.91 sec, \n",
      "          Loss: [Train: 1.91718], [Test: 2.90058],\n",
      "          Effect: [ate-q], [train: 0.06606], [test: 0.06798]\n",
      "********************************************************************************\n",
      "epoch: 115 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.91618], [Test: 2.91822],\n",
      "          Effect: [ate-q], [train: 0.03126], [test: 0.03311]\n",
      "********************************************************************************\n",
      "epoch: 116 / 500, time cost: 42.03 sec, \n",
      "          Loss: [Train: 1.91206], [Test: 2.92437],\n",
      "          Effect: [ate-q], [train: 0.02536], [test: 0.02726]\n",
      "********************************************************************************\n",
      "epoch: 117 / 500, time cost: 41.63 sec, \n",
      "          Loss: [Train: 1.91173], [Test: 2.92914],\n",
      "          Effect: [ate-q], [train: 0.00582], [test: 0.00778]\n",
      "********************************************************************************\n",
      "epoch: 118 / 500, time cost: 41.75 sec, \n",
      "          Loss: [Train: 1.91111], [Test: 2.92447],\n",
      "          Effect: [ate-q], [train: 0.12004], [test: 0.12198]\n",
      "********************************************************************************\n",
      "epoch: 119 / 500, time cost: 44.20 sec, \n",
      "          Loss: [Train: 1.90954], [Test: 2.91484],\n",
      "          Effect: [ate-q], [train: 0.09380], [test: 0.09549]\n",
      "********************************************************************************\n",
      "epoch: 120 / 500, time cost: 41.98 sec, \n",
      "          Loss: [Train: 1.90702], [Test: 2.93675],\n",
      "          Effect: [ate-q], [train: 0.12246], [test: 0.12448]\n",
      "********************************************************************************\n",
      "epoch: 121 / 500, time cost: 41.83 sec, \n",
      "          Loss: [Train: 1.90705], [Test: 2.95083],\n",
      "          Effect: [ate-q], [train: 0.07098], [test: 0.07271]\n",
      "********************************************************************************\n",
      "epoch: 122 / 500, time cost: 42.19 sec, \n",
      "          Loss: [Train: 1.90651], [Test: 2.99811],\n",
      "          Effect: [ate-q], [train: -0.06585], [test: -0.06431]\n",
      "********************************************************************************\n",
      "epoch: 123 / 500, time cost: 42.11 sec, \n",
      "          Loss: [Train: 1.90206], [Test: 2.99826],\n",
      "          Effect: [ate-q], [train: -0.05128], [test: -0.04953]\n",
      "********************************************************************************\n",
      "epoch: 124 / 500, time cost: 42.08 sec, \n",
      "          Loss: [Train: 1.90186], [Test: 2.94887],\n",
      "          Effect: [ate-q], [train: 0.11827], [test: 0.12042]\n",
      "********************************************************************************\n",
      "epoch: 125 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.89936], [Test: 2.96280],\n",
      "          Effect: [ate-q], [train: 0.04606], [test: 0.04802]\n",
      "********************************************************************************\n",
      "epoch: 126 / 500, time cost: 41.99 sec, \n",
      "          Loss: [Train: 1.89760], [Test: 2.96938],\n",
      "          Effect: [ate-q], [train: 0.09616], [test: 0.09811]\n",
      "********************************************************************************\n",
      "epoch: 127 / 500, time cost: 41.78 sec, \n",
      "          Loss: [Train: 1.89847], [Test: 2.98958],\n",
      "          Effect: [ate-q], [train: -0.00659], [test: -0.00475]\n",
      "********************************************************************************\n",
      "epoch: 128 / 500, time cost: 42.25 sec, \n",
      "          Loss: [Train: 1.89541], [Test: 2.97738],\n",
      "          Effect: [ate-q], [train: 0.09842], [test: 0.10067]\n",
      "********************************************************************************\n",
      "epoch: 129 / 500, time cost: 42.11 sec, \n",
      "          Loss: [Train: 1.89648], [Test: 3.00491],\n",
      "          Effect: [ate-q], [train: -0.02896], [test: -0.02728]\n",
      "********************************************************************************\n",
      "epoch: 130 / 500, time cost: 41.10 sec, \n",
      "          Loss: [Train: 1.89287], [Test: 2.99327],\n",
      "          Effect: [ate-q], [train: 0.08513], [test: 0.08726]\n",
      "********************************************************************************\n",
      "epoch: 131 / 500, time cost: 41.09 sec, \n",
      "          Loss: [Train: 1.89178], [Test: 3.01500],\n",
      "          Effect: [ate-q], [train: -0.01136], [test: -0.00924]\n",
      "********************************************************************************\n",
      "epoch: 132 / 500, time cost: 41.34 sec, \n",
      "          Loss: [Train: 1.89086], [Test: 3.00720],\n",
      "          Effect: [ate-q], [train: 0.02726], [test: 0.02904]\n",
      "********************************************************************************\n",
      "epoch: 133 / 500, time cost: 43.77 sec, \n",
      "          Loss: [Train: 1.88879], [Test: 3.04762],\n",
      "          Effect: [ate-q], [train: -0.08987], [test: -0.08816]\n",
      "********************************************************************************\n",
      "epoch: 134 / 500, time cost: 42.21 sec, \n",
      "          Loss: [Train: 1.88869], [Test: 3.02250],\n",
      "          Effect: [ate-q], [train: -0.01120], [test: -0.00928]\n",
      "********************************************************************************\n",
      "epoch: 135 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.88482], [Test: 3.01035],\n",
      "          Effect: [ate-q], [train: 0.05088], [test: 0.05252]\n",
      "********************************************************************************\n",
      "epoch: 136 / 500, time cost: 40.62 sec, \n",
      "          Loss: [Train: 1.88467], [Test: 3.02496],\n",
      "          Effect: [ate-q], [train: 0.01816], [test: 0.01976]\n",
      "********************************************************************************\n",
      "epoch: 137 / 500, time cost: 40.72 sec, \n",
      "          Loss: [Train: 1.88420], [Test: 3.02849],\n",
      "          Effect: [ate-q], [train: 0.11016], [test: 0.11202]\n",
      "********************************************************************************\n",
      "epoch: 138 / 500, time cost: 40.83 sec, \n",
      "          Loss: [Train: 1.88282], [Test: 3.02246],\n",
      "          Effect: [ate-q], [train: 0.09436], [test: 0.09617]\n",
      "********************************************************************************\n",
      "epoch: 139 / 500, time cost: 40.69 sec, \n",
      "          Loss: [Train: 1.88190], [Test: 3.02649],\n",
      "          Effect: [ate-q], [train: 0.07923], [test: 0.08103]\n",
      "********************************************************************************\n",
      "epoch: 140 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.87816], [Test: 3.04921],\n",
      "          Effect: [ate-q], [train: 0.10902], [test: 0.11090]\n",
      "********************************************************************************\n",
      "epoch: 141 / 500, time cost: 41.75 sec, \n",
      "          Loss: [Train: 1.87640], [Test: 3.04797],\n",
      "          Effect: [ate-q], [train: 0.06749], [test: 0.06925]\n",
      "********************************************************************************\n",
      "epoch: 142 / 500, time cost: 41.86 sec, \n",
      "          Loss: [Train: 1.87553], [Test: 3.07063],\n",
      "          Effect: [ate-q], [train: -0.01917], [test: -0.01748]\n",
      "********************************************************************************\n",
      "epoch: 143 / 500, time cost: 41.87 sec, \n",
      "          Loss: [Train: 1.87465], [Test: 3.07280],\n",
      "          Effect: [ate-q], [train: -0.00584], [test: -0.00417]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 144 / 500, time cost: 41.28 sec, \n",
      "          Loss: [Train: 1.87423], [Test: 3.06632],\n",
      "          Effect: [ate-q], [train: 0.08209], [test: 0.08390]\n",
      "********************************************************************************\n",
      "epoch: 145 / 500, time cost: 41.16 sec, \n",
      "          Loss: [Train: 1.87136], [Test: 3.09463],\n",
      "          Effect: [ate-q], [train: -0.00821], [test: -0.00649]\n",
      "********************************************************************************\n",
      "epoch: 146 / 500, time cost: 41.29 sec, \n",
      "          Loss: [Train: 1.87045], [Test: 3.09285],\n",
      "          Effect: [ate-q], [train: 0.02314], [test: 0.02501]\n",
      "********************************************************************************\n",
      "epoch: 147 / 500, time cost: 43.28 sec, \n",
      "          Loss: [Train: 1.87102], [Test: 3.08150],\n",
      "          Effect: [ate-q], [train: 0.05421], [test: 0.05613]\n",
      "********************************************************************************\n",
      "epoch: 148 / 500, time cost: 41.21 sec, \n",
      "          Loss: [Train: 1.86805], [Test: 3.09234],\n",
      "          Effect: [ate-q], [train: 0.09435], [test: 0.09612]\n",
      "********************************************************************************\n",
      "epoch: 149 / 500, time cost: 42.11 sec, \n",
      "          Loss: [Train: 1.86634], [Test: 3.13795],\n",
      "          Effect: [ate-q], [train: -0.04430], [test: -0.04253]\n",
      "********************************************************************************\n",
      "epoch: 150 / 500, time cost: 42.00 sec, \n",
      "          Loss: [Train: 1.86566], [Test: 3.12929],\n",
      "          Effect: [ate-q], [train: -0.00362], [test: -0.00180]\n",
      "********************************************************************************\n",
      "epoch: 151 / 500, time cost: 41.90 sec, \n",
      "          Loss: [Train: 1.86421], [Test: 3.12134],\n",
      "          Effect: [ate-q], [train: 0.02635], [test: 0.02817]\n",
      "********************************************************************************\n",
      "epoch: 152 / 500, time cost: 41.88 sec, \n",
      "          Loss: [Train: 1.86284], [Test: 3.11236],\n",
      "          Effect: [ate-q], [train: 0.08589], [test: 0.08767]\n",
      "********************************************************************************\n",
      "epoch: 153 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.86289], [Test: 3.13966],\n",
      "          Effect: [ate-q], [train: 0.04713], [test: 0.04917]\n",
      "********************************************************************************\n",
      "epoch: 154 / 500, time cost: 41.93 sec, \n",
      "          Loss: [Train: 1.86124], [Test: 3.14224],\n",
      "          Effect: [ate-q], [train: 0.01140], [test: 0.01337]\n",
      "********************************************************************************\n",
      "epoch: 155 / 500, time cost: 42.08 sec, \n",
      "          Loss: [Train: 1.85894], [Test: 3.13517],\n",
      "          Effect: [ate-q], [train: 0.12135], [test: 0.12302]\n",
      "********************************************************************************\n",
      "epoch: 156 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.85757], [Test: 3.18485],\n",
      "          Effect: [ate-q], [train: -0.06933], [test: -0.06752]\n",
      "********************************************************************************\n",
      "epoch: 157 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.85773], [Test: 3.15565],\n",
      "          Effect: [ate-q], [train: 0.01186], [test: 0.01370]\n",
      "********************************************************************************\n",
      "epoch: 158 / 500, time cost: 42.44 sec, \n",
      "          Loss: [Train: 1.85567], [Test: 3.16430],\n",
      "          Effect: [ate-q], [train: 0.04169], [test: 0.04314]\n",
      "********************************************************************************\n",
      "epoch: 159 / 500, time cost: 42.56 sec, \n",
      "          Loss: [Train: 1.85356], [Test: 3.17638],\n",
      "          Effect: [ate-q], [train: 0.07088], [test: 0.07308]\n",
      "********************************************************************************\n",
      "epoch: 160 / 500, time cost: 42.43 sec, \n",
      "          Loss: [Train: 1.85274], [Test: 3.20306],\n",
      "          Effect: [ate-q], [train: -0.03820], [test: -0.03621]\n",
      "********************************************************************************\n",
      "epoch: 161 / 500, time cost: 44.61 sec, \n",
      "          Loss: [Train: 1.85022], [Test: 3.20588],\n",
      "          Effect: [ate-q], [train: -0.01944], [test: -0.01764]\n",
      "********************************************************************************\n",
      "epoch: 162 / 500, time cost: 42.45 sec, \n",
      "          Loss: [Train: 1.85255], [Test: 3.21950],\n",
      "          Effect: [ate-q], [train: -0.03841], [test: -0.03668]\n",
      "********************************************************************************\n",
      "epoch: 163 / 500, time cost: 42.03 sec, \n",
      "          Loss: [Train: 1.84835], [Test: 3.19559],\n",
      "          Effect: [ate-q], [train: 0.05885], [test: 0.06050]\n",
      "********************************************************************************\n",
      "epoch: 164 / 500, time cost: 41.68 sec, \n",
      "          Loss: [Train: 1.84890], [Test: 3.20362],\n",
      "          Effect: [ate-q], [train: 0.03866], [test: 0.04037]\n",
      "********************************************************************************\n",
      "epoch: 165 / 500, time cost: 40.49 sec, \n",
      "          Loss: [Train: 1.84495], [Test: 3.21750],\n",
      "          Effect: [ate-q], [train: 0.01124], [test: 0.01326]\n",
      "********************************************************************************\n",
      "epoch: 166 / 500, time cost: 41.63 sec, \n",
      "          Loss: [Train: 1.84467], [Test: 3.21548],\n",
      "          Effect: [ate-q], [train: 0.02144], [test: 0.02332]\n",
      "********************************************************************************\n",
      "epoch: 167 / 500, time cost: 40.68 sec, \n",
      "          Loss: [Train: 1.84395], [Test: 3.20673],\n",
      "          Effect: [ate-q], [train: 0.10167], [test: 0.10347]\n",
      "********************************************************************************\n",
      "epoch: 168 / 500, time cost: 41.02 sec, \n",
      "          Loss: [Train: 1.84250], [Test: 3.20874],\n",
      "          Effect: [ate-q], [train: 0.08349], [test: 0.08561]\n",
      "********************************************************************************\n",
      "epoch: 169 / 500, time cost: 40.87 sec, \n",
      "          Loss: [Train: 1.84060], [Test: 3.23944],\n",
      "          Effect: [ate-q], [train: 0.04921], [test: 0.05110]\n",
      "********************************************************************************\n",
      "epoch: 170 / 500, time cost: 40.85 sec, \n",
      "          Loss: [Train: 1.84150], [Test: 3.25297],\n",
      "          Effect: [ate-q], [train: -0.04417], [test: -0.04227]\n",
      "********************************************************************************\n",
      "epoch: 171 / 500, time cost: 40.89 sec, \n",
      "          Loss: [Train: 1.83861], [Test: 3.23486],\n",
      "          Effect: [ate-q], [train: -0.00912], [test: -0.00751]\n",
      "********************************************************************************\n",
      "epoch: 172 / 500, time cost: 40.74 sec, \n",
      "          Loss: [Train: 1.83724], [Test: 3.24562],\n",
      "          Effect: [ate-q], [train: 0.01929], [test: 0.02114]\n",
      "********************************************************************************\n",
      "epoch: 173 / 500, time cost: 40.88 sec, \n",
      "          Loss: [Train: 1.83695], [Test: 3.27354],\n",
      "          Effect: [ate-q], [train: -0.02204], [test: -0.02023]\n",
      "********************************************************************************\n",
      "epoch: 174 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 1.83547], [Test: 3.26796],\n",
      "          Effect: [ate-q], [train: 0.01205], [test: 0.01416]\n",
      "********************************************************************************\n",
      "epoch: 175 / 500, time cost: 42.90 sec, \n",
      "          Loss: [Train: 1.83404], [Test: 3.27537],\n",
      "          Effect: [ate-q], [train: -0.01159], [test: -0.01008]\n",
      "********************************************************************************\n",
      "epoch: 176 / 500, time cost: 40.91 sec, \n",
      "          Loss: [Train: 1.83270], [Test: 3.26874],\n",
      "          Effect: [ate-q], [train: 0.00497], [test: 0.00681]\n",
      "********************************************************************************\n",
      "epoch: 177 / 500, time cost: 40.95 sec, \n",
      "          Loss: [Train: 1.83169], [Test: 3.25097],\n",
      "          Effect: [ate-q], [train: 0.06402], [test: 0.06563]\n",
      "********************************************************************************\n",
      "epoch: 178 / 500, time cost: 41.06 sec, \n",
      "          Loss: [Train: 1.82885], [Test: 3.27475],\n",
      "          Effect: [ate-q], [train: 0.01979], [test: 0.02155]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 179 / 500, time cost: 40.82 sec, \n",
      "          Loss: [Train: 1.82912], [Test: 3.28602],\n",
      "          Effect: [ate-q], [train: 0.04011], [test: 0.04213]\n",
      "********************************************************************************\n",
      "epoch: 180 / 500, time cost: 40.75 sec, \n",
      "          Loss: [Train: 1.82752], [Test: 3.29442],\n",
      "          Effect: [ate-q], [train: 0.05420], [test: 0.05615]\n",
      "********************************************************************************\n",
      "epoch: 181 / 500, time cost: 40.70 sec, \n",
      "          Loss: [Train: 1.82482], [Test: 3.31738],\n",
      "          Effect: [ate-q], [train: 0.01233], [test: 0.01421]\n",
      "********************************************************************************\n",
      "epoch: 182 / 500, time cost: 40.83 sec, \n",
      "          Loss: [Train: 1.82418], [Test: 3.31623],\n",
      "          Effect: [ate-q], [train: 0.02756], [test: 0.02947]\n",
      "********************************************************************************\n",
      "epoch: 183 / 500, time cost: 41.11 sec, \n",
      "          Loss: [Train: 1.82384], [Test: 3.30993],\n",
      "          Effect: [ate-q], [train: 0.03073], [test: 0.03253]\n",
      "********************************************************************************\n",
      "epoch: 184 / 500, time cost: 41.26 sec, \n",
      "          Loss: [Train: 1.82215], [Test: 3.31632],\n",
      "          Effect: [ate-q], [train: 0.04784], [test: 0.04961]\n",
      "********************************************************************************\n",
      "epoch: 185 / 500, time cost: 41.07 sec, \n",
      "          Loss: [Train: 1.81939], [Test: 3.31580],\n",
      "          Effect: [ate-q], [train: 0.09501], [test: 0.09703]\n",
      "********************************************************************************\n",
      "epoch: 186 / 500, time cost: 40.77 sec, \n",
      "          Loss: [Train: 1.82025], [Test: 3.32030],\n",
      "          Effect: [ate-q], [train: 0.09344], [test: 0.09541]\n",
      "********************************************************************************\n",
      "epoch: 187 / 500, time cost: 40.84 sec, \n",
      "          Loss: [Train: 1.81849], [Test: 3.31526],\n",
      "          Effect: [ate-q], [train: 0.06755], [test: 0.06943]\n",
      "********************************************************************************\n",
      "epoch: 188 / 500, time cost: 40.69 sec, \n",
      "          Loss: [Train: 1.81795], [Test: 3.33770],\n",
      "          Effect: [ate-q], [train: 0.07082], [test: 0.07267]\n",
      "********************************************************************************\n",
      "epoch: 189 / 500, time cost: 44.05 sec, \n",
      "          Loss: [Train: 1.81580], [Test: 3.36553],\n",
      "          Effect: [ate-q], [train: -0.02078], [test: -0.01907]\n",
      "********************************************************************************\n",
      "epoch: 190 / 500, time cost: 42.10 sec, \n",
      "          Loss: [Train: 1.81592], [Test: 3.37332],\n",
      "          Effect: [ate-q], [train: -0.01080], [test: -0.00906]\n",
      "********************************************************************************\n",
      "epoch: 191 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 1.81479], [Test: 3.34725],\n",
      "          Effect: [ate-q], [train: 0.07758], [test: 0.07923]\n",
      "********************************************************************************\n",
      "epoch: 192 / 500, time cost: 42.03 sec, \n",
      "          Loss: [Train: 1.81082], [Test: 3.36760],\n",
      "          Effect: [ate-q], [train: 0.10987], [test: 0.11205]\n",
      "********************************************************************************\n",
      "epoch: 193 / 500, time cost: 42.08 sec, \n",
      "          Loss: [Train: 1.81022], [Test: 3.37778],\n",
      "          Effect: [ate-q], [train: 0.05191], [test: 0.05384]\n",
      "********************************************************************************\n",
      "epoch: 194 / 500, time cost: 42.20 sec, \n",
      "          Loss: [Train: 1.80821], [Test: 3.36786],\n",
      "          Effect: [ate-q], [train: 0.08233], [test: 0.08437]\n",
      "********************************************************************************\n",
      "epoch: 195 / 500, time cost: 42.11 sec, \n",
      "          Loss: [Train: 1.80649], [Test: 3.38260],\n",
      "          Effect: [ate-q], [train: 0.07597], [test: 0.07763]\n",
      "********************************************************************************\n",
      "epoch: 196 / 500, time cost: 41.92 sec, \n",
      "          Loss: [Train: 1.80691], [Test: 3.40995],\n",
      "          Effect: [ate-q], [train: 0.00644], [test: 0.00814]\n",
      "********************************************************************************\n",
      "epoch: 197 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 1.80405], [Test: 3.42950],\n",
      "          Effect: [ate-q], [train: 0.00927], [test: 0.01137]\n",
      "********************************************************************************\n",
      "epoch: 198 / 500, time cost: 40.85 sec, \n",
      "          Loss: [Train: 1.80452], [Test: 3.44432],\n",
      "          Effect: [ate-q], [train: 0.01645], [test: 0.01808]\n",
      "********************************************************************************\n",
      "epoch: 199 / 500, time cost: 40.69 sec, \n",
      "          Loss: [Train: 1.80304], [Test: 3.42822],\n",
      "          Effect: [ate-q], [train: 0.04736], [test: 0.04909]\n",
      "********************************************************************************\n",
      "epoch: 200 / 500, time cost: 40.99 sec, \n",
      "          Loss: [Train: 1.80152], [Test: 3.42132],\n",
      "          Effect: [ate-q], [train: 0.10972], [test: 0.11169]\n",
      "********************************************************************************\n",
      "epoch: 201 / 500, time cost: 41.08 sec, \n",
      "          Loss: [Train: 1.79979], [Test: 3.45289],\n",
      "          Effect: [ate-q], [train: 0.03133], [test: 0.03287]\n",
      "********************************************************************************\n",
      "epoch: 202 / 500, time cost: 42.48 sec, \n",
      "          Loss: [Train: 1.79911], [Test: 3.46806],\n",
      "          Effect: [ate-q], [train: -0.00097], [test: 0.00130]\n",
      "********************************************************************************\n",
      "epoch: 203 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.79841], [Test: 3.51687],\n",
      "          Effect: [ate-q], [train: -0.04901], [test: -0.04675]\n",
      "********************************************************************************\n",
      "epoch: 204 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.79577], [Test: 3.44812],\n",
      "          Effect: [ate-q], [train: 0.15312], [test: 0.15487]\n",
      "********************************************************************************\n",
      "epoch: 205 / 500, time cost: 42.19 sec, \n",
      "          Loss: [Train: 1.79366], [Test: 3.51255],\n",
      "          Effect: [ate-q], [train: -0.02537], [test: -0.02337]\n",
      "********************************************************************************\n",
      "epoch: 206 / 500, time cost: 42.13 sec, \n",
      "          Loss: [Train: 1.79285], [Test: 3.50038],\n",
      "          Effect: [ate-q], [train: 0.07985], [test: 0.08196]\n",
      "********************************************************************************\n",
      "epoch: 207 / 500, time cost: 42.09 sec, \n",
      "          Loss: [Train: 1.79120], [Test: 3.50341],\n",
      "          Effect: [ate-q], [train: 0.02506], [test: 0.02670]\n",
      "********************************************************************************\n",
      "epoch: 208 / 500, time cost: 41.70 sec, \n",
      "          Loss: [Train: 1.79143], [Test: 3.51448],\n",
      "          Effect: [ate-q], [train: 0.01349], [test: 0.01564]\n",
      "********************************************************************************\n",
      "epoch: 209 / 500, time cost: 42.36 sec, \n",
      "          Loss: [Train: 1.78889], [Test: 3.52908],\n",
      "          Effect: [ate-q], [train: 0.03556], [test: 0.03727]\n",
      "********************************************************************************\n",
      "epoch: 210 / 500, time cost: 41.82 sec, \n",
      "          Loss: [Train: 1.78961], [Test: 3.55518],\n",
      "          Effect: [ate-q], [train: -0.06253], [test: -0.06064]\n",
      "********************************************************************************\n",
      "epoch: 211 / 500, time cost: 41.83 sec, \n",
      "          Loss: [Train: 1.78647], [Test: 3.54026],\n",
      "          Effect: [ate-q], [train: 0.01094], [test: 0.01269]\n",
      "********************************************************************************\n",
      "epoch: 212 / 500, time cost: 41.96 sec, \n",
      "          Loss: [Train: 1.78611], [Test: 3.56061],\n",
      "          Effect: [ate-q], [train: -0.02698], [test: -0.02528]\n",
      "********************************************************************************\n",
      "epoch: 213 / 500, time cost: 42.03 sec, \n",
      "          Loss: [Train: 1.78476], [Test: 3.58637],\n",
      "          Effect: [ate-q], [train: -0.07950], [test: -0.07782]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 214 / 500, time cost: 41.93 sec, \n",
      "          Loss: [Train: 1.78404], [Test: 3.59449],\n",
      "          Effect: [ate-q], [train: -0.01442], [test: -0.01264]\n",
      "********************************************************************************\n",
      "epoch: 215 / 500, time cost: 42.28 sec, \n",
      "          Loss: [Train: 1.78624], [Test: 3.54956],\n",
      "          Effect: [ate-q], [train: 0.03276], [test: 0.03468]\n",
      "********************************************************************************\n",
      "epoch: 216 / 500, time cost: 42.17 sec, \n",
      "          Loss: [Train: 1.77929], [Test: 3.58262],\n",
      "          Effect: [ate-q], [train: -0.00586], [test: -0.00436]\n",
      "********************************************************************************\n",
      "epoch: 217 / 500, time cost: 42.97 sec, \n",
      "          Loss: [Train: 1.78135], [Test: 3.57740],\n",
      "          Effect: [ate-q], [train: 0.01975], [test: 0.02158]\n",
      "********************************************************************************\n",
      "epoch: 218 / 500, time cost: 41.92 sec, \n",
      "          Loss: [Train: 1.77849], [Test: 3.59397],\n",
      "          Effect: [ate-q], [train: 0.07904], [test: 0.08065]\n",
      "********************************************************************************\n",
      "epoch: 219 / 500, time cost: 42.02 sec, \n",
      "          Loss: [Train: 1.77791], [Test: 3.58650],\n",
      "          Effect: [ate-q], [train: 0.01182], [test: 0.01347]\n",
      "********************************************************************************\n",
      "epoch: 220 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.77631], [Test: 3.60109],\n",
      "          Effect: [ate-q], [train: 0.03217], [test: 0.03364]\n",
      "********************************************************************************\n",
      "epoch: 221 / 500, time cost: 40.94 sec, \n",
      "          Loss: [Train: 1.77421], [Test: 3.62174],\n",
      "          Effect: [ate-q], [train: -0.02828], [test: -0.02653]\n",
      "********************************************************************************\n",
      "epoch: 222 / 500, time cost: 40.72 sec, \n",
      "          Loss: [Train: 1.77451], [Test: 3.61241],\n",
      "          Effect: [ate-q], [train: 0.07605], [test: 0.07740]\n",
      "********************************************************************************\n",
      "epoch: 223 / 500, time cost: 40.98 sec, \n",
      "          Loss: [Train: 1.77406], [Test: 3.62749],\n",
      "          Effect: [ate-q], [train: 0.02189], [test: 0.02352]\n",
      "********************************************************************************\n",
      "epoch: 224 / 500, time cost: 42.39 sec, \n",
      "          Loss: [Train: 1.77067], [Test: 3.63452],\n",
      "          Effect: [ate-q], [train: -0.00705], [test: -0.00564]\n",
      "********************************************************************************\n",
      "epoch: 225 / 500, time cost: 42.19 sec, \n",
      "          Loss: [Train: 1.77085], [Test: 3.65921],\n",
      "          Effect: [ate-q], [train: 0.02459], [test: 0.02634]\n",
      "********************************************************************************\n",
      "epoch: 226 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.76897], [Test: 3.67377],\n",
      "          Effect: [ate-q], [train: 0.00815], [test: 0.00983]\n",
      "********************************************************************************\n",
      "epoch: 227 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.76984], [Test: 3.67649],\n",
      "          Effect: [ate-q], [train: 0.01585], [test: 0.01759]\n",
      "********************************************************************************\n",
      "epoch: 228 / 500, time cost: 42.32 sec, \n",
      "          Loss: [Train: 1.76678], [Test: 3.68648],\n",
      "          Effect: [ate-q], [train: -0.00244], [test: -0.00056]\n",
      "********************************************************************************\n",
      "epoch: 229 / 500, time cost: 42.40 sec, \n",
      "          Loss: [Train: 1.76469], [Test: 3.65710],\n",
      "          Effect: [ate-q], [train: 0.04725], [test: 0.04876]\n",
      "********************************************************************************\n",
      "epoch: 230 / 500, time cost: 42.38 sec, \n",
      "          Loss: [Train: 1.76353], [Test: 3.74065],\n",
      "          Effect: [ate-q], [train: -0.10468], [test: -0.10360]\n",
      "********************************************************************************\n",
      "epoch: 231 / 500, time cost: 44.46 sec, \n",
      "          Loss: [Train: 1.76212], [Test: 3.68681],\n",
      "          Effect: [ate-q], [train: 0.05831], [test: 0.05968]\n",
      "********************************************************************************\n",
      "epoch: 232 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.76235], [Test: 3.67545],\n",
      "          Effect: [ate-q], [train: 0.09829], [test: 0.10035]\n",
      "********************************************************************************\n",
      "epoch: 233 / 500, time cost: 42.35 sec, \n",
      "          Loss: [Train: 1.76128], [Test: 3.71835],\n",
      "          Effect: [ate-q], [train: -0.02442], [test: -0.02299]\n",
      "********************************************************************************\n",
      "epoch: 234 / 500, time cost: 42.36 sec, \n",
      "          Loss: [Train: 1.75972], [Test: 3.72255],\n",
      "          Effect: [ate-q], [train: -0.04007], [test: -0.03847]\n",
      "********************************************************************************\n",
      "epoch: 235 / 500, time cost: 40.95 sec, \n",
      "          Loss: [Train: 1.75901], [Test: 3.71693],\n",
      "          Effect: [ate-q], [train: 0.04551], [test: 0.04683]\n",
      "********************************************************************************\n",
      "epoch: 236 / 500, time cost: 41.11 sec, \n",
      "          Loss: [Train: 1.75709], [Test: 3.74911],\n",
      "          Effect: [ate-q], [train: 0.00328], [test: 0.00464]\n",
      "********************************************************************************\n",
      "epoch: 237 / 500, time cost: 41.11 sec, \n",
      "          Loss: [Train: 1.75709], [Test: 3.74043],\n",
      "          Effect: [ate-q], [train: 0.04130], [test: 0.04252]\n",
      "********************************************************************************\n",
      "epoch: 238 / 500, time cost: 41.11 sec, \n",
      "          Loss: [Train: 1.75366], [Test: 3.73258],\n",
      "          Effect: [ate-q], [train: 0.01873], [test: 0.02009]\n",
      "********************************************************************************\n",
      "epoch: 239 / 500, time cost: 41.03 sec, \n",
      "          Loss: [Train: 1.75274], [Test: 3.74217],\n",
      "          Effect: [ate-q], [train: 0.08677], [test: 0.08809]\n",
      "********************************************************************************\n",
      "epoch: 240 / 500, time cost: 40.93 sec, \n",
      "          Loss: [Train: 1.75295], [Test: 3.76670],\n",
      "          Effect: [ate-q], [train: 0.06619], [test: 0.06801]\n",
      "********************************************************************************\n",
      "epoch: 241 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 1.75030], [Test: 3.77612],\n",
      "          Effect: [ate-q], [train: 0.01574], [test: 0.01722]\n",
      "********************************************************************************\n",
      "epoch: 242 / 500, time cost: 41.98 sec, \n",
      "          Loss: [Train: 1.75094], [Test: 3.76767],\n",
      "          Effect: [ate-q], [train: 0.11883], [test: 0.12055]\n",
      "********************************************************************************\n",
      "epoch: 243 / 500, time cost: 42.26 sec, \n",
      "          Loss: [Train: 1.75037], [Test: 3.74304],\n",
      "          Effect: [ate-q], [train: 0.09098], [test: 0.09242]\n",
      "********************************************************************************\n",
      "epoch: 244 / 500, time cost: 42.20 sec, \n",
      "          Loss: [Train: 1.74770], [Test: 3.78889],\n",
      "          Effect: [ate-q], [train: 0.03027], [test: 0.03154]\n",
      "********************************************************************************\n",
      "epoch: 245 / 500, time cost: 43.97 sec, \n",
      "          Loss: [Train: 1.74611], [Test: 3.79800],\n",
      "          Effect: [ate-q], [train: 0.05519], [test: 0.05630]\n",
      "********************************************************************************\n",
      "epoch: 246 / 500, time cost: 41.79 sec, \n",
      "          Loss: [Train: 1.74602], [Test: 3.80646],\n",
      "          Effect: [ate-q], [train: 0.04951], [test: 0.05082]\n",
      "********************************************************************************\n",
      "epoch: 247 / 500, time cost: 41.95 sec, \n",
      "          Loss: [Train: 1.74426], [Test: 3.83389],\n",
      "          Effect: [ate-q], [train: 0.00683], [test: 0.00818]\n",
      "********************************************************************************\n",
      "epoch: 248 / 500, time cost: 41.56 sec, \n",
      "          Loss: [Train: 1.74394], [Test: 3.81716],\n",
      "          Effect: [ate-q], [train: 0.09113], [test: 0.09270]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 249 / 500, time cost: 42.09 sec, \n",
      "          Loss: [Train: 1.74118], [Test: 3.85600],\n",
      "          Effect: [ate-q], [train: -0.00156], [test: -0.00032]\n",
      "********************************************************************************\n",
      "epoch: 250 / 500, time cost: 41.84 sec, \n",
      "          Loss: [Train: 1.73976], [Test: 3.87380],\n",
      "          Effect: [ate-q], [train: -0.01374], [test: -0.01253]\n",
      "********************************************************************************\n",
      "epoch: 251 / 500, time cost: 41.94 sec, \n",
      "          Loss: [Train: 1.74074], [Test: 3.86773],\n",
      "          Effect: [ate-q], [train: 0.01853], [test: 0.01965]\n",
      "********************************************************************************\n",
      "epoch: 252 / 500, time cost: 42.02 sec, \n",
      "          Loss: [Train: 1.73891], [Test: 3.88019],\n",
      "          Effect: [ate-q], [train: 0.03267], [test: 0.03364]\n",
      "********************************************************************************\n",
      "epoch: 253 / 500, time cost: 42.09 sec, \n",
      "          Loss: [Train: 1.73673], [Test: 3.89262],\n",
      "          Effect: [ate-q], [train: -0.00157], [test: -0.00016]\n",
      "********************************************************************************\n",
      "epoch: 254 / 500, time cost: 42.28 sec, \n",
      "          Loss: [Train: 1.73694], [Test: 3.89614],\n",
      "          Effect: [ate-q], [train: 0.01585], [test: 0.01701]\n",
      "********************************************************************************\n",
      "epoch: 255 / 500, time cost: 42.03 sec, \n",
      "          Loss: [Train: 1.73457], [Test: 3.90300],\n",
      "          Effect: [ate-q], [train: -0.02700], [test: -0.02570]\n",
      "********************************************************************************\n",
      "epoch: 256 / 500, time cost: 41.94 sec, \n",
      "          Loss: [Train: 1.73479], [Test: 3.86361],\n",
      "          Effect: [ate-q], [train: 0.11474], [test: 0.11640]\n",
      "********************************************************************************\n",
      "epoch: 257 / 500, time cost: 41.94 sec, \n",
      "          Loss: [Train: 1.73291], [Test: 3.91897],\n",
      "          Effect: [ate-q], [train: 0.00526], [test: 0.00667]\n",
      "********************************************************************************\n",
      "epoch: 258 / 500, time cost: 41.94 sec, \n",
      "          Loss: [Train: 1.73130], [Test: 3.92038],\n",
      "          Effect: [ate-q], [train: 0.07838], [test: 0.07990]\n",
      "********************************************************************************\n",
      "epoch: 259 / 500, time cost: 44.22 sec, \n",
      "          Loss: [Train: 1.73177], [Test: 3.97174],\n",
      "          Effect: [ate-q], [train: -0.05297], [test: -0.05169]\n",
      "********************************************************************************\n",
      "epoch: 260 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.72987], [Test: 3.95278],\n",
      "          Effect: [ate-q], [train: -0.00973], [test: -0.00853]\n",
      "********************************************************************************\n",
      "epoch: 261 / 500, time cost: 42.19 sec, \n",
      "          Loss: [Train: 1.72812], [Test: 3.96155],\n",
      "          Effect: [ate-q], [train: -0.03072], [test: -0.02933]\n",
      "********************************************************************************\n",
      "epoch: 262 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.72758], [Test: 3.96039],\n",
      "          Effect: [ate-q], [train: 0.07118], [test: 0.07258]\n",
      "********************************************************************************\n",
      "epoch: 263 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.72778], [Test: 3.95487],\n",
      "          Effect: [ate-q], [train: 0.03281], [test: 0.03436]\n",
      "********************************************************************************\n",
      "epoch: 264 / 500, time cost: 42.39 sec, \n",
      "          Loss: [Train: 1.72461], [Test: 4.01368],\n",
      "          Effect: [ate-q], [train: -0.01234], [test: -0.01054]\n",
      "********************************************************************************\n",
      "epoch: 265 / 500, time cost: 42.29 sec, \n",
      "          Loss: [Train: 1.72289], [Test: 3.99359],\n",
      "          Effect: [ate-q], [train: 0.04431], [test: 0.04547]\n",
      "********************************************************************************\n",
      "epoch: 266 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.72309], [Test: 3.98507],\n",
      "          Effect: [ate-q], [train: 0.04938], [test: 0.05097]\n",
      "********************************************************************************\n",
      "epoch: 267 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.72269], [Test: 4.01912],\n",
      "          Effect: [ate-q], [train: 0.01530], [test: 0.01667]\n",
      "********************************************************************************\n",
      "epoch: 268 / 500, time cost: 42.22 sec, \n",
      "          Loss: [Train: 1.72089], [Test: 4.06521],\n",
      "          Effect: [ate-q], [train: 0.00961], [test: 0.01124]\n",
      "********************************************************************************\n",
      "epoch: 269 / 500, time cost: 42.26 sec, \n",
      "          Loss: [Train: 1.72103], [Test: 3.99095],\n",
      "          Effect: [ate-q], [train: 0.03681], [test: 0.03789]\n",
      "********************************************************************************\n",
      "epoch: 270 / 500, time cost: 42.30 sec, \n",
      "          Loss: [Train: 1.71834], [Test: 4.00741],\n",
      "          Effect: [ate-q], [train: 0.05172], [test: 0.05318]\n",
      "********************************************************************************\n",
      "epoch: 271 / 500, time cost: 41.86 sec, \n",
      "          Loss: [Train: 1.71827], [Test: 4.01734],\n",
      "          Effect: [ate-q], [train: 0.00196], [test: 0.00380]\n",
      "********************************************************************************\n",
      "epoch: 272 / 500, time cost: 42.01 sec, \n",
      "          Loss: [Train: 1.71595], [Test: 4.05355],\n",
      "          Effect: [ate-q], [train: 0.05515], [test: 0.05678]\n",
      "********************************************************************************\n",
      "epoch: 273 / 500, time cost: 44.41 sec, \n",
      "          Loss: [Train: 1.71519], [Test: 4.06135],\n",
      "          Effect: [ate-q], [train: 0.02166], [test: 0.02320]\n",
      "********************************************************************************\n",
      "epoch: 274 / 500, time cost: 41.77 sec, \n",
      "          Loss: [Train: 1.71447], [Test: 4.04657],\n",
      "          Effect: [ate-q], [train: 0.04966], [test: 0.05052]\n",
      "********************************************************************************\n",
      "epoch: 275 / 500, time cost: 42.00 sec, \n",
      "          Loss: [Train: 1.71318], [Test: 4.04781],\n",
      "          Effect: [ate-q], [train: 0.02589], [test: 0.02702]\n",
      "********************************************************************************\n",
      "epoch: 276 / 500, time cost: 41.76 sec, \n",
      "          Loss: [Train: 1.71319], [Test: 4.03498],\n",
      "          Effect: [ate-q], [train: 0.08961], [test: 0.09078]\n",
      "********************************************************************************\n",
      "epoch: 277 / 500, time cost: 42.17 sec, \n",
      "          Loss: [Train: 1.71303], [Test: 4.11134],\n",
      "          Effect: [ate-q], [train: -0.01323], [test: -0.01158]\n",
      "********************************************************************************\n",
      "epoch: 278 / 500, time cost: 41.84 sec, \n",
      "          Loss: [Train: 1.71143], [Test: 4.10669],\n",
      "          Effect: [ate-q], [train: 0.01808], [test: 0.01922]\n",
      "********************************************************************************\n",
      "epoch: 279 / 500, time cost: 42.28 sec, \n",
      "          Loss: [Train: 1.70896], [Test: 4.10471],\n",
      "          Effect: [ate-q], [train: -0.00903], [test: -0.00754]\n",
      "********************************************************************************\n",
      "epoch: 280 / 500, time cost: 40.96 sec, \n",
      "          Loss: [Train: 1.70841], [Test: 4.10503],\n",
      "          Effect: [ate-q], [train: 0.04545], [test: 0.04657]\n",
      "********************************************************************************\n",
      "epoch: 281 / 500, time cost: 43.43 sec, \n",
      "          Loss: [Train: 1.70715], [Test: 4.09445],\n",
      "          Effect: [ate-q], [train: 0.04747], [test: 0.04846]\n",
      "********************************************************************************\n",
      "epoch: 282 / 500, time cost: 41.12 sec, \n",
      "          Loss: [Train: 1.70662], [Test: 4.10775],\n",
      "          Effect: [ate-q], [train: 0.07569], [test: 0.07666]\n",
      "********************************************************************************\n",
      "epoch: 283 / 500, time cost: 42.07 sec, \n",
      "          Loss: [Train: 1.70419], [Test: 4.10420],\n",
      "          Effect: [ate-q], [train: 0.03845], [test: 0.03923]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 284 / 500, time cost: 41.08 sec, \n",
      "          Loss: [Train: 1.70505], [Test: 4.12259],\n",
      "          Effect: [ate-q], [train: 0.04465], [test: 0.04562]\n",
      "********************************************************************************\n",
      "epoch: 285 / 500, time cost: 41.29 sec, \n",
      "          Loss: [Train: 1.70175], [Test: 4.17460],\n",
      "          Effect: [ate-q], [train: 0.00430], [test: 0.00547]\n",
      "********************************************************************************\n",
      "epoch: 286 / 500, time cost: 42.03 sec, \n",
      "          Loss: [Train: 1.70229], [Test: 4.17304],\n",
      "          Effect: [ate-q], [train: 0.02956], [test: 0.03052]\n",
      "********************************************************************************\n",
      "epoch: 287 / 500, time cost: 42.56 sec, \n",
      "          Loss: [Train: 1.70098], [Test: 4.17464],\n",
      "          Effect: [ate-q], [train: 0.04812], [test: 0.04924]\n",
      "********************************************************************************\n",
      "epoch: 288 / 500, time cost: 40.69 sec, \n",
      "          Loss: [Train: 1.69992], [Test: 4.14211],\n",
      "          Effect: [ate-q], [train: 0.11395], [test: 0.11499]\n",
      "********************************************************************************\n",
      "epoch: 289 / 500, time cost: 40.62 sec, \n",
      "          Loss: [Train: 1.69908], [Test: 4.16303],\n",
      "          Effect: [ate-q], [train: 0.06687], [test: 0.06792]\n",
      "********************************************************************************\n",
      "epoch: 290 / 500, time cost: 40.71 sec, \n",
      "          Loss: [Train: 1.69795], [Test: 4.13856],\n",
      "          Effect: [ate-q], [train: 0.10102], [test: 0.10235]\n",
      "********************************************************************************\n",
      "epoch: 291 / 500, time cost: 42.40 sec, \n",
      "          Loss: [Train: 1.69672], [Test: 4.23028],\n",
      "          Effect: [ate-q], [train: 0.01378], [test: 0.01471]\n",
      "********************************************************************************\n",
      "epoch: 292 / 500, time cost: 42.33 sec, \n",
      "          Loss: [Train: 1.69378], [Test: 4.24808],\n",
      "          Effect: [ate-q], [train: -0.03690], [test: -0.03592]\n",
      "********************************************************************************\n",
      "epoch: 293 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.69408], [Test: 4.23761],\n",
      "          Effect: [ate-q], [train: 0.00274], [test: 0.00405]\n",
      "********************************************************************************\n",
      "epoch: 294 / 500, time cost: 42.23 sec, \n",
      "          Loss: [Train: 1.69352], [Test: 4.26108],\n",
      "          Effect: [ate-q], [train: -0.02272], [test: -0.02178]\n",
      "********************************************************************************\n",
      "epoch: 295 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.69355], [Test: 4.23335],\n",
      "          Effect: [ate-q], [train: 0.05901], [test: 0.06002]\n",
      "********************************************************************************\n",
      "epoch: 296 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.69080], [Test: 4.26087],\n",
      "          Effect: [ate-q], [train: 0.01057], [test: 0.01166]\n",
      "********************************************************************************\n",
      "epoch: 297 / 500, time cost: 42.20 sec, \n",
      "          Loss: [Train: 1.69012], [Test: 4.29736],\n",
      "          Effect: [ate-q], [train: 0.02301], [test: 0.02396]\n",
      "********************************************************************************\n",
      "epoch: 298 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 1.69233], [Test: 4.26893],\n",
      "          Effect: [ate-q], [train: 0.02035], [test: 0.02109]\n",
      "********************************************************************************\n",
      "epoch: 299 / 500, time cost: 42.05 sec, \n",
      "          Loss: [Train: 1.68818], [Test: 4.28238],\n",
      "          Effect: [ate-q], [train: 0.00721], [test: 0.00827]\n",
      "********************************************************************************\n",
      "epoch: 300 / 500, time cost: 42.18 sec, \n",
      "          Loss: [Train: 1.68885], [Test: 4.29958],\n",
      "          Effect: [ate-q], [train: 0.02813], [test: 0.02912]\n",
      "********************************************************************************\n",
      "epoch: 301 / 500, time cost: 44.04 sec, \n",
      "          Loss: [Train: 1.68628], [Test: 4.29380],\n",
      "          Effect: [ate-q], [train: 0.05153], [test: 0.05242]\n",
      "********************************************************************************\n",
      "epoch: 302 / 500, time cost: 42.23 sec, \n",
      "          Loss: [Train: 1.68528], [Test: 4.34383],\n",
      "          Effect: [ate-q], [train: -0.02561], [test: -0.02469]\n",
      "********************************************************************************\n",
      "epoch: 303 / 500, time cost: 42.23 sec, \n",
      "          Loss: [Train: 1.68513], [Test: 4.36487],\n",
      "          Effect: [ate-q], [train: -0.04174], [test: -0.04063]\n",
      "********************************************************************************\n",
      "epoch: 304 / 500, time cost: 42.01 sec, \n",
      "          Loss: [Train: 1.68475], [Test: 4.32783],\n",
      "          Effect: [ate-q], [train: 0.04871], [test: 0.04974]\n",
      "********************************************************************************\n",
      "epoch: 305 / 500, time cost: 41.92 sec, \n",
      "          Loss: [Train: 1.68304], [Test: 4.35780],\n",
      "          Effect: [ate-q], [train: -0.01725], [test: -0.01627]\n",
      "********************************************************************************\n",
      "epoch: 306 / 500, time cost: 41.93 sec, \n",
      "          Loss: [Train: 1.68350], [Test: 4.35787],\n",
      "          Effect: [ate-q], [train: 0.01983], [test: 0.02061]\n",
      "********************************************************************************\n",
      "epoch: 307 / 500, time cost: 41.92 sec, \n",
      "          Loss: [Train: 1.68150], [Test: 4.38882],\n",
      "          Effect: [ate-q], [train: -0.01369], [test: -0.01276]\n",
      "********************************************************************************\n",
      "epoch: 308 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.67932], [Test: 4.35589],\n",
      "          Effect: [ate-q], [train: 0.06442], [test: 0.06538]\n",
      "********************************************************************************\n",
      "epoch: 309 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.67975], [Test: 4.38631],\n",
      "          Effect: [ate-q], [train: -0.00460], [test: -0.00398]\n",
      "********************************************************************************\n",
      "epoch: 310 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.67772], [Test: 4.42486],\n",
      "          Effect: [ate-q], [train: 0.00625], [test: 0.00715]\n",
      "********************************************************************************\n",
      "epoch: 311 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.67789], [Test: 4.44073],\n",
      "          Effect: [ate-q], [train: -0.01383], [test: -0.01299]\n",
      "********************************************************************************\n",
      "epoch: 312 / 500, time cost: 42.09 sec, \n",
      "          Loss: [Train: 1.67591], [Test: 4.38846],\n",
      "          Effect: [ate-q], [train: 0.02200], [test: 0.02306]\n",
      "********************************************************************************\n",
      "epoch: 313 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.67493], [Test: 4.37817],\n",
      "          Effect: [ate-q], [train: 0.11585], [test: 0.11679]\n",
      "********************************************************************************\n",
      "epoch: 314 / 500, time cost: 42.44 sec, \n",
      "          Loss: [Train: 1.67400], [Test: 4.39673],\n",
      "          Effect: [ate-q], [train: 0.07593], [test: 0.07699]\n",
      "********************************************************************************\n",
      "epoch: 315 / 500, time cost: 44.56 sec, \n",
      "          Loss: [Train: 1.67335], [Test: 4.44109],\n",
      "          Effect: [ate-q], [train: 0.02346], [test: 0.02450]\n",
      "********************************************************************************\n",
      "epoch: 316 / 500, time cost: 42.40 sec, \n",
      "          Loss: [Train: 1.67188], [Test: 4.41753],\n",
      "          Effect: [ate-q], [train: 0.07087], [test: 0.07199]\n",
      "********************************************************************************\n",
      "epoch: 317 / 500, time cost: 42.34 sec, \n",
      "          Loss: [Train: 1.67167], [Test: 4.43438],\n",
      "          Effect: [ate-q], [train: 0.05081], [test: 0.05217]\n",
      "********************************************************************************\n",
      "epoch: 318 / 500, time cost: 42.20 sec, \n",
      "          Loss: [Train: 1.66964], [Test: 4.42647],\n",
      "          Effect: [ate-q], [train: 0.06816], [test: 0.06904]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 319 / 500, time cost: 41.79 sec, \n",
      "          Loss: [Train: 1.66803], [Test: 4.51164],\n",
      "          Effect: [ate-q], [train: 0.03003], [test: 0.03105]\n",
      "********************************************************************************\n",
      "epoch: 320 / 500, time cost: 41.94 sec, \n",
      "          Loss: [Train: 1.66958], [Test: 4.45997],\n",
      "          Effect: [ate-q], [train: 0.02767], [test: 0.02842]\n",
      "********************************************************************************\n",
      "epoch: 321 / 500, time cost: 41.90 sec, \n",
      "          Loss: [Train: 1.66843], [Test: 4.53215],\n",
      "          Effect: [ate-q], [train: -0.02623], [test: -0.02521]\n",
      "********************************************************************************\n",
      "epoch: 322 / 500, time cost: 40.66 sec, \n",
      "          Loss: [Train: 1.66737], [Test: 4.50224],\n",
      "          Effect: [ate-q], [train: -0.00474], [test: -0.00407]\n",
      "********************************************************************************\n",
      "epoch: 323 / 500, time cost: 40.80 sec, \n",
      "          Loss: [Train: 1.66541], [Test: 4.52696],\n",
      "          Effect: [ate-q], [train: -0.01073], [test: -0.01047]\n",
      "********************************************************************************\n",
      "epoch: 324 / 500, time cost: 41.18 sec, \n",
      "          Loss: [Train: 1.66451], [Test: 4.56250],\n",
      "          Effect: [ate-q], [train: 0.04280], [test: 0.04357]\n",
      "********************************************************************************\n",
      "epoch: 325 / 500, time cost: 41.19 sec, \n",
      "          Loss: [Train: 1.66414], [Test: 4.50389],\n",
      "          Effect: [ate-q], [train: 0.03661], [test: 0.03697]\n",
      "********************************************************************************\n",
      "epoch: 326 / 500, time cost: 41.09 sec, \n",
      "          Loss: [Train: 1.66264], [Test: 4.52972],\n",
      "          Effect: [ate-q], [train: 0.02884], [test: 0.02954]\n",
      "********************************************************************************\n",
      "epoch: 327 / 500, time cost: 41.10 sec, \n",
      "          Loss: [Train: 1.66243], [Test: 4.53840],\n",
      "          Effect: [ate-q], [train: 0.02377], [test: 0.02437]\n",
      "********************************************************************************\n",
      "epoch: 328 / 500, time cost: 41.07 sec, \n",
      "          Loss: [Train: 1.66160], [Test: 4.55565],\n",
      "          Effect: [ate-q], [train: 0.04877], [test: 0.04922]\n",
      "********************************************************************************\n",
      "epoch: 329 / 500, time cost: 43.04 sec, \n",
      "          Loss: [Train: 1.66043], [Test: 4.59984],\n",
      "          Effect: [ate-q], [train: -0.00935], [test: -0.00866]\n",
      "********************************************************************************\n",
      "epoch: 330 / 500, time cost: 41.72 sec, \n",
      "          Loss: [Train: 1.65956], [Test: 4.55189],\n",
      "          Effect: [ate-q], [train: 0.03533], [test: 0.03580]\n",
      "********************************************************************************\n",
      "epoch: 331 / 500, time cost: 41.10 sec, \n",
      "          Loss: [Train: 1.65789], [Test: 4.57096],\n",
      "          Effect: [ate-q], [train: 0.06254], [test: 0.06297]\n",
      "********************************************************************************\n",
      "epoch: 332 / 500, time cost: 41.73 sec, \n",
      "          Loss: [Train: 1.65792], [Test: 4.57038],\n",
      "          Effect: [ate-q], [train: 0.04376], [test: 0.04420]\n",
      "********************************************************************************\n",
      "epoch: 333 / 500, time cost: 41.05 sec, \n",
      "          Loss: [Train: 1.65542], [Test: 4.64168],\n",
      "          Effect: [ate-q], [train: -0.01950], [test: -0.01922]\n",
      "********************************************************************************\n",
      "epoch: 334 / 500, time cost: 41.22 sec, \n",
      "          Loss: [Train: 1.65601], [Test: 4.61298],\n",
      "          Effect: [ate-q], [train: 0.00226], [test: 0.00258]\n",
      "********************************************************************************\n",
      "epoch: 335 / 500, time cost: 41.11 sec, \n",
      "          Loss: [Train: 1.65699], [Test: 4.58235],\n",
      "          Effect: [ate-q], [train: 0.06126], [test: 0.06185]\n",
      "********************************************************************************\n",
      "epoch: 336 / 500, time cost: 40.90 sec, \n",
      "          Loss: [Train: 1.65312], [Test: 4.62711],\n",
      "          Effect: [ate-q], [train: 0.03470], [test: 0.03528]\n",
      "********************************************************************************\n",
      "epoch: 337 / 500, time cost: 40.90 sec, \n",
      "          Loss: [Train: 1.65414], [Test: 4.61963],\n",
      "          Effect: [ate-q], [train: 0.03362], [test: 0.03410]\n",
      "********************************************************************************\n",
      "epoch: 338 / 500, time cost: 41.66 sec, \n",
      "          Loss: [Train: 1.65032], [Test: 4.63280],\n",
      "          Effect: [ate-q], [train: 0.01664], [test: 0.01699]\n",
      "********************************************************************************\n",
      "epoch: 339 / 500, time cost: 40.63 sec, \n",
      "          Loss: [Train: 1.65204], [Test: 4.66118],\n",
      "          Effect: [ate-q], [train: -0.01990], [test: -0.01922]\n",
      "********************************************************************************\n",
      "epoch: 340 / 500, time cost: 40.95 sec, \n",
      "          Loss: [Train: 1.64992], [Test: 4.63106],\n",
      "          Effect: [ate-q], [train: 0.05399], [test: 0.05446]\n",
      "********************************************************************************\n",
      "epoch: 341 / 500, time cost: 41.15 sec, \n",
      "          Loss: [Train: 1.65021], [Test: 4.68694],\n",
      "          Effect: [ate-q], [train: 0.01672], [test: 0.01685]\n",
      "********************************************************************************\n",
      "epoch: 342 / 500, time cost: 40.84 sec, \n",
      "          Loss: [Train: 1.64870], [Test: 4.63009],\n",
      "          Effect: [ate-q], [train: 0.08351], [test: 0.08386]\n",
      "********************************************************************************\n",
      "epoch: 343 / 500, time cost: 42.82 sec, \n",
      "          Loss: [Train: 1.64776], [Test: 4.68363],\n",
      "          Effect: [ate-q], [train: 0.01980], [test: 0.02000]\n",
      "********************************************************************************\n",
      "epoch: 344 / 500, time cost: 40.76 sec, \n",
      "          Loss: [Train: 1.64736], [Test: 4.70117],\n",
      "          Effect: [ate-q], [train: -0.00278], [test: -0.00242]\n",
      "********************************************************************************\n",
      "epoch: 345 / 500, time cost: 40.69 sec, \n",
      "          Loss: [Train: 1.64501], [Test: 4.67989],\n",
      "          Effect: [ate-q], [train: 0.05954], [test: 0.05970]\n",
      "********************************************************************************\n",
      "epoch: 346 / 500, time cost: 40.87 sec, \n",
      "          Loss: [Train: 1.64654], [Test: 4.69219],\n",
      "          Effect: [ate-q], [train: 0.04892], [test: 0.04896]\n",
      "********************************************************************************\n",
      "epoch: 347 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.64519], [Test: 4.70739],\n",
      "          Effect: [ate-q], [train: -0.00562], [test: -0.00536]\n",
      "********************************************************************************\n",
      "epoch: 348 / 500, time cost: 42.25 sec, \n",
      "          Loss: [Train: 1.64313], [Test: 4.72981],\n",
      "          Effect: [ate-q], [train: 0.04634], [test: 0.04639]\n",
      "********************************************************************************\n",
      "epoch: 349 / 500, time cost: 41.84 sec, \n",
      "          Loss: [Train: 1.64373], [Test: 4.72100],\n",
      "          Effect: [ate-q], [train: 0.03755], [test: 0.03776]\n",
      "********************************************************************************\n",
      "epoch: 350 / 500, time cost: 42.32 sec, \n",
      "          Loss: [Train: 1.64099], [Test: 4.70711],\n",
      "          Effect: [ate-q], [train: 0.03388], [test: 0.03421]\n",
      "********************************************************************************\n",
      "epoch: 351 / 500, time cost: 42.06 sec, \n",
      "          Loss: [Train: 1.64128], [Test: 4.78378],\n",
      "          Effect: [ate-q], [train: -0.02605], [test: -0.02595]\n",
      "********************************************************************************\n",
      "epoch: 352 / 500, time cost: 42.39 sec, \n",
      "          Loss: [Train: 1.64071], [Test: 4.73485],\n",
      "          Effect: [ate-q], [train: 0.01280], [test: 0.01279]\n",
      "********************************************************************************\n",
      "epoch: 353 / 500, time cost: 42.13 sec, \n",
      "          Loss: [Train: 1.63856], [Test: 4.75227],\n",
      "          Effect: [ate-q], [train: 0.03645], [test: 0.03699]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 354 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.63959], [Test: 4.77922],\n",
      "          Effect: [ate-q], [train: 0.02009], [test: 0.02066]\n",
      "********************************************************************************\n",
      "epoch: 355 / 500, time cost: 42.30 sec, \n",
      "          Loss: [Train: 1.63847], [Test: 4.79084],\n",
      "          Effect: [ate-q], [train: 0.02484], [test: 0.02476]\n",
      "********************************************************************************\n",
      "epoch: 356 / 500, time cost: 42.50 sec, \n",
      "          Loss: [Train: 1.63720], [Test: 4.77853],\n",
      "          Effect: [ate-q], [train: 0.00560], [test: 0.00574]\n",
      "********************************************************************************\n",
      "epoch: 357 / 500, time cost: 44.66 sec, \n",
      "          Loss: [Train: 1.63705], [Test: 4.78639],\n",
      "          Effect: [ate-q], [train: 0.03591], [test: 0.03612]\n",
      "********************************************************************************\n",
      "epoch: 358 / 500, time cost: 42.40 sec, \n",
      "          Loss: [Train: 1.63551], [Test: 4.80825],\n",
      "          Effect: [ate-q], [train: 0.01249], [test: 0.01236]\n",
      "********************************************************************************\n",
      "epoch: 359 / 500, time cost: 42.29 sec, \n",
      "          Loss: [Train: 1.63513], [Test: 4.75423],\n",
      "          Effect: [ate-q], [train: 0.04024], [test: 0.04073]\n",
      "********************************************************************************\n",
      "epoch: 360 / 500, time cost: 42.21 sec, \n",
      "          Loss: [Train: 1.63515], [Test: 4.77603],\n",
      "          Effect: [ate-q], [train: 0.10661], [test: 0.10706]\n",
      "********************************************************************************\n",
      "epoch: 361 / 500, time cost: 42.47 sec, \n",
      "          Loss: [Train: 1.63387], [Test: 4.82358],\n",
      "          Effect: [ate-q], [train: 0.00411], [test: 0.00423]\n",
      "********************************************************************************\n",
      "epoch: 362 / 500, time cost: 42.04 sec, \n",
      "          Loss: [Train: 1.63309], [Test: 4.89028],\n",
      "          Effect: [ate-q], [train: 0.00063], [test: 0.00086]\n",
      "********************************************************************************\n",
      "epoch: 363 / 500, time cost: 42.33 sec, \n",
      "          Loss: [Train: 1.63155], [Test: 4.89458],\n",
      "          Effect: [ate-q], [train: -0.01822], [test: -0.01805]\n",
      "********************************************************************************\n",
      "epoch: 364 / 500, time cost: 42.25 sec, \n",
      "          Loss: [Train: 1.63123], [Test: 4.86904],\n",
      "          Effect: [ate-q], [train: -0.01392], [test: -0.01377]\n",
      "********************************************************************************\n",
      "epoch: 365 / 500, time cost: 40.93 sec, \n",
      "          Loss: [Train: 1.63084], [Test: 4.84488],\n",
      "          Effect: [ate-q], [train: 0.01252], [test: 0.01255]\n",
      "********************************************************************************\n",
      "epoch: 366 / 500, time cost: 40.93 sec, \n",
      "          Loss: [Train: 1.62976], [Test: 4.86222],\n",
      "          Effect: [ate-q], [train: -0.00214], [test: -0.00217]\n",
      "********************************************************************************\n",
      "epoch: 367 / 500, time cost: 41.84 sec, \n",
      "          Loss: [Train: 1.62850], [Test: 4.82368],\n",
      "          Effect: [ate-q], [train: 0.05650], [test: 0.05636]\n",
      "********************************************************************************\n",
      "epoch: 368 / 500, time cost: 40.79 sec, \n",
      "          Loss: [Train: 1.62813], [Test: 4.91314],\n",
      "          Effect: [ate-q], [train: 0.01095], [test: 0.01064]\n",
      "********************************************************************************\n",
      "epoch: 369 / 500, time cost: 40.76 sec, \n",
      "          Loss: [Train: 1.62718], [Test: 4.84965],\n",
      "          Effect: [ate-q], [train: 0.03658], [test: 0.03624]\n",
      "********************************************************************************\n",
      "epoch: 370 / 500, time cost: 40.81 sec, \n",
      "          Loss: [Train: 1.62585], [Test: 4.88939],\n",
      "          Effect: [ate-q], [train: 0.05231], [test: 0.05236]\n",
      "********************************************************************************\n",
      "epoch: 371 / 500, time cost: 43.97 sec, \n",
      "          Loss: [Train: 1.62741], [Test: 4.88646],\n",
      "          Effect: [ate-q], [train: 0.02493], [test: 0.02502]\n",
      "********************************************************************************\n",
      "epoch: 372 / 500, time cost: 41.13 sec, \n",
      "          Loss: [Train: 1.62443], [Test: 4.92230],\n",
      "          Effect: [ate-q], [train: 0.03022], [test: 0.03018]\n",
      "********************************************************************************\n",
      "epoch: 373 / 500, time cost: 41.45 sec, \n",
      "          Loss: [Train: 1.62417], [Test: 4.90402],\n",
      "          Effect: [ate-q], [train: 0.02258], [test: 0.02266]\n",
      "********************************************************************************\n",
      "epoch: 374 / 500, time cost: 41.48 sec, \n",
      "          Loss: [Train: 1.62449], [Test: 4.90980],\n",
      "          Effect: [ate-q], [train: 0.01866], [test: 0.01899]\n",
      "********************************************************************************\n",
      "epoch: 375 / 500, time cost: 40.88 sec, \n",
      "          Loss: [Train: 1.62373], [Test: 4.96524],\n",
      "          Effect: [ate-q], [train: -0.01450], [test: -0.01454]\n",
      "********************************************************************************\n",
      "epoch: 376 / 500, time cost: 41.02 sec, \n",
      "          Loss: [Train: 1.62246], [Test: 4.88392],\n",
      "          Effect: [ate-q], [train: 0.05337], [test: 0.05351]\n",
      "********************************************************************************\n",
      "epoch: 377 / 500, time cost: 41.04 sec, \n",
      "          Loss: [Train: 1.62080], [Test: 4.90805],\n",
      "          Effect: [ate-q], [train: 0.05789], [test: 0.05772]\n",
      "********************************************************************************\n",
      "epoch: 378 / 500, time cost: 40.98 sec, \n",
      "          Loss: [Train: 1.62173], [Test: 4.90687],\n",
      "          Effect: [ate-q], [train: 0.05401], [test: 0.05388]\n",
      "********************************************************************************\n",
      "epoch: 379 / 500, time cost: 41.17 sec, \n",
      "          Loss: [Train: 1.62097], [Test: 5.02515],\n",
      "          Effect: [ate-q], [train: -0.02862], [test: -0.02886]\n",
      "********************************************************************************\n",
      "epoch: 380 / 500, time cost: 40.90 sec, \n",
      "          Loss: [Train: 1.61892], [Test: 4.91047],\n",
      "          Effect: [ate-q], [train: 0.03298], [test: 0.03320]\n",
      "********************************************************************************\n",
      "epoch: 381 / 500, time cost: 40.95 sec, \n",
      "          Loss: [Train: 1.61820], [Test: 4.95264],\n",
      "          Effect: [ate-q], [train: 0.05234], [test: 0.05219]\n",
      "********************************************************************************\n",
      "epoch: 382 / 500, time cost: 41.25 sec, \n",
      "          Loss: [Train: 1.61717], [Test: 4.98159],\n",
      "          Effect: [ate-q], [train: 0.02129], [test: 0.02128]\n",
      "********************************************************************************\n",
      "epoch: 383 / 500, time cost: 41.87 sec, \n",
      "          Loss: [Train: 1.61729], [Test: 4.98772],\n",
      "          Effect: [ate-q], [train: 0.03352], [test: 0.03334]\n",
      "********************************************************************************\n",
      "epoch: 384 / 500, time cost: 40.54 sec, \n",
      "          Loss: [Train: 1.61681], [Test: 4.99220],\n",
      "          Effect: [ate-q], [train: 0.03854], [test: 0.03862]\n",
      "********************************************************************************\n",
      "epoch: 385 / 500, time cost: 42.60 sec, \n",
      "          Loss: [Train: 1.61616], [Test: 5.02184],\n",
      "          Effect: [ate-q], [train: 0.02324], [test: 0.02291]\n",
      "********************************************************************************\n",
      "epoch: 386 / 500, time cost: 40.56 sec, \n",
      "          Loss: [Train: 1.61553], [Test: 4.97128],\n",
      "          Effect: [ate-q], [train: 0.03540], [test: 0.03482]\n",
      "********************************************************************************\n",
      "epoch: 387 / 500, time cost: 42.06 sec, \n",
      "          Loss: [Train: 1.61505], [Test: 4.98684],\n",
      "          Effect: [ate-q], [train: 0.03293], [test: 0.03303]\n",
      "********************************************************************************\n",
      "epoch: 388 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.61392], [Test: 5.00293],\n",
      "          Effect: [ate-q], [train: 0.02347], [test: 0.02309]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 389 / 500, time cost: 42.16 sec, \n",
      "          Loss: [Train: 1.61384], [Test: 5.05836],\n",
      "          Effect: [ate-q], [train: 0.01257], [test: 0.01229]\n",
      "********************************************************************************\n",
      "epoch: 390 / 500, time cost: 39.52 sec, \n",
      "          Loss: [Train: 1.61267], [Test: 5.01512],\n",
      "          Effect: [ate-q], [train: 0.07246], [test: 0.07217]\n",
      "********************************************************************************\n",
      "epoch: 391 / 500, time cost: 40.82 sec, \n",
      "          Loss: [Train: 1.61112], [Test: 5.00518],\n",
      "          Effect: [ate-q], [train: 0.03304], [test: 0.03261]\n",
      "********************************************************************************\n",
      "epoch: 392 / 500, time cost: 42.05 sec, \n",
      "          Loss: [Train: 1.61140], [Test: 4.98830],\n",
      "          Effect: [ate-q], [train: 0.05313], [test: 0.05288]\n",
      "********************************************************************************\n",
      "epoch: 393 / 500, time cost: 42.04 sec, \n",
      "          Loss: [Train: 1.61056], [Test: 5.05874],\n",
      "          Effect: [ate-q], [train: 0.01874], [test: 0.01827]\n",
      "********************************************************************************\n",
      "epoch: 394 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.60993], [Test: 5.04173],\n",
      "          Effect: [ate-q], [train: 0.05234], [test: 0.05177]\n",
      "********************************************************************************\n",
      "epoch: 395 / 500, time cost: 42.01 sec, \n",
      "          Loss: [Train: 1.60890], [Test: 5.07610],\n",
      "          Effect: [ate-q], [train: 0.05444], [test: 0.05422]\n",
      "********************************************************************************\n",
      "epoch: 396 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.60987], [Test: 5.06483],\n",
      "          Effect: [ate-q], [train: 0.02475], [test: 0.02441]\n",
      "********************************************************************************\n",
      "epoch: 397 / 500, time cost: 42.09 sec, \n",
      "          Loss: [Train: 1.60878], [Test: 5.03702],\n",
      "          Effect: [ate-q], [train: 0.04865], [test: 0.04855]\n",
      "********************************************************************************\n",
      "epoch: 398 / 500, time cost: 42.22 sec, \n",
      "          Loss: [Train: 1.60666], [Test: 5.07802],\n",
      "          Effect: [ate-q], [train: 0.03000], [test: 0.02951]\n",
      "********************************************************************************\n",
      "epoch: 399 / 500, time cost: 43.83 sec, \n",
      "          Loss: [Train: 1.60737], [Test: 5.01806],\n",
      "          Effect: [ate-q], [train: 0.08892], [test: 0.08881]\n",
      "********************************************************************************\n",
      "epoch: 400 / 500, time cost: 41.68 sec, \n",
      "          Loss: [Train: 1.60535], [Test: 5.09867],\n",
      "          Effect: [ate-q], [train: -0.00579], [test: -0.00639]\n",
      "********************************************************************************\n",
      "epoch: 401 / 500, time cost: 41.89 sec, \n",
      "          Loss: [Train: 1.60546], [Test: 5.12103],\n",
      "          Effect: [ate-q], [train: 0.01011], [test: 0.00968]\n",
      "********************************************************************************\n",
      "epoch: 402 / 500, time cost: 41.95 sec, \n",
      "          Loss: [Train: 1.60418], [Test: 5.10590],\n",
      "          Effect: [ate-q], [train: 0.04636], [test: 0.04593]\n",
      "********************************************************************************\n",
      "epoch: 403 / 500, time cost: 42.59 sec, \n",
      "          Loss: [Train: 1.60442], [Test: 5.10773],\n",
      "          Effect: [ate-q], [train: 0.03491], [test: 0.03455]\n",
      "********************************************************************************\n",
      "epoch: 404 / 500, time cost: 41.97 sec, \n",
      "          Loss: [Train: 1.60439], [Test: 5.11395],\n",
      "          Effect: [ate-q], [train: -0.00583], [test: -0.00645]\n",
      "********************************************************************************\n",
      "epoch: 405 / 500, time cost: 42.12 sec, \n",
      "          Loss: [Train: 1.60259], [Test: 5.10765],\n",
      "          Effect: [ate-q], [train: 0.01540], [test: 0.01517]\n",
      "********************************************************************************\n",
      "epoch: 406 / 500, time cost: 42.22 sec, \n",
      "          Loss: [Train: 1.60248], [Test: 5.11744],\n",
      "          Effect: [ate-q], [train: 0.02417], [test: 0.02368]\n",
      "********************************************************************************\n",
      "epoch: 407 / 500, time cost: 41.62 sec, \n",
      "          Loss: [Train: 1.60171], [Test: 5.14339],\n",
      "          Effect: [ate-q], [train: 0.01581], [test: 0.01554]\n",
      "********************************************************************************\n",
      "epoch: 408 / 500, time cost: 41.63 sec, \n",
      "          Loss: [Train: 1.60174], [Test: 5.12162],\n",
      "          Effect: [ate-q], [train: 0.05554], [test: 0.05540]\n",
      "********************************************************************************\n",
      "epoch: 409 / 500, time cost: 42.13 sec, \n",
      "          Loss: [Train: 1.60112], [Test: 5.11963],\n",
      "          Effect: [ate-q], [train: 0.08761], [test: 0.08708]\n",
      "********************************************************************************\n",
      "epoch: 410 / 500, time cost: 41.84 sec, \n",
      "          Loss: [Train: 1.60112], [Test: 5.16064],\n",
      "          Effect: [ate-q], [train: -0.00652], [test: -0.00718]\n",
      "********************************************************************************\n",
      "epoch: 411 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.59956], [Test: 5.16547],\n",
      "          Effect: [ate-q], [train: 0.04752], [test: 0.04701]\n",
      "********************************************************************************\n",
      "epoch: 412 / 500, time cost: 42.38 sec, \n",
      "          Loss: [Train: 1.59946], [Test: 5.19271],\n",
      "          Effect: [ate-q], [train: -0.00710], [test: -0.00765]\n",
      "********************************************************************************\n",
      "epoch: 413 / 500, time cost: 43.24 sec, \n",
      "          Loss: [Train: 1.59919], [Test: 5.18570],\n",
      "          Effect: [ate-q], [train: 0.03630], [test: 0.03573]\n",
      "********************************************************************************\n",
      "epoch: 414 / 500, time cost: 40.98 sec, \n",
      "          Loss: [Train: 1.59830], [Test: 5.23242],\n",
      "          Effect: [ate-q], [train: -0.00885], [test: -0.00982]\n",
      "********************************************************************************\n",
      "epoch: 415 / 500, time cost: 41.06 sec, \n",
      "          Loss: [Train: 1.59819], [Test: 5.18283],\n",
      "          Effect: [ate-q], [train: 0.01678], [test: 0.01617]\n",
      "********************************************************************************\n",
      "epoch: 416 / 500, time cost: 41.53 sec, \n",
      "          Loss: [Train: 1.59684], [Test: 5.16797],\n",
      "          Effect: [ate-q], [train: 0.02316], [test: 0.02275]\n",
      "********************************************************************************\n",
      "epoch: 417 / 500, time cost: 40.83 sec, \n",
      "          Loss: [Train: 1.59673], [Test: 5.15595],\n",
      "          Effect: [ate-q], [train: 0.02522], [test: 0.02451]\n",
      "********************************************************************************\n",
      "epoch: 418 / 500, time cost: 40.93 sec, \n",
      "          Loss: [Train: 1.59558], [Test: 5.21504],\n",
      "          Effect: [ate-q], [train: -0.00100], [test: -0.00208]\n",
      "********************************************************************************\n",
      "epoch: 419 / 500, time cost: 41.03 sec, \n",
      "          Loss: [Train: 1.59552], [Test: 5.21039],\n",
      "          Effect: [ate-q], [train: 0.00440], [test: 0.00396]\n",
      "********************************************************************************\n",
      "epoch: 420 / 500, time cost: 40.83 sec, \n",
      "          Loss: [Train: 1.59565], [Test: 5.19715],\n",
      "          Effect: [ate-q], [train: 0.03713], [test: 0.03672]\n",
      "********************************************************************************\n",
      "epoch: 421 / 500, time cost: 40.55 sec, \n",
      "          Loss: [Train: 1.59445], [Test: 5.20546],\n",
      "          Effect: [ate-q], [train: 0.03128], [test: 0.03066]\n",
      "********************************************************************************\n",
      "epoch: 422 / 500, time cost: 40.73 sec, \n",
      "          Loss: [Train: 1.59461], [Test: 5.24396],\n",
      "          Effect: [ate-q], [train: -0.00499], [test: -0.00590]\n",
      "********************************************************************************\n",
      "epoch: 423 / 500, time cost: 42.05 sec, \n",
      "          Loss: [Train: 1.59390], [Test: 5.19079],\n",
      "          Effect: [ate-q], [train: 0.02401], [test: 0.02310]\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 424 / 500, time cost: 40.99 sec, \n",
      "          Loss: [Train: 1.59274], [Test: 5.16674],\n",
      "          Effect: [ate-q], [train: 0.07979], [test: 0.07935]\n",
      "********************************************************************************\n",
      "epoch: 425 / 500, time cost: 41.20 sec, \n",
      "          Loss: [Train: 1.59273], [Test: 5.24404],\n",
      "          Effect: [ate-q], [train: -0.00649], [test: -0.00696]\n",
      "********************************************************************************\n",
      "epoch: 426 / 500, time cost: 41.26 sec, \n",
      "          Loss: [Train: 1.59141], [Test: 5.24622],\n",
      "          Effect: [ate-q], [train: 0.02168], [test: 0.02081]\n",
      "********************************************************************************\n",
      "epoch: 427 / 500, time cost: 43.74 sec, \n",
      "          Loss: [Train: 1.59213], [Test: 5.20829],\n",
      "          Effect: [ate-q], [train: 0.04631], [test: 0.04535]\n",
      "********************************************************************************\n",
      "epoch: 428 / 500, time cost: 42.15 sec, \n",
      "          Loss: [Train: 1.59042], [Test: 5.21737],\n",
      "          Effect: [ate-q], [train: 0.01399], [test: 0.01330]\n",
      "********************************************************************************\n",
      "epoch: 429 / 500, time cost: 42.35 sec, \n",
      "          Loss: [Train: 1.59107], [Test: 5.23604],\n",
      "          Effect: [ate-q], [train: 0.03436], [test: 0.03364]\n",
      "********************************************************************************\n",
      "epoch: 430 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.59069], [Test: 5.28674],\n",
      "          Effect: [ate-q], [train: 0.00344], [test: 0.00243]\n",
      "********************************************************************************\n",
      "epoch: 431 / 500, time cost: 40.19 sec, \n",
      "          Loss: [Train: 1.58964], [Test: 5.25039],\n",
      "          Effect: [ate-q], [train: 0.02330], [test: 0.02241]\n",
      "********************************************************************************\n",
      "epoch: 432 / 500, time cost: 42.14 sec, \n",
      "          Loss: [Train: 1.58908], [Test: 5.25386],\n",
      "          Effect: [ate-q], [train: 0.04666], [test: 0.04607]\n",
      "********************************************************************************\n",
      "epoch: 433 / 500, time cost: 41.79 sec, \n",
      "          Loss: [Train: 1.58844], [Test: 5.25793],\n",
      "          Effect: [ate-q], [train: 0.01918], [test: 0.01834]\n",
      "********************************************************************************\n",
      "epoch: 434 / 500, time cost: 41.96 sec, \n",
      "          Loss: [Train: 1.58771], [Test: 5.30809],\n",
      "          Effect: [ate-q], [train: 0.02237], [test: 0.02165]\n",
      "********************************************************************************\n",
      "epoch: 435 / 500, time cost: 41.87 sec, \n",
      "          Loss: [Train: 1.58830], [Test: 5.32058],\n",
      "          Effect: [ate-q], [train: -0.01040], [test: -0.01108]\n",
      "********************************************************************************\n",
      "epoch: 436 / 500, time cost: 42.04 sec, \n",
      "          Loss: [Train: 1.58801], [Test: 5.26385],\n",
      "          Effect: [ate-q], [train: 0.02158], [test: 0.02074]\n",
      "********************************************************************************\n",
      "epoch: 437 / 500, time cost: 42.24 sec, \n",
      "          Loss: [Train: 1.58733], [Test: 5.29934],\n",
      "          Effect: [ate-q], [train: 0.04845], [test: 0.04779]\n",
      "********************************************************************************\n",
      "epoch: 438 / 500, time cost: 42.09 sec, \n",
      "          Loss: [Train: 1.58642], [Test: 5.30007],\n",
      "          Effect: [ate-q], [train: 0.03438], [test: 0.03371]\n",
      "********************************************************************************\n",
      "epoch: 439 / 500, time cost: 42.31 sec, \n",
      "          Loss: [Train: 1.58550], [Test: 5.28240],\n",
      "          Effect: [ate-q], [train: 0.05398], [test: 0.05318]\n",
      "********************************************************************************\n",
      "epoch: 440 / 500, time cost: 42.13 sec, \n",
      "          Loss: [Train: 1.58518], [Test: 5.33406],\n",
      "          Effect: [ate-q], [train: 0.01288], [test: 0.01253]\n",
      "********************************************************************************\n",
      "epoch: 441 / 500, time cost: 44.26 sec, \n",
      "          Loss: [Train: 1.58520], [Test: 5.32702],\n",
      "          Effect: [ate-q], [train: 0.04192], [test: 0.04117]\n",
      "********************************************************************************\n",
      "epoch: 442 / 500, time cost: 42.30 sec, \n",
      "          Loss: [Train: 1.58406], [Test: 5.31797],\n",
      "          Effect: [ate-q], [train: 0.04551], [test: 0.04467]\n",
      "********************************************************************************\n",
      "epoch: 443 / 500, time cost: 42.25 sec, \n",
      "          Loss: [Train: 1.58357], [Test: 5.34015],\n",
      "          Effect: [ate-q], [train: 0.03012], [test: 0.02945]\n",
      "********************************************************************************\n",
      "epoch: 444 / 500, time cost: 42.23 sec, \n",
      "          Loss: [Train: 1.58458], [Test: 5.30081],\n",
      "          Effect: [ate-q], [train: 0.02043], [test: 0.01983]\n",
      "********************************************************************************\n",
      "epoch: 445 / 500, time cost: 41.96 sec, \n",
      "          Loss: [Train: 1.58388], [Test: 5.32977],\n",
      "          Effect: [ate-q], [train: 0.03268], [test: 0.03157]\n",
      "********************************************************************************\n",
      "epoch: 446 / 500, time cost: 42.63 sec, \n",
      "          Loss: [Train: 1.58248], [Test: 5.34412],\n",
      "          Effect: [ate-q], [train: 0.00569], [test: 0.00518]\n",
      "********************************************************************************\n",
      "epoch: 447 / 500, time cost: 42.05 sec, \n",
      "          Loss: [Train: 1.58214], [Test: 5.30597],\n",
      "          Effect: [ate-q], [train: 0.04722], [test: 0.04645]\n",
      "********************************************************************************\n",
      "epoch: 448 / 500, time cost: 42.05 sec, \n",
      "          Loss: [Train: 1.58217], [Test: 5.33977],\n",
      "          Effect: [ate-q], [train: 0.01629], [test: 0.01520]\n",
      "********************************************************************************\n",
      "epoch: 449 / 500, time cost: 41.66 sec, \n",
      "          Loss: [Train: 1.58126], [Test: 5.33653],\n",
      "          Effect: [ate-q], [train: 0.00631], [test: 0.00562]\n",
      "********************************************************************************\n",
      "epoch: 450 / 500, time cost: 42.01 sec, \n",
      "          Loss: [Train: 1.58158], [Test: 5.30828],\n",
      "          Effect: [ate-q], [train: 0.07149], [test: 0.07090]\n",
      "********************************************************************************\n",
      "epoch: 451 / 500, time cost: 41.92 sec, \n",
      "          Loss: [Train: 1.58099], [Test: 5.32758],\n",
      "          Effect: [ate-q], [train: 0.05094], [test: 0.05000]\n",
      "********************************************************************************\n",
      "epoch: 452 / 500, time cost: 42.20 sec, \n",
      "          Loss: [Train: 1.58022], [Test: 5.33868],\n",
      "          Effect: [ate-q], [train: 0.04285], [test: 0.04213]\n",
      "********************************************************************************\n"
     ]
    }
   ],
=======
      "epoch: 1 / 50, time cost: 47.09 sec, \n",
      "          Loss: [Train: 2.43431], [Test: 2.43865],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.04839], [test: 0.04839]\n",
      "********************************************************************************\n",
      "epoch: 2 / 50, time cost: 45.36 sec, \n",
      "          Loss: [Train: 2.43400], [Test: 2.43771],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.04880], [test: 0.04880]\n",
      "********************************************************************************\n",
      "epoch: 3 / 50, time cost: 47.05 sec, \n",
      "          Loss: [Train: 2.43320], [Test: 2.43692],\n",
      "          Accuracy: [prop score:  0.93655], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.04985], [test: 0.04985]\n",
      "********************************************************************************\n",
      "epoch: 4 / 50, time cost: 45.39 sec, \n",
      "          Loss: [Train: 2.43220], [Test: 2.43596],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.05182], [test: 0.05182]\n",
      "********************************************************************************\n",
      "epoch: 5 / 50, time cost: 48.47 sec, \n",
      "          Loss: [Train: 2.43104], [Test: 2.43470],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.05523], [test: 0.05523]\n",
      "********************************************************************************\n",
      "epoch: 6 / 50, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.42984], [Test: 2.43366],\n",
      "          Accuracy: [prop score:  0.93648], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.05958], [test: 0.05958]\n",
      "********************************************************************************\n",
      "epoch: 7 / 50, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.42873], [Test: 2.43262],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.06469], [test: 0.06468]\n",
      "********************************************************************************\n",
      "epoch: 8 / 50, time cost: 46.40 sec, \n",
      "          Loss: [Train: 2.42767], [Test: 2.43145],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.07072], [test: 0.07072]\n",
      "********************************************************************************\n",
      "epoch: 9 / 50, time cost: 48.51 sec, \n",
      "          Loss: [Train: 2.42708], [Test: 2.43098],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.07603], [test: 0.07603]\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "rs_loss, rq1_loss, rq0_loss = [0.] * 3\n",
    "\n",
    "train_loss_hist, test_loss_hist, est_effect = [], [], []\n",
    "for e in range(1, epoch + 1):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        \n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = create_vocab(merged=True, uni_diag=True)\n",
    "# tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "# alpha = 0.25\n",
    "# beta = 5.\n",
    "# c = 0.2\n",
    "# i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "#                              alpha=alpha, beta=beta, c=c, i=i, \n",
    "#                              group=list(range(1)), max_length=512, min_length=10,\n",
    "#                              truncate_method='first', device=device, seed=1)\n",
    "\n",
    "# print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "# start = time.time()\n",
    "# testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "#                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "#                             group=[9], max_length=512, min_length=10,\n",
    "#                             truncate_method='first', device=device)\n",
    "\n",
    "# print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "# test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "# print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "# print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# # trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "# model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "# token_embed = model.get_input_embeddings()\n",
    "# model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=hidden_size, prop_is_logit=True).to(device)\n",
    "\n",
    "# pos_portion = trainset.treatment.mean()\n",
    "# pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# epoch_iter = len(train_loader)\n",
    "# total_steps = epoch * epoch_iter\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 2, num_training_steps=total_steps)\n",
    "\n",
    "# q_loss = nn.BCELoss()\n",
    "# # prop_score_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# # Please specify the effect and estimation we want to use here.\n",
    "# effect = 'ate'\n",
    "# estimation = 'q'\n",
    "\n",
    "# effect = effect.lower()\n",
    "# estimation = estimation.lower()\n",
    "# assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "# assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 4ac631a77b28de01a83a4aca96454a5e1990a54a
   "source": [
    "# rs_loss, rq1_loss, rq0_loss = [0.] * 3\n",
    "\n",
    "# train_loss_hist, test_loss_hist, est_effect = [], [], []\n",
    "# for e in range(1, epoch + 1):\n",
    "#     model.train()\n",
    "#     start = time.time()\n",
    "#     run_loss = 0.\n",
    "#     for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "#         loss = prop_score_loss(prop_score, treatment)\n",
    "#         if len(response[treatment == 1]) > 0:\n",
    "#             loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "#         if len(response[treatment == 0]) > 0:\n",
    "#             loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "#         loss.backward()\n",
    "        \n",
    "# #         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         run_loss += loss.item()\n",
    "        \n",
    "#     run_idx = idx\n",
    "\n",
    "#     # Evaluation.\n",
    "#     train_loss = run_loss / (run_idx + 1)\n",
    "#     train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "#     test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "#     test_loss = q1_loss_test + q0_loss_test\n",
    "#     test_loss += g_loss_test\n",
    "    \n",
    "#     train_loss_hist.append(train_loss)\n",
    "#     test_loss_hist.append(test_loss)\n",
    "#     est_effect.append(test_effect)\n",
    "#     print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "#           Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "#           Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "#           Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "#     print('*'* 80)\n",
    "#     start = time.time()\n",
    "#     run_loss = 0.\n",
    "\n",
    "# print('Finish training...')\n",
    "\n",
    "# # With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real = true_casual_effect(test_loader)\n",
    "# unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "# show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = create_vocab(merged=True, uni_diag=True)\n",
    "# tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "# alpha = 0.75\n",
    "# beta = 25.\n",
    "# c = 0.2\n",
    "# i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "#                              alpha=alpha, beta=beta, c=c, i=i, \n",
    "#                              group=list(range(1)), max_length=512, min_length=10,\n",
    "#                              truncate_method='first', device=device, seed=1)\n",
    "\n",
    "# print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "# start = time.time()\n",
    "# testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "#                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "#                             group=[9], max_length=512, min_length=10,\n",
    "#                             truncate_method='first', device=device)\n",
    "\n",
    "# print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "# test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "# print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "# print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# # trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "# model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "# token_embed = model.get_input_embeddings()\n",
    "# model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=hidden_size, prop_is_logit=True).to(device)\n",
    "\n",
    "# pos_portion = trainset.treatment.mean()\n",
    "# pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# epoch_iter = len(train_loader)\n",
    "# total_steps = epoch * epoch_iter\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 2, num_training_steps=total_steps)\n",
    "\n",
    "# q_loss = nn.BCELoss()\n",
    "# # prop_score_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# # Please specify the effect and estimation we want to use here.\n",
    "# effect = 'ate'\n",
    "# estimation = 'q'\n",
    "\n",
    "# effect = effect.lower()\n",
    "# estimation = estimation.lower()\n",
    "# assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "# assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_loss, rq1_loss, rq0_loss = [0.] * 3\n",
    "\n",
    "# train_loss_hist, test_loss_hist, est_effect = [], [], []\n",
    "# for e in range(1, epoch + 1):\n",
    "#     model.train()\n",
    "#     start = time.time()\n",
    "#     run_loss = 0.\n",
    "#     for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "#         loss = prop_score_loss(prop_score, treatment)\n",
    "#         if len(response[treatment == 1]) > 0:\n",
    "#             loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "#         if len(response[treatment == 0]) > 0:\n",
    "#             loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "#         loss.backward()\n",
    "        \n",
    "# #         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         run_loss += loss.item()\n",
    "        \n",
    "#     run_idx = idx\n",
    "\n",
    "#     # Evaluation.\n",
    "#     train_loss = run_loss / (run_idx + 1)\n",
    "#     train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "#     test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "#     test_loss = q1_loss_test + q0_loss_test\n",
    "#     test_loss += g_loss_test\n",
    "    \n",
    "#     train_loss_hist.append(train_loss)\n",
    "#     test_loss_hist.append(test_loss)\n",
    "#     est_effect.append(test_effect)\n",
    "#     print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "#           Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "#           Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "#           Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "#     print('*'* 80)\n",
    "#     start = time.time()\n",
    "#     run_loss = 0.\n",
    "\n",
    "# print('Finish training...')\n",
    "\n",
    "# # With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real = true_casual_effect(test_loader)\n",
    "# unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "# show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
