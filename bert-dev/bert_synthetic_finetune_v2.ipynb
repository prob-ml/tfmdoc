{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Synthetic data (V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = 100  # number of billing codes\n",
    "T = 100  # total time steps\n",
    "token_len = 110\n",
    "\n",
    "special_X = False\n",
    "\n",
    "pretrain_size = 1_000_000\n",
    "train_size = 51200 * 2\n",
    "eval_size = 100_000\n",
    "\n",
    "add_special_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(z, a=1, b=3):\n",
    "    return (z >= a ) * (z <= b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pretrain_size\n",
    "\n",
    "# model parameters\n",
    "Psi = torch.arange(0, xdim * 0.01, 0.01)\n",
    "\n",
    "# random variables\n",
    "Z_true = torch.zeros((N, T)) # latent \n",
    "X = torch.zeros((N, T)) # observed\n",
    "X_onehot = torch.zeros((N, T, xdim))\n",
    "Y = torch.zeros((N,1))\n",
    "\n",
    "for t in range(0,T):\n",
    "    # Zit | Zi,t-1, Yi\n",
    "    meanz = (0.9 * Z_true[:, t - 1]) if t != 0 else torch.zeros((N,))\n",
    "    Zt = Normal(meanz, 1)\n",
    "    Z_true[:, t] = Zt.sample()\n",
    "    \n",
    "    # Xit | Zit\n",
    "    Psi_z = Z_true[:,t].view(N, 1) * Psi.view(1, xdim)\n",
    "    PX = F.softmax(Psi_z, dim = 1)\n",
    "    Xt = Categorical(PX)\n",
    "    Xit = Xt.sample()\n",
    "    X[:, t] = Xit\n",
    "    X_onehot[:, t] = F.one_hot(Xit, num_classes = xdim)\n",
    "\n",
    "for t in range(T-2):\n",
    "    Y[:,0] += is_in(Z_true[:,t]) * is_in(Z_true[:,t+1]) * is_in(Z_true[:,t+2])\n",
    "\n",
    "# Truncate the Y values which are greater than 1.\n",
    "Y = torch.cat((Y, torch.ones((N,1))), 1).min(dim = 1).values\n",
    "\n",
    "\n",
    "# Save data to txt file, separate with whitespace ' '.\n",
    "X_ = X.cpu().data.numpy().astype(int)\n",
    "Y_ = Y.cpu().data.numpy().astype(int)\n",
    "\n",
    "if special_X:\n",
    "    # If X[t] == 64, then X[t+2] == 65\n",
    "    for t in range(T-2):\n",
    "        X_special_loc = (X_[:, t] == 64)\n",
    "        X_[X_special_loc, t+2] = 65\n",
    "    \n",
    "np.savetxt('synthetic_pretrain_X.txt', X_, delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_size  # number of patients\n",
    "\n",
    "# model parameters\n",
    "Psi = torch.arange(0, xdim * 0.01, 0.01)\n",
    "\n",
    "# random variables\n",
    "Z_true = torch.zeros((N, T)) # latent \n",
    "X = torch.zeros((N, T)) # observed\n",
    "X_onehot = torch.zeros((N, T, xdim))\n",
    "Y = torch.zeros((N,1))\n",
    "\n",
    "for t in range(T):\n",
    "    # Zit | Zi,t-1\n",
    "    meanz = (0.9 * Z_true[:, t - 1]) if t > 0 else torch.zeros((N,))\n",
    "    Zt = Normal(meanz, 1)\n",
    "    Z_true[:, t] = Zt.sample()\n",
    "    \n",
    "    # Xit | Zit\n",
    "    Psi_z = Z_true[:, t].view(N, 1) * Psi.view(1, xdim)\n",
    "    PX = F.softmax(Psi_z, dim = 1)\n",
    "    Xt = Categorical(PX)\n",
    "    Xit = Xt.sample()\n",
    "    X[:, t] = Xit\n",
    "    X_onehot[:, t] = F.one_hot(Xit, num_classes = xdim)\n",
    "\n",
    "for t in range(T-2):\n",
    "    Y[:,0] += is_in(Z_true[:,t]) * is_in(Z_true[:,t+1]) * is_in(Z_true[:,t+2])\n",
    "\n",
    "# Truncate the Y values which are greater than 1.\n",
    "Y = torch.cat((Y, torch.ones((N,1))), 1).min(dim = 1).values\n",
    "\n",
    "\n",
    "# Save data to txt file, separate with whitespace ' '.\n",
    "X_ = X.cpu().data.numpy().astype(int)\n",
    "Y_ = Y.cpu().data.numpy().astype(int)\n",
    "\n",
    "if special_X:\n",
    "    # If X[t] == 64, then X[t+2] == 65\n",
    "    for t in range(T-2):\n",
    "        X_special_loc = (X_[:, t] == 64)\n",
    "        X_[X_special_loc, t+2] = 65\n",
    "    \n",
    "np.savetxt('synthetic_train_X.txt', X_, delimiter=' ', fmt='%s')\n",
    "np.savetxt('synthetic_train_Y.txt', Y_, delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = eval_size  # number of patients\n",
    "\n",
    "# model parameters\n",
    "Psi = torch.arange(0, xdim * 0.01, 0.01)\n",
    "\n",
    "# random variables\n",
    "Z_true = torch.zeros((N, T)) # latent \n",
    "X = torch.zeros((N, T)) # observed\n",
    "X_onehot = torch.zeros((N, T, xdim))\n",
    "Y = torch.zeros((N,1))\n",
    "\n",
    "for t in range(0,T):\n",
    "    # Zit | Zi,t-1, Yi\n",
    "    meanz = (0.9 * Z_true[:, t - 1]) if t != 0 else torch.zeros((N,))\n",
    "    Zt = Normal(meanz, 1)\n",
    "    Z_true[:, t] = Zt.sample()\n",
    "    \n",
    "    # Xit | Zit\n",
    "    Psi_z = Z_true[:,t].view(N, 1) * Psi.view(1, xdim)\n",
    "    PX = F.softmax(Psi_z, dim = 1)\n",
    "    Xt = Categorical(PX)\n",
    "    Xit = Xt.sample()\n",
    "    X[:, t] = Xit\n",
    "    X_onehot[:, t] = F.one_hot(Xit, num_classes = xdim)\n",
    "\n",
    "for t in range(T-2):\n",
    "    Y[:,0] += is_in(Z_true[:,t]) * is_in(Z_true[:,t+1]) * is_in(Z_true[:,t+2])\n",
    "\n",
    "# Truncate the Y values which are greater than 1.\n",
    "Y = torch.cat((Y, torch.ones((N,1))), 1).min(dim = 1).values\n",
    "\n",
    "\n",
    "# Save data to txt file, separate with whitespace ' '.\n",
    "X_ = X.cpu().data.numpy().astype(int)\n",
    "Y_ = Y.cpu().data.numpy().astype(int)\n",
    "\n",
    "if special_X:\n",
    "    # If X[t] == 64, then X[t+2] == 65\n",
    "    for t in range(T-2):\n",
    "        X_special_loc = (X_[:, t] == 64)\n",
    "        X_[X_special_loc, t+2] = 65\n",
    "    \n",
    "np.savetxt('synthetic_eval_X.txt', X_, delimiter=' ', fmt='%s')\n",
    "np.savetxt('synthetic_eval_Y.txt', Y_, delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create customized Tokenizer.\n",
    "\n",
    "This task is separated into three substeps:\n",
    "\n",
    "*NOTE: Maybe I didn't do it completely correct in this version. Now it just can work.*\n",
    "\n",
    "1. Create a word-level vocabulary based on `Tokenizer` library. See [Issue 232](https://github.com/huggingface/tokenizers/issues/232), [Issue 243](https://github.com/huggingface/tokenizers/issues/243#issuecomment-617860020) for codes.\n",
    "\n",
    "2. Create our own `WordLevelTokenizer` based on `Tokenizer` library. \n",
    "See [whitespace/word level](https://github.com/huggingface/tokenizers/issues/244) for codes.\n",
    "\n",
    "3. Create our own `WordLevelBertTokenizer` that can be used to train Transformers(Bert/Roberta) by wrapping the class from last step based on `Transformer` library. See [Why doesn't this library share the same tokenizer api as the transformers library?](https://github.com/huggingface/tokenizers/issues/259) for tutorial to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create a word-level vocabulary\n",
    "\n",
    "UPDATE: No need to do this, vocab can be directly created by hardcode, see `eda_diag_2010.ipynb` for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "for i, v in enumerate(set(X.unique())):\n",
    "    vocab[str(int(v.item()))] = i\n",
    "\n",
    "for j, v in enumerate(['[UNK]', '[SEP]', '[CLS]']):\n",
    "    vocab[v] = i + j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./Synthetic/vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create WordLevelTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers.models import WordLevel, TokenizedSequence, TokenizedSequenceWithOffsets\n",
    "from tokenizers import Tokenizer, Encoding, AddedToken\n",
    "from tokenizers.normalizers import Lowercase, Sequence, unicode_normalizer_from_str\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit, WhitespaceSplit\n",
    "from tokenizers.implementations import BaseTokenizer \n",
    "\n",
    "\n",
    "class WordLevelTokenizer(BaseTokenizer):\n",
    "    \"\"\" WordLevelBertTokenizer\n",
    "    Represents a simple word level tokenization for BERT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file: Optional[str] = None,\n",
    "        unk_token: Union[str, AddedToken] = \"[UNK]\",\n",
    "        sep_token: Union[str, AddedToken] = \"[SEP]\",\n",
    "        cls_token: Union[str, AddedToken] = \"[CLS]\",\n",
    "        pad_token: Union[str, AddedToken] = \"[PAD]\",\n",
    "        mask_token: Union[str, AddedToken] = \"[MASK]\",\n",
    "        \n",
    "        \n",
    "        lowercase: bool = False,\n",
    "        unicode_normalizer: Optional[str] = None,\n",
    "    ):\n",
    "        if vocab_file is not None:\n",
    "            tokenizer = Tokenizer(WordLevel(vocab_file, unk_token='[UNK]'))\n",
    "        else:\n",
    "            tokenizer = Tokenizer(WordLevel())\n",
    "\n",
    "        # Let the tokenizer know about special tokens if they are part of the vocab\n",
    "        if tokenizer.token_to_id(str(unk_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(unk_token)])\n",
    "        if tokenizer.token_to_id(str(sep_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(sep_token)])\n",
    "        if tokenizer.token_to_id(str(cls_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(cls_token)])\n",
    "        if tokenizer.token_to_id(str(pad_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(pad_token)])\n",
    "        if tokenizer.token_to_id(str(mask_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(mask_token)])\n",
    "\n",
    "        # Check for Unicode normalization first (before everything else)\n",
    "        normalizers = []\n",
    "\n",
    "        if unicode_normalizer:\n",
    "            normalizers += [unicode_normalizer_from_str(unicode_normalizer)]\n",
    "\n",
    "        if lowercase:\n",
    "            normalizers += [Lowercase()]\n",
    "\n",
    "        # Create the normalizer structure\n",
    "        if len(normalizers) > 0:\n",
    "            if len(normalizers) > 1:\n",
    "                tokenizer.normalizer = Sequence(normalizers)\n",
    "            else:\n",
    "                tokenizer.normalizer = normalizers[0]\n",
    "\n",
    "        tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "#         tokenizer.pre_tokenizer = CharDelimiterSplit(',')\n",
    "\n",
    "        if vocab_file is not None:\n",
    "            cls_token_id = tokenizer.token_to_id(str(cls_token))\n",
    "            if cls_token_id is None:\n",
    "                raise TypeError(\"cls_token not found in the vocabulary\")\n",
    "                \n",
    "            sep_token_id = tokenizer.token_to_id(str(sep_token))\n",
    "            if sep_token_id is None:\n",
    "                raise TypeError(\"sep_token not found in the vocabulary\")\n",
    "\n",
    "\n",
    "            tokenizer.post_processor = tokenizers.processors.BertProcessing(\n",
    "                (str(sep_token), sep_token_id), (str(cls_token), cls_token_id)\n",
    "            )\n",
    "\n",
    "        parameters = {\n",
    "            \"model\": \"WordLevel\",\n",
    "            \"unk_token\": unk_token,\n",
    "            \"sep_token\": sep_token,\n",
    "            \"cls_token\": cls_token,\n",
    "            \"pad_token\": pad_token,\n",
    "            \"mask_token\": mask_token,\n",
    "            \"lowercase\": lowercase,\n",
    "            \"unicode_normalizer\": unicode_normalizer,\n",
    "        }\n",
    "\n",
    "        super().__init__(tokenizer, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create WordLevelBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "class WordLevelBertTokenizer(PreTrainedTokenizerFast):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        bos_token=\"[CLS]\",\n",
    "        eos_token=\"[SEP]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            tokenizer,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "    # Copied from [BertTokenizer]\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True if the token list is already formatted with special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "    \n",
    "    # Copied from [BertTokenizer]\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A BERT sequence has the following format:\n",
    "\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Instantize Tokenizes we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=105, model=WordLevel, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], lowercase=False, unicode_normalizer=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = WordLevelTokenizer('./Synthetic/vocab.json')\n",
    "BertTokenizer = WordLevelBertTokenizer(tokenizer = tokenizer)\n",
    "\n",
    "len(BertTokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Instantize Bert, Dataset, Data_collator, Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bert model contains 15525993 parameters.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U numpy\n",
    "import torch\n",
    "from transformers import BertForMaskedLM\n",
    "import os\n",
    "\n",
    "# Specify visible CUDA for the script, try to avoid encounder out of memory issue.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "\n",
    "TRAIN_NEW_MODEL = True\n",
    "\n",
    "if TRAIN_NEW_MODEL:\n",
    "    \n",
    "    from transformers import BertConfig\n",
    "    \n",
    "    config = BertConfig(\n",
    "        vocab_size=len(BertTokenizer),\n",
    "        max_position_embeddings=token_len,\n",
    "        num_attention_heads=1,\n",
    "        num_hidden_layers=2,\n",
    "#         hidden_size\n",
    "        type_vocab_size=1,\n",
    "\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # load a pre-trained model.\n",
    "    from transformers import AutoConfig\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
    "    \n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "print(f'The Bert model contains {model.num_parameters()} parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
    "        assert os.path.isfile(file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "#         logger.info(\"Creating features from dataset file at %s\", file_path)\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        batch_encoding = tokenizer.batch_encode_plus(lines, add_special_tokens=add_special_tokens, max_length=block_size, truncation=True)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 42.5 s, total: 2min 57s\n",
      "Wall time: 42.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from transformers import LineByLineTextDataset\n",
    "\n",
    "# Define a dataset.\n",
    "# Each output is the encoded row.\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=BertTokenizer,\n",
    "    file_path=\"./synthetic_pretrain_X.txt\",\n",
    "    block_size=token_len,\n",
    ")\n",
    "\n",
    "# Define a data-collator. \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=BertTokenizer, mlm=True, mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Synthetic/finetune_v1\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "#     save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35d3614d24f48169d348fadcad3ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46a924aaf1440c0825fcbdc7d4b3eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=31250.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b2f21e9776461c9bb0afa32626b010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=31250.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CPU times: user 50min 49s, sys: 9min 38s, total: 1h 27s\n",
      "Wall time: 1h 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=62500, training_loss=4.204258345924377)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./Synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/.local/lib/python3.7/site-packages/transformers/modeling_auto.py:792: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./Synthetic\",\n",
    "    tokenizer=BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 1 64 0 0 21 [SEP]',\n",
       "  'score': 0.024993469938635826,\n",
       "  'token': 5,\n",
       "  'token_str': '0'},\n",
       " {'sequence': '[CLS] 1 64 0 2 21 [SEP]',\n",
       "  'score': 0.02365591563284397,\n",
       "  'token': 7,\n",
       "  'token_str': '2'},\n",
       " {'sequence': '[CLS] 1 64 0 1 21 [SEP]',\n",
       "  'score': 0.023554932326078415,\n",
       "  'token': 6,\n",
       "  'token_str': '1'},\n",
       " {'sequence': '[CLS] 1 64 0 3 21 [SEP]',\n",
       "  'score': 0.02277740277349949,\n",
       "  'token': 8,\n",
       "  'token_str': '3'},\n",
       " {'sequence': '[CLS] 1 64 0 4 21 [SEP]',\n",
       "  'score': 0.02265048399567604,\n",
       "  'token': 9,\n",
       "  'token_str': '4'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the Bert can fill the mask at least.\n",
    "fill_mask(\"1 64 0 [MASK] 21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 See training logs.\n",
    "run `tensorboard dev upload --logdir runs` just outside of the `runs` fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Sequence Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 New Dataset\n",
    "\n",
    "This dataset allows us to load the label as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class ClsLineByLineTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, sequence_file_path: str, label_file_path: str, block_size: int):\n",
    "        assert os.path.isfile(sequence_file_path) and os.path.isfile(label_file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "\n",
    "        with open(sequence_file_path, encoding=\"utf-8\") as f:\n",
    "            seqs = [seq for seq in f.read().splitlines() if (len(seq) > 0 and not seq.isspace())]\n",
    "        batch_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=add_special_tokens, max_length=block_size, truncation=True)\n",
    "        self.input_ids = batch_encoding[\"input_ids\"]\n",
    "        self.attention_mask = batch_encoding[\"attention_mask\"]\n",
    "        self.token_type_ids = batch_encoding['token_type_ids']\n",
    "        \n",
    "        with open(label_file_path, encoding=\"utf-8\") as f:\n",
    "            labels = [int(label) for label in f.read().splitlines() if (len(label) > 0 and not label.isspace())]\n",
    "        self.labels = labels\n",
    "        assert len(self.labels) == len(self.input_ids)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.input_ids[i], dtype=torch.long), torch.tensor(self.labels[i]).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = ClsLineByLineTextDataset(\n",
    "    tokenizer=BertTokenizer,\n",
    "    sequence_file_path=\"./synthetic_train_X.txt\", label_file_path=\"./synthetic_train_Y.txt\",\n",
    "    block_size=token_len,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=512, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bert model contains 14935298 parameters.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./Synthetic/')\n",
    "print(f'The Bert model contains {model.num_parameters()} parameters.')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input_ids, labels in dataloader:\n",
    "#     print(input_ids)\n",
    "#     print(labels)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Train and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------Start epoch: 1/10----------------------------------------\n",
      "iter: 100, average loss: 16.973604701459408.\n",
      "iter: 200, average loss: 13.396220453083515.\n",
      "----------------------------------------Start epoch: 2/10----------------------------------------\n",
      "iter: 300, average loss: 13.423288814723492.\n",
      "iter: 400, average loss: 13.016181953251362.\n",
      "----------------------------------------Start epoch: 3/10----------------------------------------\n",
      "iter: 500, average loss: 13.136520199477673.\n",
      "iter: 600, average loss: 12.781998857855797.\n",
      "----------------------------------------Start epoch: 4/10----------------------------------------\n",
      "iter: 700, average loss: 12.840252920985222.\n",
      "iter: 800, average loss: 12.53277326375246.\n",
      "----------------------------------------Start epoch: 5/10----------------------------------------\n",
      "iter: 900, average loss: 12.332804150879383.\n",
      "iter: 1000, average loss: 12.246495254337788.\n",
      "----------------------------------------Start epoch: 6/10----------------------------------------\n",
      "iter: 1100, average loss: 11.668080426752567.\n",
      "iter: 1200, average loss: 12.119072489440441.\n",
      "----------------------------------------Start epoch: 7/10----------------------------------------\n",
      "iter: 1300, average loss: 11.108392901718616.\n",
      "iter: 1400, average loss: 11.85619044303894.\n",
      "----------------------------------------Start epoch: 8/10----------------------------------------\n",
      "iter: 1500, average loss: 10.746657077223063.\n",
      "iter: 1600, average loss: 11.618874795734882.\n",
      "----------------------------------------Start epoch: 9/10----------------------------------------\n",
      "iter: 1700, average loss: 10.153961505740881.\n",
      "iter: 1800, average loss: 11.049677602946758.\n",
      "----------------------------------------Start epoch: 10/10----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "\n",
    "iters = 0\n",
    "iter_loss = 0.\n",
    "for e in range(epoch):\n",
    "    print('-' * 40 + f'Start epoch: {e + 1}/{epoch}' + '-' * 40)\n",
    "    for input_ids, labels in dataloader:\n",
    "        loss, logits = model(input_ids, labels=labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iters += 1\n",
    "        iter_loss += loss.item()\n",
    "        if not iters % 100:\n",
    "            print(f'iter: {iters}, average loss: {iter_loss}.')\n",
    "            iter_loss = 0.\n",
    "\n",
    "print('Finish all.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score,roc_auc_score, roc_curve, auc\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "def ROC(y_test, y_prob):\n",
    "\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate, true_positive_rate, color = 'red', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1], linestyle = '--')\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_id, label in dataloader:\n",
    "        # True labels\n",
    "        labels.append(label.squeeze())\n",
    "        # Predict probability\n",
    "        logits = model(input_id)[0]\n",
    "        pred_prob.append(F.softmax(logits, dim=1)[:,1])\n",
    "        \n",
    "labels = torch.cat(labels)\n",
    "pred_prob = torch.cat(pred_prob)\n",
    "pred_labels = 1 * (pred_prob > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training accuracy: {accuracy_score(labels, pred_labels)}.')\n",
    "\n",
    "confusion_matrix(labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(labels, pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalset = ClsLineByLineTextDataset(\n",
    "    tokenizer=BertTokenizer,\n",
    "    sequence_file_path=\"./synthetic_eval_X.txt\", label_file_path=\"./synthetic_eval_Y.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "evalloader = DataLoader(evalset, batch_size=200)\n",
    "\n",
    "pred_prob = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_id, label in evalloader:\n",
    "        # True labels\n",
    "        labels.append(label.squeeze())\n",
    "        # Predict probability\n",
    "        logits = model(input_id)[0]\n",
    "        pred_prob.append(F.softmax(logits, dim=1)[:,1])\n",
    "        \n",
    "labels = torch.cat(labels)\n",
    "pred_prob = torch.cat(pred_prob)\n",
    "pred_labels = 1 * (pred_prob > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Testing accuracy: {accuracy_score(labels, pred_labels)}.')\n",
    "\n",
    "confusion_matrix(labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(labels, pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
