{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source notebook for this notebook can be found at: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=EIS-irI0f32P\n",
    "\n",
    "There is another tutorial for pre train a Bert, which I only went through briefly yet: https://d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-18 01:15:23--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
      "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 13.249.87.37, 13.249.87.74, 13.249.87.7, ...\n",
      "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|13.249.87.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/liutianc/.local/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: tokenizers in /home/liutianc/.local/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: numpy in /home/liutianc/.local/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sacremoses in /home/liutianc/.local/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece in /home/liutianc/.local/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/liutianc/.local/lib/python3.7/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda/lib/python3.7/site-packages (from transformers) (19.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda/lib/python3.7/site-packages (from transformers) (4.32.1)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: six in /usr/local/anaconda/lib/python3.7/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/anaconda/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda/lib/python3.7/site-packages (from packaging->transformers) (2.4.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/anaconda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda/lib/python3.7/site-packages (from requests->transformers) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda/lib/python3.7/site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/anaconda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/anaconda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "tokenizers                         0.7.0    \n",
      "transformers                       2.11.0   \n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/anaconda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We won't need TensorFlow here\n",
    "\n",
    "# Install `transformers` from master\n",
    "\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "!pip install transformers tokenizers\n",
    "!pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.9.1\n",
    "# tokenizers version at notebook update --- 0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 ms, sys: 4.43 ms, total: 7.12 ms\n",
      "Wall time: 5.78 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('EsperBERTo'):\n",
    "    os.makedirs('EsperBERTo')\n",
    "    \n",
    "    # Initialize a tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    #TODO: Is this code runnable on GPU?\n",
    "    paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "\n",
    "    # Customize training\n",
    "    tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "    \n",
    "    # TODO: Cannot get this code run with the latest version of tokenizer.\n",
    "    tokenizer.save('EsperBERTo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./EsperBERTo/vocab.json\",\n",
    "    \"./EsperBERTo/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Specify visible CUDA for the script, try to avoid encounder out of memory issue.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4,5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bert model contains 84095008 parameters.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# NOTE: If we want a PRE-TRAINED model, we may want to specify it in the config.\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print(f'The Bert model contains {model.num_parameters()} parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 52s, sys: 36.3 s, total: 9min 29s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "# Define a dataset.\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./oscar.eo.txt\",\n",
    "    block_size=64,\n",
    ")\n",
    "\n",
    "# Define a data-collator. \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./EsperBERTo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Trainer is a wrapper of: \n",
    "model, \n",
    "training hyperparameters,\n",
    "data_collator,\n",
    "dataset: TODO: what is the difference and connection between data_coolator and dataset?\n",
    "prediction_loss_only\n",
    "\"\"\" \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063cab41494043629d67b411a9d290e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9709860aff6c4eceb5c554c3513cd381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=6091.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 7.458342568397522, \"learning_rate\": 4.589558364800526e-05, \"epoch\": 0.08208832703989492, \"step\": 500}\n",
      "{\"loss\": 6.836832889556884, \"learning_rate\": 4.179116729601051e-05, \"epoch\": 0.16417665407978985, \"step\": 1000}\n",
      "{\"loss\": 6.644652894973754, \"learning_rate\": 3.768675094401576e-05, \"epoch\": 0.24626498111968478, \"step\": 1500}\n",
      "{\"loss\": 6.499795407295227, \"learning_rate\": 3.358233459202101e-05, \"epoch\": 0.3283533081595797, \"step\": 2000}\n",
      "{\"loss\": 6.344393545150757, \"learning_rate\": 2.947791824002627e-05, \"epoch\": 0.41044163519947463, \"step\": 2500}\n",
      "{\"loss\": 6.118794407844543, \"learning_rate\": 2.5373501888031527e-05, \"epoch\": 0.49252996223936957, \"step\": 3000}\n",
      "{\"loss\": 5.912473185539246, \"learning_rate\": 2.1269085536036776e-05, \"epoch\": 0.5746182892792645, \"step\": 3500}\n",
      "{\"loss\": 5.7373830652236935, \"learning_rate\": 1.7164669184042028e-05, \"epoch\": 0.6567066163191594, \"step\": 4000}\n",
      "{\"loss\": 5.6290211706161495, \"learning_rate\": 1.3060252832047284e-05, \"epoch\": 0.7387949433590544, \"step\": 4500}\n",
      "{\"loss\": 5.543111830711365, \"learning_rate\": 8.955836480052538e-06, \"epoch\": 0.8208832703989493, \"step\": 5000}\n",
      "{\"loss\": 5.489462571144104, \"learning_rate\": 4.8514201280577905e-06, \"epoch\": 0.9029715974388441, \"step\": 5500}\n",
      "{\"loss\": 5.456834121704102, \"learning_rate\": 7.470037760630439e-07, \"epoch\": 0.9850599244787391, \"step\": 6000}\n",
      "\n",
      "\n",
      "CPU times: user 1h 19min 46s, sys: 24min 38s, total: 1h 44min 24s\n",
      "Wall time: 52min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6091, training_loss=6.128779335848668)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '<s> La suno estas.</s>',\n",
       "  'score': 0.0185850802809,\n",
       "  'token': 316},\n",
       " {'sequence': '<s> La suno estis.</s>',\n",
       "  'score': 0.014196408912539482,\n",
       "  'token': 394},\n",
       " {'sequence': '<s> La suno de.</s>',\n",
       "  'score': 0.01179225742816925,\n",
       "  'token': 274},\n",
       " {'sequence': '<s> La suno urbo.</s>',\n",
       "  'score': 0.004567708354443312,\n",
       "  'token': 871},\n",
       " {'sequence': '<s> La suno tago.</s>',\n",
       "  'score': 0.0030458723194897175,\n",
       "  'token': 1633}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sun <mask>.\n",
    "# =>\n",
    "\n",
    "fill_mask(\"La suno <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '<s> Jen la komenco de bela mondo.</s>',\n",
       "  'score': 0.009763634763658047,\n",
       "  'token': 945},\n",
       " {'sequence': '<s> Jen la komenco de bela lingvo.</s>',\n",
       "  'score': 0.00913243368268013,\n",
       "  'token': 697},\n",
       " {'sequence': '<s> Jen la komenco de bela vivo.</s>',\n",
       "  'score': 0.006484068930149078,\n",
       "  'token': 1160},\n",
       " {'sequence': '<s> Jen la komenco de bela Esperanto.</s>',\n",
       "  'score': 0.00600389065220952,\n",
       "  'token': 540},\n",
       " {'sequence': '<s> Jen la komenco de bela lando.</s>',\n",
       "  'score': 0.005966768134385347,\n",
       "  'token': 1076}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Jen la komenco de bela <mask>.\")\n",
    "\n",
    "# This is the beginning of a beautiful <mask>.\n",
    "# =>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
