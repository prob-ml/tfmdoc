{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source notebook can be found at: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=EIS-irI0f32P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
      "Looking in indexes: http://pypi.python.org/simple, https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl\n",
      "Collecting tokenizers\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/98/a2/11e6465beaecbf92a3f203e44447a43110e3e0ee2cfdc9cfe03c7e2c1051/tokenizers-0.7.0-cp37-cp37m-macosx_10_10_x86_64.whl\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/b8/7b/01510a6229c2176425bda54d15fba05a4b3df169b87265b008480261d2f9/regex-2020.6.8.tar.gz\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 4.5MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting sentencepiece (from transformers)\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/98/cf/f235332b2e557b25e628835bf4ca6b604ba19610438f171ac98fe9bcc608/sentencepiece-0.1.92-cp37-cp37m-macosx_10_6_x86_64.whl\n",
      "Collecting sacremoses (from transformers)\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz\n",
      "Collecting packaging (from transformers)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting requests (from transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 21.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy (from transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/3e/00/0266fefaafb839760d5b25b884375b2ab0f842ebe138ee6c1ef807af44bb/numpy-1.18.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.1MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1MB 11.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six (from sacremoses->transformers)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting click (from sacremoses->transformers)\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://mirrors.aliyun.com/pypi/packages/b8/a6/d1a816b89aa1e9e96bcb298eb1ee1854f21662ebc6d55ffa3d7b3b50122b/joblib-0.15.1-py3-none-any.whl\n",
      "Collecting pyparsing>=2.0.2 (from packaging->transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 12.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 1.2MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/98/99/def511020aa8f663d4a2cfaa38467539e864799289ff354569e339e375b1/certifi-2020.4.5.2-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 22.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl (126kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 20.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://mirrors.aliyun.com/pypi/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 25.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex, tqdm, sentencepiece, six, click, joblib, sacremoses, pyparsing, packaging, tokenizers, filelock, idna, certifi, urllib3, chardet, requests, numpy, transformers\n",
      "  Running setup.py install for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed certifi-2020.4.5.2 chardet-3.0.4 click-7.1.2 filelock-3.0.12 idna-2.9 joblib-0.15.1 numpy-1.18.5 packaging-20.4 pyparsing-2.4.7 regex-2020.6.8 requests-2.23.0 sacremoses-0.0.43 sentencepiece-0.1.92 six-1.15.0 tokenizers-0.7.0 tqdm-4.46.1 transformers-2.11.0 urllib3-1.25.9\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "tokenizers    0.7.0     \n",
      "transformers  2.11.0    \n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# We won't need TensorFlow here\n",
    "# Install `transformers` from master\n",
    "\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "\n",
    "!pip install transformers tokenizers\n",
    "!pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.9.1\n",
    "# tokenizers version at notebook update --- 0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 4s, sys: 11min 56s, total: 26min 1s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('EsperBERTo'):\n",
    "    os.makedirs('EsperBERTo')\n",
    "\n",
    "tokenizer.save('EsperBERTo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
