{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = 512\n",
    "add_special_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers.models import WordLevel, TokenizedSequence, TokenizedSequenceWithOffsets\n",
    "from tokenizers import Tokenizer, Encoding, AddedToken\n",
    "from tokenizers.normalizers import Lowercase, Sequence, unicode_normalizer_from_str\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit, WhitespaceSplit\n",
    "from tokenizers.implementations import BaseTokenizer \n",
    "\n",
    "\n",
    "class WordLevelTokenizer(BaseTokenizer):\n",
    "    \"\"\" WordLevelBertTokenizer\n",
    "    Represents a simple word level tokenization for BERT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file: Optional[str] = None,\n",
    "        unk_token: Union[str, AddedToken] = \"[UNK]\",\n",
    "        sep_token: Union[str, AddedToken] = \"[SEP]\",\n",
    "        cls_token: Union[str, AddedToken] = \"[CLS]\",\n",
    "        pad_token: Union[str, AddedToken] = \"[PAD]\",\n",
    "        mask_token: Union[str, AddedToken] = \"[MASK]\",\n",
    "        lowercase: bool = False,\n",
    "        unicode_normalizer: Optional[str] = None,\n",
    "    ):\n",
    "        if vocab_file is not None:\n",
    "            tokenizer = Tokenizer(WordLevel(vocab_file, unk_token='[UNK]'))\n",
    "        else:\n",
    "            tokenizer = Tokenizer(WordLevel())\n",
    "\n",
    "        # Let the tokenizer know about special tokens if they are part of the vocab\n",
    "        if tokenizer.token_to_id(str(unk_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(unk_token)])\n",
    "        if tokenizer.token_to_id(str(sep_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(sep_token)])\n",
    "        if tokenizer.token_to_id(str(cls_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(cls_token)])\n",
    "        if tokenizer.token_to_id(str(pad_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(pad_token)])\n",
    "        if tokenizer.token_to_id(str(mask_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(mask_token)])\n",
    "\n",
    "        # Check for Unicode normalization first (before everything else)\n",
    "        normalizers = []\n",
    "\n",
    "        if unicode_normalizer:\n",
    "            normalizers += [unicode_normalizer_from_str(unicode_normalizer)]\n",
    "\n",
    "        if lowercase:\n",
    "            normalizers += [Lowercase()]\n",
    "\n",
    "        # Create the normalizer structure\n",
    "        if len(normalizers) > 0:\n",
    "            if len(normalizers) > 1:\n",
    "                tokenizer.normalizer = Sequence(normalizers)\n",
    "            else:\n",
    "                tokenizer.normalizer = normalizers[0]\n",
    "\n",
    "        tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "#         tokenizer.pre_tokenizer = CharDelimiterSplit(',')\n",
    "\n",
    "        if vocab_file is not None:\n",
    "#             tokenizer.add_special_tokens({unk_token:'[UNK]', sep_token:'[SEP]', cls_token:'[CLS]'})\n",
    "#             tokenizer.add_special_tokens(['[UNK]', '[SEP]', '[CLS]'])\n",
    "\n",
    "            cls_token_id = tokenizer.token_to_id(str(cls_token))\n",
    "            if cls_token_id is None:\n",
    "                raise TypeError(\"cls_token not found in the vocabulary\")\n",
    "                \n",
    "            sep_token_id = tokenizer.token_to_id(str(sep_token))\n",
    "            if sep_token_id is None:\n",
    "                raise TypeError(\"sep_token not found in the vocabulary\")\n",
    "\n",
    "            tokenizer.post_processor = tokenizers.processors.BertProcessing(\n",
    "                (str(sep_token), sep_token_id), \n",
    "                (str(cls_token), cls_token_id)\n",
    "            )\n",
    "\n",
    "        parameters = {\n",
    "            \"model\": \"WordLevel\",\n",
    "            \"unk_token\": unk_token,\n",
    "            \"sep_token\": sep_token,\n",
    "            \"cls_token\": cls_token,\n",
    "            \"pad_token\": pad_token,\n",
    "            \"mask_token\": mask_token,\n",
    "            \"lowercase\": lowercase,\n",
    "            \"unicode_normalizer\": unicode_normalizer,\n",
    "        }\n",
    "\n",
    "        super().__init__(tokenizer, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "class WordLevelBertTokenizer(PreTrainedTokenizerFast):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        bos_token=\"[CLS]\",\n",
    "        eos_token=\"[SEP]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            tokenizer,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "    # Copied from [BertTokenizer]\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True if the token list is already formatted with special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "    \n",
    "    # Copied from [BertTokenizer]\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A BERT sequence has the following format:\n",
    "\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] [SEP]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = WordLevelTokenizer('./Real/vocab.json')\n",
    "BertTokenizer = WordLevelBertTokenizer(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bert model contains 1963154 parameters.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U numpy\n",
    "import torch\n",
    "from transformers import BertForMaskedLM\n",
    "import os\n",
    "\n",
    "# Specify visible CUDA for the script, try to avoid encounder out of memory issue.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "TRAIN_NEW_MODEL = True\n",
    "\n",
    "if TRAIN_NEW_MODEL:\n",
    "    \n",
    "    from transformers import BertConfig\n",
    "    \n",
    "    config = BertConfig(\n",
    "        vocab_size=len(BertTokenizer),\n",
    "        max_position_embeddings=token_len,\n",
    "        num_attention_heads=1,\n",
    "        num_hidden_layers=2,\n",
    "        hidden_size=64,\n",
    "        type_vocab_size=1,\n",
    "\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # load a pre-trained model.\n",
    "    from transformers import AutoConfig\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
    "    \n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "print(f'The Bert model contains {model.num_parameters()} parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
    "        assert os.path.isfile(file_path)\n",
    "        # Here, we do not cache the features, operating under the assumption\n",
    "        # that we will soon use fast multithreaded tokenizers from the\n",
    "        # `tokenizers` repo everywhere =)\n",
    "#         logger.info(\"Creating features from dataset file at %s\", file_path)\n",
    "\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "        batch_encoding = tokenizer.batch_encode_plus(lines, add_special_tokens=add_special_tokens, max_length=block_size, truncation=True)\n",
    "        self.examples = batch_encoding[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 22s, sys: 33 s, total: 6min 55s\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from transformers import LineByLineTextDataset\n",
    "\n",
    "# Define a dataset, each output is the encoded row.\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=BertTokenizer,\n",
    "    file_path=\"./data/claim2010.txt\",\n",
    "    block_size=token_len,\n",
    ")\n",
    "\n",
    "# Define a data-collator. \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=BertTokenizer, mlm=True, mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Real/claim2010_v1\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "#     save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e725d120fb4e3d8126f9baff4f032b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a1b3536b9341fb970e0e9928f0308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=41977.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b5869649704fe9915880a23cab9aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=41977.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CPU times: user 1h 22min 37s, sys: 10min 2s, total: 1h 32min 40s\n",
      "Wall time: 1h 32min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=83954, training_loss=5.383341884475019)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
