{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Synthetic data (V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(z, a=1, b=3):\n",
    "    return (z >= a ) * (z <= b)\n",
    "\n",
    "xdim = 100  # number of billing codes\n",
    "N = 20_0000  # number of patients\n",
    "T = 5  # total time steps\n",
    "\n",
    "# model parameters\n",
    "Psi = torch.arange(0, xdim * 0.01, 0.01)\n",
    "\n",
    "# random variables\n",
    "Z_true = torch.zeros((N, T)) # latent \n",
    "X = torch.zeros((N, T)) # observed\n",
    "X_onehot = torch.zeros((N, T, xdim))\n",
    "Y = torch.zeros((N,1))\n",
    "\n",
    "for t in range(0,T):\n",
    "    # Zit | Zi,t-1, Yi\n",
    "    meanz = (0.9 * Z_true[:, t - 1]) if t != 0 else torch.zeros((N,))\n",
    "    Zt = Normal(meanz, 1)\n",
    "    Z_true[:, t] = Zt.sample()\n",
    "    \n",
    "    # Xit | Zit\n",
    "    Psi_z = Z_true[:,t].view(N, 1) * Psi.view(1, xdim)\n",
    "    PX = F.softmax(Psi_z, dim = 1)\n",
    "    Xt = Categorical(PX)\n",
    "    Xit = Xt.sample()\n",
    "    X[:, t] = Xit\n",
    "    X_onehot[:, t] = F.one_hot(Xit, num_classes = xdim)\n",
    "\n",
    "for t in range(T-2):\n",
    "    Y[:,0] += is_in(Z_true[:,t]) * is_in(Z_true[:,t+1]) * is_in(Z_true[:,t+2])\n",
    "\n",
    "# Truncate the Y values which are greater than 1.\n",
    "Y = torch.cat((Y, torch.ones((N,1))), 1).min(dim = 1).values\n",
    "\n",
    "\n",
    "# Save data to txt file, separate with whitespace ' '.\n",
    "X_ = X.cpu().data.numpy().astype(int)\n",
    "Y_ = Y.cpu().data.numpy().astype(int)\n",
    "\n",
    "np.savetxt('synthetic_X.txt', X_, delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create customized Tokenizer.\n",
    "\n",
    "This task is separated into three substeps:\n",
    "\n",
    "*NOTE: Maybe I didn't do it completely correct in this version. Now it just can work.*\n",
    "\n",
    "1. Create a word-level vocabulary based on `Tokenizer` library. See [Issue 232](https://github.com/huggingface/tokenizers/issues/232), [Issue 243](https://github.com/huggingface/tokenizers/issues/243#issuecomment-617860020) for codes.\n",
    "\n",
    "2. Create our own `WordLevelTokenizer` based on `Tokenizer` library. \n",
    "See [whitespace/word level](https://github.com/huggingface/tokenizers/issues/244) for codes.\n",
    "\n",
    "3. Create our own `WordLevelBertTokenizer` that can be used to train Transformers(Bert/Roberta) by wrapping the class from last step based on `Transformer` library. See [Why doesn't this library share the same tokenizer api as the transformers library?](https://github.com/huggingface/tokenizers/issues/259) for tutorial to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create a word-level vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['synthetic_X.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', '6', '64', '9', '0', '48', '[SEP]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time \n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import Tokenizer, trainers\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# We build our custom tokenizer:\n",
    "tokenizer = Tokenizer(WordLevel()) \n",
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "# We can train this tokenizer by giving it a list of path to text files:\n",
    "trainer = trainers.BpeTrainer(special_tokens=\n",
    "        [\"[SEP]\",\n",
    "        \"[PAD]\",\n",
    "        \"[CLS]\",\n",
    "        \"[UNK]\",\n",
    "        \"[MASK]\",\n",
    "    ])\n",
    "\n",
    "\n",
    "files = [str(x) for x in Path(\".\").glob(\"**/synthetic_X.txt\")]\n",
    "print(files)\n",
    "\n",
    "tokenizer.train(trainer, files)\n",
    "\n",
    "# Add post_processor.\n",
    "\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), # SEP \n",
    "    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), # CLS\n",
    ")\n",
    "\n",
    "    \n",
    "if not os.path.exists('Synthetic'):\n",
    "    os.makedirs('Synthetic')\n",
    "    \n",
    "# Set truncation.\n",
    "tokenizer.enable_truncation(max_length=128)\n",
    "\n",
    "# And now it is ready, we can save the vocabulary with\n",
    "tokenizer.model.save(\"./Synthetic\")\n",
    "\n",
    "# And simply use it\n",
    "tokenizer.encode('30, 63, 48, 31, 31').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '30,', '63,', '48,', '31,', '31', '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0,]\n",
    "\n",
    "tokenizer.encode('30, 63, 48, 31, 31').tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create WordLevelTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers.models import WordLevel, TokenizedSequence, TokenizedSequenceWithOffsets\n",
    "from tokenizers import Tokenizer, Encoding, AddedToken\n",
    "from tokenizers.normalizers import Lowercase, Sequence, unicode_normalizer_from_str\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit, WhitespaceSplit\n",
    "from tokenizers.implementations import BaseTokenizer \n",
    "\n",
    "\n",
    "class WordLevelTokenizer(BaseTokenizer):\n",
    "    \"\"\" WordLevelBertTokenizer\n",
    "    Represents a simple word level tokenization for BERT.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file: Optional[str] = None,\n",
    "        unk_token: Union[str, AddedToken] = \"[UNK]\",\n",
    "        sep_token: Union[str, AddedToken] = \"[SEP]\",\n",
    "        cls_token: Union[str, AddedToken] = \"[CLS]\",\n",
    "        pad_token: Union[str, AddedToken] = \"[PAD]\",\n",
    "        mask_token: Union[str, AddedToken] = \"[MASK]\",\n",
    "        \n",
    "        \n",
    "        lowercase: bool = False,\n",
    "        unicode_normalizer: Optional[str] = None,\n",
    "    ):\n",
    "        if vocab_file is not None:\n",
    "            tokenizer = Tokenizer(WordLevel(vocab_file, unk_token='[UNK]'))\n",
    "        else:\n",
    "            tokenizer = Tokenizer(WordLevel())\n",
    "\n",
    "        # Let the tokenizer know about special tokens if they are part of the vocab\n",
    "        if tokenizer.token_to_id(str(unk_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(unk_token)])\n",
    "        if tokenizer.token_to_id(str(sep_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(sep_token)])\n",
    "        if tokenizer.token_to_id(str(cls_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(cls_token)])\n",
    "        if tokenizer.token_to_id(str(pad_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(pad_token)])\n",
    "        if tokenizer.token_to_id(str(mask_token)) is not None:\n",
    "            tokenizer.add_special_tokens([str(mask_token)])\n",
    "\n",
    "        # Check for Unicode normalization first (before everything else)\n",
    "        normalizers = []\n",
    "\n",
    "        if unicode_normalizer:\n",
    "            normalizers += [unicode_normalizer_from_str(unicode_normalizer)]\n",
    "\n",
    "        if lowercase:\n",
    "            normalizers += [Lowercase()]\n",
    "\n",
    "        # Create the normalizer structure\n",
    "        if len(normalizers) > 0:\n",
    "            if len(normalizers) > 1:\n",
    "                tokenizer.normalizer = Sequence(normalizers)\n",
    "            else:\n",
    "                tokenizer.normalizer = normalizers[0]\n",
    "\n",
    "        tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "        if vocab_file is not None:\n",
    "            cls_token_id = tokenizer.token_to_id(str(cls_token))\n",
    "            if cls_token_id is None:\n",
    "                raise TypeError(\"cls_token not found in the vocabulary\")\n",
    "                \n",
    "            sep_token_id = tokenizer.token_to_id(str(sep_token))\n",
    "            if sep_token_id is None:\n",
    "                raise TypeError(\"sep_token not found in the vocabulary\")\n",
    "\n",
    "\n",
    "            tokenizer.post_processor = tokenizers.processors.BertProcessing(\n",
    "                (str(sep_token), sep_token_id), (str(cls_token), cls_token_id)\n",
    "            )\n",
    "\n",
    "        parameters = {\n",
    "            \"model\": \"WordLevel\",\n",
    "            \"unk_token\": unk_token,\n",
    "            \"sep_token\": sep_token,\n",
    "            \"cls_token\": cls_token,\n",
    "            \"pad_token\": pad_token,\n",
    "            \"mask_token\": mask_token,\n",
    "            \"lowercase\": lowercase,\n",
    "            \"unicode_normalizer\": unicode_normalizer,\n",
    "        }\n",
    "\n",
    "        super().__init__(tokenizer, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create WordLevelBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "class WordLevelBertTokenizer(PreTrainedTokenizerFast):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        bos_token=\"[CLS]\",\n",
    "        eos_token=\"[SEP]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            tokenizer,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "    # Copied from [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html?highlight=get_special_tokens_mask#transformers.BertTokenizer.get_special_tokens_mask)\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True if the token list is already formatted with special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "    \n",
    "    # Copied from [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html?highlight=get_special_tokens_mask#transformers.BertTokenizer.get_special_tokens_mask)\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A BERT sequence has the following format:\n",
    "\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Instantize Tokenizes we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordLevelTokenizer('./Synthetic/vocab.json')\n",
    "BertTokenizer = WordLevelBertTokenizer(tokenizer = tokenizer, unk_token='[UNK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Instantize Bert, Dataset, Data_collator, Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bert model contains 8451945 parameters.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U numpy\n",
    "import torch\n",
    "from transformers import BertForMaskedLM\n",
    "import os\n",
    "\n",
    "# Specify visible CUDA for the script, try to avoid encounder out of memory issue.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "TRAIN_NEW_MODEL = True\n",
    "\n",
    "if TRAIN_NEW_MODEL:\n",
    "    \n",
    "    from transformers import BertConfig\n",
    "    \n",
    "    config = BertConfig(\n",
    "        vocab_size=len(BertTokenizer),\n",
    "        max_position_embeddings=128,\n",
    "        num_attention_heads=1,\n",
    "        num_hidden_layers=1,\n",
    "        type_vocab_size=1,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # load a pre-trained model.\n",
    "    from transformers import AutoConfig\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
    "    \n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "print(f'The Bert model contains {model.num_parameters()} parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.51 s, sys: 848 ms, total: 6.36 s\n",
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "# Define a dataset.\n",
    "# Each output is the encoded row.\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=BertTokenizer,\n",
    "    file_path=\"./synthetic_X.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "# Define a data-collator. \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=BertTokenizer, mlm=True, mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, tmp_data in enumerate(dataset):\n",
    "#     print(idx)\n",
    "#     print(tmp_data)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Synthetic/\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacfabd370864e49b5d06148694116a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a419eea5caae49ffafc7a173e006a536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=6250.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CPU times: user 1min 44s, sys: 1.85 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=4.335336205024719)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./Synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/.local/lib/python3.7/site-packages/transformers/modeling_auto.py:792: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./Synthetic\",\n",
    "    tokenizer=BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 6 64 0 99 99 [SEP]',\n",
       "  'score': 0.012895897962152958,\n",
       "  'token': 15,\n",
       "  'token_str': '99'},\n",
       " {'sequence': '[CLS] 6 64 0 0 99 [SEP]',\n",
       "  'score': 0.012775693088769913,\n",
       "  'token': 5,\n",
       "  'token_str': '0'},\n",
       " {'sequence': '[CLS] 6 64 0 4 99 [SEP]',\n",
       "  'score': 0.011902562342584133,\n",
       "  'token': 9,\n",
       "  'token_str': '4'},\n",
       " {'sequence': '[CLS] 6 64 0 1 99 [SEP]',\n",
       "  'score': 0.011871114373207092,\n",
       "  'token': 6,\n",
       "  'token_str': '1'},\n",
       " {'sequence': '[CLS] 6 64 0 2 99 [SEP]',\n",
       "  'score': 0.011860175058245659,\n",
       "  'token': 7,\n",
       "  'token_str': '2'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the Bert can fill the mask at least.\n",
    "fill_mask(\"6 64 0 [MASK] 99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
