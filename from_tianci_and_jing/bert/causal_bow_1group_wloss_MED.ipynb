{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.binomial import Binomial\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from tokens import WordLevelBertTokenizer\n",
    "from vocab import create_vocab\n",
    "from data import CausalBertDataset, MLMDataset\n",
    "from causal_bert import CausalBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 256\n",
    "epoch = 500\n",
    "hidden_size = 64\n",
    "lr = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def true_casual_effect(data_loader, effect='ate', estimation='q'):\n",
    "    assert effect == 'ate' and estimation == 'q', f'unallowed effect/estimation: {effect}/{estimation}'\n",
    "    \n",
    "    dataset = data_loader.dataset\n",
    "    \n",
    "    Q1 = dataset.treatment * dataset.response + (1 - dataset.treatment) * dataset.pseudo_response\n",
    "    Q1 = Q1.cpu().data.numpy().squeeze()\n",
    "\n",
    "    Q0 = dataset.treatment * dataset.pseudo_response + (1 - dataset.treatment) * dataset.response\n",
    "    Q0 = Q0.cpu().data.numpy().squeeze()\n",
    "\n",
    "    treatment = dataset.treatment.cpu().data.numpy().squeeze()\n",
    "    prop_scores = dataset.prop_scores.cpu().data.numpy().squeeze()\n",
    "    \n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (treatment * (Q1 - Q0))\n",
    "            return phi.sum() / treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            return (Q1 - Q0).mean()\n",
    "        \n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            return phi / treatment.mean()\n",
    "        elif effect == 'ate': \n",
    "            return phi\n",
    "        \n",
    "def est_casual_effect(data_loader, model, effect='ate', estimation='q', evaluate=True, **kwargs):\n",
    "    # We use `real_treatment` here to emphasize the estimations use real instead of estimated treatment.\n",
    "    real_response, real_treatment, real_prop_scores = [], [], []\n",
    "    prop_scores, Q1, Q0 = [], [], []\n",
    "    \n",
    "    if evaluate:\n",
    "        g_loss = kwargs.get('g_loss')\n",
    "        q_loss = kwargs.get('q_loss')\n",
    "        g_loss_test, q1_loss_test, q0_loss_test  = [], [], []\n",
    "        \n",
    "    model.eval()\n",
    "    for idx, (tokens, treatment, response, real_prop_score) in enumerate(data_loader):\n",
    "        real_response.append(response.cpu().data.numpy().squeeze())\n",
    "        real_treatment.append(treatment.cpu().data.numpy().squeeze())\n",
    "        real_prop_scores.append(real_prop_score.cpu().data.numpy().squeeze())\n",
    "\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        prop_scores.append(prop_score.cpu().data.numpy().squeeze())\n",
    "        Q1.append(q1.cpu().data.numpy().squeeze())\n",
    "        Q0.append(q0.cpu().data.numpy().squeeze())\n",
    "        \n",
    "        # Evaulate loss\n",
    "        if evaluate:\n",
    "            g_loss_val  = g_loss(prop_score, treatment)\n",
    "            q1_loss_val = q_loss(q1[treatment==1], response[treatment==1])\n",
    "            q0_loss_val = q_loss(q0[treatment==0], response[treatment==0])\n",
    "            \n",
    "            g_loss_test.append(g_loss_val.item())\n",
    "            q1_loss_test.append(q1_loss_val.item())\n",
    "            q0_loss_test.append(q0_loss_val.item())\n",
    "    \n",
    "    g_loss = np.array(g_loss_test).mean() if evaluate else None\n",
    "    q1_loss = np.array(q1_loss_test).mean() if evaluate else None\n",
    "    q0_loss = np.array(q0_loss_test).mean() if evaluate else None\n",
    "\n",
    "    Q1 = np.concatenate(Q1, axis=0)\n",
    "    Q0 = np.concatenate(Q0, axis=0)\n",
    "    prop_scores = np.concatenate(prop_scores, axis=0)\n",
    "    \n",
    "    real_response = np.concatenate(real_response, axis=0)\n",
    "    real_treatment = np.concatenate(real_treatment, axis=0)\n",
    "    real_prop_scores = np.concatenate(real_prop_scores, axis=0)\n",
    "    \n",
    "    # Evaluate accuracy.\n",
    "    if evaluate:\n",
    "        dataset = data_loader.dataset\n",
    "        \n",
    "        real_q1_prob = sigmoid(dataset.alpha + dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        real_q0_prob = sigmoid(dataset.beta * (real_prop_scores - dataset.c) + dataset.i)\n",
    "        thre = (real_q1_prob + real_q0_prob) / 2\n",
    "\n",
    "    # prop score: real and estimated must locate one the same side of 0.5.\n",
    "    prop_accu = (1. * (((real_prop_scores - .5) * (prop_scores - .5)) > 0.)).mean() if evaluate else None\n",
    "    # q: estimate is more close to corresponding real value than the other.\n",
    "    q1_accu = (1. * (dataset.alpha > 0) * (Q1 > thre)).mean() if evaluate else None\n",
    "    q0_accu = (1. * (dataset.alpha > 0) * (Q0 < thre)).mean() if evaluate else None\n",
    "\n",
    "    if estimation == 'q':\n",
    "        if effect == 'att':\n",
    "            phi = (real_treatment * (Q1 - Q0))\n",
    "            effect = phi.sum() / real_treatment.sum()\n",
    "        elif effect == 'ate':\n",
    "            effect = (Q1 - Q0).mean()\n",
    "\n",
    "    elif estimation == 'plugin':\n",
    "        phi = (prop_scores * (Q1 - Q0)).mean()\n",
    "        if effect == 'att':\n",
    "            effect = phi / real_treatment.mean()\n",
    "        elif effect == 'ate':\n",
    "            effect = phi\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return effect, g_loss, q1_loss, q0_loss, prop_accu, q1_accu, q0_accu\n",
    "\n",
    "def show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch):\n",
    "    train_loss_hist = np.array(train_loss_hist)\n",
    "    test_loss_hist = np.array(test_loss_hist)\n",
    "    est_effect = np.array(est_effect)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    lns1 = ax.plot(np.arange(epoch), test_loss_hist, label='Eval loss')\n",
    "    ax_r = plt.twinx()\n",
    "    lns2 = ax_r.plot(np.arange(epoch), est_effect, color='coral', label='Estimate ATE')\n",
    "    lns3 = ax_r.plot(np.arange(epoch), np.ones(epoch) * real, color='red', ls='--', label='Real ATE')\n",
    "    lns4 = ax_r.plot(np.arange(epoch), np.ones(epoch) * unadjust, color='green', ls='--', label='Unadjusted ATE')\n",
    "\n",
    "    lns = lns1+lns2+lns3+lns4\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax_r.legend(lns, labs, loc=0)\n",
    "    ax.set_ylabel('Eval loss')\n",
    "    ax_r.set_ylabel('ATEs')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(merged=True, uni_diag=True)\n",
    "tokenizer = WordLevelBertTokenizer(vocab)\n",
    "\n",
    "alpha = 0.25\n",
    "beta = 5.\n",
    "c = 0.2\n",
    "i = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training set in 73.18 sec\n",
      "Load validation set in 66.24 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                             alpha=alpha, beta=beta, c=c, i=i, \n",
    "                             group=list(range(1)), max_length=512, min_length=10,\n",
    "                             truncate_method='first', device=device, seed=1)\n",
    "\n",
    "print(f'Load training set in {(time.time() - start):.2f} sec')\n",
    "\n",
    "start = time.time()\n",
    "testset = CausalBertDataset(tokenizer=tokenizer, data_type='merged', is_unidiag=True,\n",
    "                            alpha=alpha, beta=beta, c=c, i=i, \n",
    "                            group=[9], max_length=512, min_length=10,\n",
    "                            truncate_method='first', device=device)\n",
    "\n",
    "print(f'Load validation set in {(time.time() - start):.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=bsz, drop_last=True, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=2048, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [effect: ate], [estimation: q], [value: 0.06024]\n",
      "Unadjusted: [value: 0.1403]\n"
     ]
    }
   ],
   "source": [
    "real_att_q = true_casual_effect(test_loader)\n",
    "\n",
    "print(f'Real: [effect: ate], [estimation: q], [value: {real_att_q:.5f}]')\n",
    "print(f'Unadjusted: [value: {(testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item():.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bert = '/nfs/turbo/lsa-regier/bert-results/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "# trained_bert = '/home/liutianc/emr/bert/results/behrt/MLM/merged/unidiag/checkpoint-6018425/'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(trained_bert)\n",
    "token_embed = model.get_input_embeddings()\n",
    "model = CausalBOW(token_embed, learnable_docu_embed=False, hidden_size=hidden_size, prop_is_logit=True).to(device)\n",
    "\n",
    "pos_portion = trainset.treatment.mean()\n",
    "pos_weight = (1 - pos_portion) / pos_portion\n",
    "\n",
    "epoch_iter = len(train_loader)\n",
    "total_steps = epoch * epoch_iter\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "q_loss = nn.BCELoss()\n",
    "# prop_score_loss = nn.BCELoss()\n",
    "prop_score_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Please specify the effect and estimation we want to use here.\n",
    "effect = 'ate'\n",
    "estimation = 'q'\n",
    "\n",
    "effect = effect.lower()\n",
    "estimation = estimation.lower()\n",
    "assert effect in ['att', 'ate'], f'Wrong effect: {effect}...'\n",
    "assert estimation in ['q', 'plugin'], f'Wrong estimation: {estimation}...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 500, time cost: 53.14 sec, \n",
      "          Loss: [Train: 2.42089], [Test: 2.42048],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.06186], [test: 0.06186]\n",
      "********************************************************************************\n",
      "epoch: 2 / 500, time cost: 53.45 sec, \n",
      "          Loss: [Train: 2.41106], [Test: 2.41160],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.08378], [test: 0.08378]\n",
      "********************************************************************************\n",
      "epoch: 3 / 500, time cost: 50.29 sec, \n",
      "          Loss: [Train: 2.40345], [Test: 2.40617],\n",
      "          Accuracy: [prop score:  0.93655], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.10849], [test: 0.10848]\n",
      "********************************************************************************\n",
      "epoch: 4 / 500, time cost: 48.99 sec, \n",
      "          Loss: [Train: 2.39972], [Test: 2.40434],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.12577], [test: 0.12577]\n",
      "********************************************************************************\n",
      "epoch: 5 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.39819], [Test: 2.40371],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13421], [test: 0.13420]\n",
      "********************************************************************************\n",
      "epoch: 6 / 500, time cost: 45.42 sec, \n",
      "          Loss: [Train: 2.39778], [Test: 2.40361],\n",
      "          Accuracy: [prop score:  0.93648], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13794], [test: 0.13793]\n",
      "********************************************************************************\n",
      "epoch: 7 / 500, time cost: 47.95 sec, \n",
      "          Loss: [Train: 2.39755], [Test: 2.40330],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13960], [test: 0.13959]\n",
      "********************************************************************************\n",
      "epoch: 8 / 500, time cost: 45.38 sec, \n",
      "          Loss: [Train: 2.39694], [Test: 2.40269],\n",
      "          Accuracy: [prop score:  0.93646], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14200], [test: 0.14199]\n",
      "********************************************************************************\n",
      "epoch: 9 / 500, time cost: 47.97 sec, \n",
      "          Loss: [Train: 2.39710], [Test: 2.40273],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14140], [test: 0.14139]\n",
      "********************************************************************************\n",
      "epoch: 10 / 500, time cost: 44.73 sec, \n",
      "          Loss: [Train: 2.39687], [Test: 2.40256],\n",
      "          Accuracy: [prop score:  0.93642], [q1: 0.93642], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14044], [test: 0.14042]\n",
      "********************************************************************************\n",
      "epoch: 11 / 500, time cost: 46.99 sec, \n",
      "          Loss: [Train: 2.39650], [Test: 2.40227],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14013], [test: 0.14012]\n",
      "********************************************************************************\n",
      "epoch: 12 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.39604], [Test: 2.40224],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14035], [test: 0.14034]\n",
      "********************************************************************************\n",
      "epoch: 13 / 500, time cost: 47.61 sec, \n",
      "          Loss: [Train: 2.39623], [Test: 2.40209],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14068], [test: 0.14067]\n",
      "********************************************************************************\n",
      "epoch: 14 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.39592], [Test: 2.40209],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14039], [test: 0.14038]\n",
      "********************************************************************************\n",
      "epoch: 15 / 500, time cost: 47.53 sec, \n",
      "          Loss: [Train: 2.39571], [Test: 2.40183],\n",
      "          Accuracy: [prop score:  0.93641], [q1: 0.93641], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14073], [test: 0.14072]\n",
      "********************************************************************************\n",
      "epoch: 16 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.39563], [Test: 2.40138],\n",
      "          Accuracy: [prop score:  0.93653], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14001], [test: 0.13999]\n",
      "********************************************************************************\n",
      "epoch: 17 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.39503], [Test: 2.40150],\n",
      "          Accuracy: [prop score:  0.93657], [q1: 0.93657], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14149], [test: 0.14148]\n",
      "********************************************************************************\n",
      "epoch: 18 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.39488], [Test: 2.40115],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14178], [test: 0.14176]\n",
      "********************************************************************************\n",
      "epoch: 19 / 500, time cost: 53.40 sec, \n",
      "          Loss: [Train: 2.39472], [Test: 2.40079],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14032], [test: 0.14030]\n",
      "********************************************************************************\n",
      "epoch: 20 / 500, time cost: 52.20 sec, \n",
      "          Loss: [Train: 2.39446], [Test: 2.40084],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14026], [test: 0.14024]\n",
      "********************************************************************************\n",
      "epoch: 21 / 500, time cost: 51.08 sec, \n",
      "          Loss: [Train: 2.39404], [Test: 2.40021],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14083], [test: 0.14082]\n",
      "********************************************************************************\n",
      "epoch: 22 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.39401], [Test: 2.40030],\n",
      "          Accuracy: [prop score:  0.93649], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13944], [test: 0.13943]\n",
      "********************************************************************************\n",
      "epoch: 23 / 500, time cost: 46.99 sec, \n",
      "          Loss: [Train: 2.39384], [Test: 2.39990],\n",
      "          Accuracy: [prop score:  0.93647], [q1: 0.93647], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14069], [test: 0.14067]\n",
      "********************************************************************************\n",
      "epoch: 24 / 500, time cost: 44.92 sec, \n",
      "          Loss: [Train: 2.39340], [Test: 2.39985],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14013], [test: 0.14011]\n",
      "********************************************************************************\n",
      "epoch: 25 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.39325], [Test: 2.39937],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13896], [test: 0.13894]\n",
      "********************************************************************************\n",
      "epoch: 26 / 500, time cost: 49.17 sec, \n",
      "          Loss: [Train: 2.39284], [Test: 2.39941],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13894], [test: 0.13892]\n",
      "********************************************************************************\n",
      "epoch: 27 / 500, time cost: 47.32 sec, \n",
      "          Loss: [Train: 2.39237], [Test: 2.39888],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14088], [test: 0.14086]\n",
      "********************************************************************************\n",
      "epoch: 28 / 500, time cost: 46.05 sec, \n",
      "          Loss: [Train: 2.39239], [Test: 2.39884],\n",
      "          Accuracy: [prop score:  0.93644], [q1: 0.93644], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13982], [test: 0.13980]\n",
      "********************************************************************************\n",
      "epoch: 29 / 500, time cost: 48.25 sec, \n",
      "          Loss: [Train: 2.39199], [Test: 2.39797],\n",
      "          Accuracy: [prop score:  0.93651], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13951], [test: 0.13949]\n",
      "********************************************************************************\n",
      "epoch: 30 / 500, time cost: 46.46 sec, \n",
      "          Loss: [Train: 2.39144], [Test: 2.39792],\n",
      "          Accuracy: [prop score:  0.93652], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13891], [test: 0.13889]\n",
      "********************************************************************************\n",
      "epoch: 31 / 500, time cost: 49.88 sec, \n",
      "          Loss: [Train: 2.39104], [Test: 2.39778],\n",
      "          Accuracy: [prop score:  0.93650], [q1: 0.93649], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13991], [test: 0.13989]\n",
      "********************************************************************************\n",
      "epoch: 32 / 500, time cost: 56.06 sec, \n",
      "          Loss: [Train: 2.39079], [Test: 2.39748],\n",
      "          Accuracy: [prop score:  0.93648], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13963], [test: 0.13961]\n",
      "********************************************************************************\n",
      "epoch: 33 / 500, time cost: 50.09 sec, \n",
      "          Loss: [Train: 2.39057], [Test: 2.39706],\n",
      "          Accuracy: [prop score:  0.93649], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13917], [test: 0.13916]\n",
      "********************************************************************************\n",
      "epoch: 34 / 500, time cost: 50.73 sec, \n",
      "          Loss: [Train: 2.39002], [Test: 2.39687],\n",
      "          Accuracy: [prop score:  0.93657], [q1: 0.93651], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13869], [test: 0.13867]\n",
      "********************************************************************************\n",
      "epoch: 35 / 500, time cost: 48.82 sec, \n",
      "          Loss: [Train: 2.38972], [Test: 2.39648],\n",
      "          Accuracy: [prop score:  0.93662], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13980], [test: 0.13978]\n",
      "********************************************************************************\n",
      "epoch: 36 / 500, time cost: 46.82 sec, \n",
      "          Loss: [Train: 2.38935], [Test: 2.39635],\n",
      "          Accuracy: [prop score:  0.93660], [q1: 0.93650], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14002], [test: 0.14000]\n",
      "********************************************************************************\n",
      "epoch: 37 / 500, time cost: 48.86 sec, \n",
      "          Loss: [Train: 2.38861], [Test: 2.39583],\n",
      "          Accuracy: [prop score:  0.93657], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13965], [test: 0.13962]\n",
      "********************************************************************************\n",
      "epoch: 38 / 500, time cost: 46.28 sec, \n",
      "          Loss: [Train: 2.38843], [Test: 2.39520],\n",
      "          Accuracy: [prop score:  0.93661], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13896], [test: 0.13895]\n",
      "********************************************************************************\n",
      "epoch: 39 / 500, time cost: 46.70 sec, \n",
      "          Loss: [Train: 2.38789], [Test: 2.39483],\n",
      "          Accuracy: [prop score:  0.93672], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13896], [test: 0.13894]\n",
      "********************************************************************************\n",
      "epoch: 40 / 500, time cost: 45.86 sec, \n",
      "          Loss: [Train: 2.38746], [Test: 2.39473],\n",
      "          Accuracy: [prop score:  0.93668], [q1: 0.93645], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.14074], [test: 0.14072]\n",
      "********************************************************************************\n",
      "epoch: 41 / 500, time cost: 48.54 sec, \n",
      "          Loss: [Train: 2.38672], [Test: 2.39402],\n",
      "          Accuracy: [prop score:  0.93674], [q1: 0.93643], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13952], [test: 0.13951]\n",
      "********************************************************************************\n",
      "epoch: 42 / 500, time cost: 46.09 sec, \n",
      "          Loss: [Train: 2.38628], [Test: 2.39343],\n",
      "          Accuracy: [prop score:  0.93685], [q1: 0.93648], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13865], [test: 0.13864]\n",
      "********************************************************************************\n",
      "epoch: 43 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.38570], [Test: 2.39309],\n",
      "          Accuracy: [prop score:  0.93690], [q1: 0.93641], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13856], [test: 0.13855]\n",
      "********************************************************************************\n",
      "epoch: 44 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 2.38533], [Test: 2.39257],\n",
      "          Accuracy: [prop score:  0.93705], [q1: 0.93653], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13854], [test: 0.13853]\n",
      "********************************************************************************\n",
      "epoch: 45 / 500, time cost: 49.70 sec, \n",
      "          Loss: [Train: 2.38456], [Test: 2.39223],\n",
      "          Accuracy: [prop score:  0.93706], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13812], [test: 0.13812]\n",
      "********************************************************************************\n",
      "epoch: 46 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.38445], [Test: 2.39151],\n",
      "          Accuracy: [prop score:  0.93725], [q1: 0.93662], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13626], [test: 0.13625]\n",
      "********************************************************************************\n",
      "epoch: 47 / 500, time cost: 48.52 sec, \n",
      "          Loss: [Train: 2.38367], [Test: 2.39090],\n",
      "          Accuracy: [prop score:  0.93714], [q1: 0.93646], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13798], [test: 0.13797]\n",
      "********************************************************************************\n",
      "epoch: 48 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 2.38276], [Test: 2.39043],\n",
      "          Accuracy: [prop score:  0.93733], [q1: 0.93655], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13639], [test: 0.13638]\n",
      "********************************************************************************\n",
      "epoch: 49 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.38239], [Test: 2.38988],\n",
      "          Accuracy: [prop score:  0.93743], [q1: 0.93652], [q0: 1.00000],\n",
      "          Effect: [ate-q], [train: 0.13578], [test: 0.13578]\n",
      "********************************************************************************\n",
      "epoch: 50 / 500, time cost: 46.55 sec, \n",
      "          Loss: [Train: 2.38182], [Test: 2.38933],\n",
      "          Accuracy: [prop score:  0.93747], [q1: 0.93655], [q0: 0.99995],\n",
      "          Effect: [ate-q], [train: 0.13477], [test: 0.13478]\n",
      "********************************************************************************\n",
      "epoch: 51 / 500, time cost: 48.99 sec, \n",
      "          Loss: [Train: 2.38132], [Test: 2.38838],\n",
      "          Accuracy: [prop score:  0.93742], [q1: 0.93644], [q0: 0.99995],\n",
      "          Effect: [ate-q], [train: 0.13383], [test: 0.13384]\n",
      "********************************************************************************\n",
      "epoch: 52 / 500, time cost: 46.35 sec, \n",
      "          Loss: [Train: 2.38047], [Test: 2.38784],\n",
      "          Accuracy: [prop score:  0.93746], [q1: 0.93642], [q0: 0.99991],\n",
      "          Effect: [ate-q], [train: 0.13484], [test: 0.13485]\n",
      "********************************************************************************\n",
      "epoch: 53 / 500, time cost: 48.71 sec, \n",
      "          Loss: [Train: 2.37973], [Test: 2.38735],\n",
      "          Accuracy: [prop score:  0.93767], [q1: 0.93651], [q0: 0.99986],\n",
      "          Effect: [ate-q], [train: 0.13528], [test: 0.13529]\n",
      "********************************************************************************\n",
      "epoch: 54 / 500, time cost: 44.86 sec, \n",
      "          Loss: [Train: 2.37881], [Test: 2.38663],\n",
      "          Accuracy: [prop score:  0.93774], [q1: 0.93650], [q0: 0.99967],\n",
      "          Effect: [ate-q], [train: 0.13489], [test: 0.13490]\n",
      "********************************************************************************\n",
      "epoch: 55 / 500, time cost: 52.63 sec, \n",
      "          Loss: [Train: 2.37805], [Test: 2.38621],\n",
      "          Accuracy: [prop score:  0.93790], [q1: 0.93652], [q0: 0.99959],\n",
      "          Effect: [ate-q], [train: 0.13422], [test: 0.13423]\n",
      "********************************************************************************\n",
      "epoch: 56 / 500, time cost: 48.29 sec, \n",
      "          Loss: [Train: 2.37728], [Test: 2.38545],\n",
      "          Accuracy: [prop score:  0.93782], [q1: 0.93642], [q0: 0.99919],\n",
      "          Effect: [ate-q], [train: 0.13400], [test: 0.13402]\n",
      "********************************************************************************\n",
      "epoch: 57 / 500, time cost: 48.64 sec, \n",
      "          Loss: [Train: 2.37678], [Test: 2.38508],\n",
      "          Accuracy: [prop score:  0.93801], [q1: 0.93652], [q0: 0.99934],\n",
      "          Effect: [ate-q], [train: 0.13507], [test: 0.13510]\n",
      "********************************************************************************\n",
      "epoch: 58 / 500, time cost: 47.18 sec, \n",
      "          Loss: [Train: 2.37618], [Test: 2.38372],\n",
      "          Accuracy: [prop score:  0.93808], [q1: 0.93649], [q0: 0.99891],\n",
      "          Effect: [ate-q], [train: 0.13172], [test: 0.13176]\n",
      "********************************************************************************\n",
      "epoch: 59 / 500, time cost: 48.91 sec, \n",
      "          Loss: [Train: 2.37536], [Test: 2.38351],\n",
      "          Accuracy: [prop score:  0.93829], [q1: 0.93658], [q0: 0.99851],\n",
      "          Effect: [ate-q], [train: 0.13336], [test: 0.13340]\n",
      "********************************************************************************\n",
      "epoch: 60 / 500, time cost: 46.04 sec, \n",
      "          Loss: [Train: 2.37432], [Test: 2.38295],\n",
      "          Accuracy: [prop score:  0.93837], [q1: 0.93644], [q0: 0.99842],\n",
      "          Effect: [ate-q], [train: 0.13398], [test: 0.13402]\n",
      "********************************************************************************\n",
      "epoch: 61 / 500, time cost: 47.96 sec, \n",
      "          Loss: [Train: 2.37343], [Test: 2.38165],\n",
      "          Accuracy: [prop score:  0.93849], [q1: 0.93648], [q0: 0.99774],\n",
      "          Effect: [ate-q], [train: 0.13228], [test: 0.13232]\n",
      "********************************************************************************\n",
      "epoch: 62 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.37296], [Test: 2.38131],\n",
      "          Accuracy: [prop score:  0.93866], [q1: 0.93651], [q0: 0.99766],\n",
      "          Effect: [ate-q], [train: 0.13109], [test: 0.13113]\n",
      "********************************************************************************\n",
      "epoch: 63 / 500, time cost: 48.81 sec, \n",
      "          Loss: [Train: 2.37209], [Test: 2.38065],\n",
      "          Accuracy: [prop score:  0.93887], [q1: 0.93656], [q0: 0.99756],\n",
      "          Effect: [ate-q], [train: 0.13318], [test: 0.13323]\n",
      "********************************************************************************\n",
      "epoch: 64 / 500, time cost: 45.55 sec, \n",
      "          Loss: [Train: 2.37112], [Test: 2.37977],\n",
      "          Accuracy: [prop score:  0.93893], [q1: 0.93658], [q0: 0.99653],\n",
      "          Effect: [ate-q], [train: 0.12780], [test: 0.12785]\n",
      "********************************************************************************\n",
      "epoch: 65 / 500, time cost: 47.77 sec, \n",
      "          Loss: [Train: 2.37018], [Test: 2.37920],\n",
      "          Accuracy: [prop score:  0.93902], [q1: 0.93653], [q0: 0.99677],\n",
      "          Effect: [ate-q], [train: 0.12938], [test: 0.12945]\n",
      "********************************************************************************\n",
      "epoch: 66 / 500, time cost: 48.68 sec, \n",
      "          Loss: [Train: 2.36978], [Test: 2.37841],\n",
      "          Accuracy: [prop score:  0.93911], [q1: 0.93653], [q0: 0.99628],\n",
      "          Effect: [ate-q], [train: 0.13016], [test: 0.13022]\n",
      "********************************************************************************\n",
      "epoch: 67 / 500, time cost: 49.30 sec, \n",
      "          Loss: [Train: 2.36830], [Test: 2.37790],\n",
      "          Accuracy: [prop score:  0.93910], [q1: 0.93644], [q0: 0.99522],\n",
      "          Effect: [ate-q], [train: 0.12934], [test: 0.12942]\n",
      "********************************************************************************\n",
      "epoch: 68 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.36792], [Test: 2.37731],\n",
      "          Accuracy: [prop score:  0.93923], [q1: 0.93652], [q0: 0.99426],\n",
      "          Effect: [ate-q], [train: 0.13049], [test: 0.13057]\n",
      "********************************************************************************\n",
      "epoch: 69 / 500, time cost: 48.46 sec, \n",
      "          Loss: [Train: 2.36726], [Test: 2.37667],\n",
      "          Accuracy: [prop score:  0.93927], [q1: 0.93647], [q0: 0.99433],\n",
      "          Effect: [ate-q], [train: 0.12911], [test: 0.12919]\n",
      "********************************************************************************\n",
      "epoch: 70 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.36635], [Test: 2.37587],\n",
      "          Accuracy: [prop score:  0.93941], [q1: 0.93645], [q0: 0.99405],\n",
      "          Effect: [ate-q], [train: 0.13191], [test: 0.13199]\n",
      "********************************************************************************\n",
      "epoch: 71 / 500, time cost: 47.94 sec, \n",
      "          Loss: [Train: 2.36536], [Test: 2.37517],\n",
      "          Accuracy: [prop score:  0.93942], [q1: 0.93625], [q0: 0.99298],\n",
      "          Effect: [ate-q], [train: 0.12749], [test: 0.12758]\n",
      "********************************************************************************\n",
      "epoch: 72 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.36470], [Test: 2.37473],\n",
      "          Accuracy: [prop score:  0.93951], [q1: 0.93619], [q0: 0.99208],\n",
      "          Effect: [ate-q], [train: 0.12911], [test: 0.12920]\n",
      "********************************************************************************\n",
      "epoch: 73 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.36413], [Test: 2.37396],\n",
      "          Accuracy: [prop score:  0.93961], [q1: 0.93591], [q0: 0.99207],\n",
      "          Effect: [ate-q], [train: 0.12723], [test: 0.12734]\n",
      "********************************************************************************\n",
      "epoch: 74 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.36351], [Test: 2.37371],\n",
      "          Accuracy: [prop score:  0.93966], [q1: 0.93578], [q0: 0.99181],\n",
      "          Effect: [ate-q], [train: 0.12879], [test: 0.12888]\n",
      "********************************************************************************\n",
      "epoch: 75 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.36276], [Test: 2.37292],\n",
      "          Accuracy: [prop score:  0.93972], [q1: 0.93503], [q0: 0.99076],\n",
      "          Effect: [ate-q], [train: 0.12544], [test: 0.12554]\n",
      "********************************************************************************\n",
      "epoch: 76 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.36208], [Test: 2.37269],\n",
      "          Accuracy: [prop score:  0.93979], [q1: 0.93368], [q0: 0.99061],\n",
      "          Effect: [ate-q], [train: 0.12252], [test: 0.12264]\n",
      "********************************************************************************\n",
      "epoch: 77 / 500, time cost: 48.56 sec, \n",
      "          Loss: [Train: 2.36112], [Test: 2.37211],\n",
      "          Accuracy: [prop score:  0.93986], [q1: 0.93424], [q0: 0.98852],\n",
      "          Effect: [ate-q], [train: 0.12829], [test: 0.12841]\n",
      "********************************************************************************\n",
      "epoch: 78 / 500, time cost: 46.30 sec, \n",
      "          Loss: [Train: 2.36090], [Test: 2.37166],\n",
      "          Accuracy: [prop score:  0.93983], [q1: 0.93305], [q0: 0.98900],\n",
      "          Effect: [ate-q], [train: 0.12746], [test: 0.12760]\n",
      "********************************************************************************\n",
      "epoch: 79 / 500, time cost: 48.70 sec, \n",
      "          Loss: [Train: 2.36021], [Test: 2.37080],\n",
      "          Accuracy: [prop score:  0.94003], [q1: 0.93094], [q0: 0.98778],\n",
      "          Effect: [ate-q], [train: 0.12377], [test: 0.12390]\n",
      "********************************************************************************\n",
      "epoch: 80 / 500, time cost: 46.51 sec, \n",
      "          Loss: [Train: 2.35915], [Test: 2.37005],\n",
      "          Accuracy: [prop score:  0.94004], [q1: 0.92930], [q0: 0.98765],\n",
      "          Effect: [ate-q], [train: 0.12358], [test: 0.12372]\n",
      "********************************************************************************\n",
      "epoch: 81 / 500, time cost: 48.74 sec, \n",
      "          Loss: [Train: 2.35860], [Test: 2.36993],\n",
      "          Accuracy: [prop score:  0.94011], [q1: 0.92839], [q0: 0.98512],\n",
      "          Effect: [ate-q], [train: 0.12361], [test: 0.12376]\n",
      "********************************************************************************\n",
      "epoch: 82 / 500, time cost: 46.43 sec, \n",
      "          Loss: [Train: 2.35795], [Test: 2.36986],\n",
      "          Accuracy: [prop score:  0.94009], [q1: 0.92769], [q0: 0.98469],\n",
      "          Effect: [ate-q], [train: 0.12511], [test: 0.12527]\n",
      "********************************************************************************\n",
      "epoch: 83 / 500, time cost: 48.64 sec, \n",
      "          Loss: [Train: 2.35755], [Test: 2.36898],\n",
      "          Accuracy: [prop score:  0.94016], [q1: 0.92312], [q0: 0.98346],\n",
      "          Effect: [ate-q], [train: 0.12031], [test: 0.12046]\n",
      "********************************************************************************\n",
      "epoch: 84 / 500, time cost: 46.30 sec, \n",
      "          Loss: [Train: 2.35694], [Test: 2.36877],\n",
      "          Accuracy: [prop score:  0.94020], [q1: 0.92284], [q0: 0.98282],\n",
      "          Effect: [ate-q], [train: 0.12211], [test: 0.12226]\n",
      "********************************************************************************\n",
      "epoch: 85 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.35651], [Test: 2.36862],\n",
      "          Accuracy: [prop score:  0.94051], [q1: 0.92163], [q0: 0.98173],\n",
      "          Effect: [ate-q], [train: 0.12233], [test: 0.12248]\n",
      "********************************************************************************\n",
      "epoch: 86 / 500, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.35606], [Test: 2.36763],\n",
      "          Accuracy: [prop score:  0.94038], [q1: 0.91911], [q0: 0.97911],\n",
      "          Effect: [ate-q], [train: 0.12095], [test: 0.12110]\n",
      "********************************************************************************\n",
      "epoch: 87 / 500, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.35550], [Test: 2.36775],\n",
      "          Accuracy: [prop score:  0.94051], [q1: 0.91729], [q0: 0.97665],\n",
      "          Effect: [ate-q], [train: 0.12024], [test: 0.12041]\n",
      "********************************************************************************\n",
      "epoch: 88 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.35499], [Test: 2.36739],\n",
      "          Accuracy: [prop score:  0.94031], [q1: 0.91528], [q0: 0.98069],\n",
      "          Effect: [ate-q], [train: 0.12218], [test: 0.12237]\n",
      "********************************************************************************\n",
      "epoch: 89 / 500, time cost: 48.09 sec, \n",
      "          Loss: [Train: 2.35455], [Test: 2.36737],\n",
      "          Accuracy: [prop score:  0.94041], [q1: 0.91017], [q0: 0.97759],\n",
      "          Effect: [ate-q], [train: 0.11821], [test: 0.11837]\n",
      "********************************************************************************\n",
      "epoch: 90 / 500, time cost: 46.05 sec, \n",
      "          Loss: [Train: 2.35415], [Test: 2.36668],\n",
      "          Accuracy: [prop score:  0.94041], [q1: 0.90960], [q0: 0.97640],\n",
      "          Effect: [ate-q], [train: 0.11933], [test: 0.11951]\n",
      "********************************************************************************\n",
      "epoch: 91 / 500, time cost: 47.95 sec, \n",
      "          Loss: [Train: 2.35375], [Test: 2.36640],\n",
      "          Accuracy: [prop score:  0.94054], [q1: 0.90923], [q0: 0.97161],\n",
      "          Effect: [ate-q], [train: 0.11938], [test: 0.11956]\n",
      "********************************************************************************\n",
      "epoch: 92 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.35335], [Test: 2.36610],\n",
      "          Accuracy: [prop score:  0.94058], [q1: 0.90265], [q0: 0.97673],\n",
      "          Effect: [ate-q], [train: 0.11813], [test: 0.11832]\n",
      "********************************************************************************\n",
      "epoch: 93 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.35277], [Test: 2.36580],\n",
      "          Accuracy: [prop score:  0.94050], [q1: 0.90100], [q0: 0.97669],\n",
      "          Effect: [ate-q], [train: 0.11844], [test: 0.11861]\n",
      "********************************************************************************\n",
      "epoch: 94 / 500, time cost: 45.53 sec, \n",
      "          Loss: [Train: 2.35230], [Test: 2.36567],\n",
      "          Accuracy: [prop score:  0.94052], [q1: 0.89890], [q0: 0.97131],\n",
      "          Effect: [ate-q], [train: 0.11659], [test: 0.11678]\n",
      "********************************************************************************\n",
      "epoch: 95 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.35166], [Test: 2.36527],\n",
      "          Accuracy: [prop score:  0.94065], [q1: 0.89339], [q0: 0.97267],\n",
      "          Effect: [ate-q], [train: 0.11491], [test: 0.11510]\n",
      "********************************************************************************\n",
      "epoch: 96 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.35120], [Test: 2.36503],\n",
      "          Accuracy: [prop score:  0.94073], [q1: 0.89518], [q0: 0.96886],\n",
      "          Effect: [ate-q], [train: 0.11596], [test: 0.11615]\n",
      "********************************************************************************\n",
      "epoch: 97 / 500, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.35052], [Test: 2.36454],\n",
      "          Accuracy: [prop score:  0.94084], [q1: 0.89717], [q0: 0.96945],\n",
      "          Effect: [ate-q], [train: 0.11845], [test: 0.11862]\n",
      "********************************************************************************\n",
      "epoch: 98 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.35028], [Test: 2.36449],\n",
      "          Accuracy: [prop score:  0.94091], [q1: 0.89386], [q0: 0.96780],\n",
      "          Effect: [ate-q], [train: 0.11734], [test: 0.11754]\n",
      "********************************************************************************\n",
      "epoch: 99 / 500, time cost: 48.43 sec, \n",
      "          Loss: [Train: 2.34988], [Test: 2.36378],\n",
      "          Accuracy: [prop score:  0.94101], [q1: 0.88950], [q0: 0.96826],\n",
      "          Effect: [ate-q], [train: 0.11622], [test: 0.11643]\n",
      "********************************************************************************\n",
      "epoch: 100 / 500, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.34968], [Test: 2.36407],\n",
      "          Accuracy: [prop score:  0.94112], [q1: 0.89175], [q0: 0.96913],\n",
      "          Effect: [ate-q], [train: 0.11894], [test: 0.11914]\n",
      "********************************************************************************\n",
      "epoch: 101 / 500, time cost: 48.29 sec, \n",
      "          Loss: [Train: 2.34917], [Test: 2.36357],\n",
      "          Accuracy: [prop score:  0.94104], [q1: 0.88176], [q0: 0.96788],\n",
      "          Effect: [ate-q], [train: 0.11400], [test: 0.11418]\n",
      "********************************************************************************\n",
      "epoch: 102 / 500, time cost: 52.12 sec, \n",
      "          Loss: [Train: 2.34863], [Test: 2.36352],\n",
      "          Accuracy: [prop score:  0.94107], [q1: 0.88401], [q0: 0.96769],\n",
      "          Effect: [ate-q], [train: 0.11613], [test: 0.11636]\n",
      "********************************************************************************\n",
      "epoch: 103 / 500, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.34839], [Test: 2.36322],\n",
      "          Accuracy: [prop score:  0.94116], [q1: 0.88232], [q0: 0.97196],\n",
      "          Effect: [ate-q], [train: 0.11773], [test: 0.11792]\n",
      "********************************************************************************\n",
      "epoch: 104 / 500, time cost: 46.15 sec, \n",
      "          Loss: [Train: 2.34771], [Test: 2.36237],\n",
      "          Accuracy: [prop score:  0.94117], [q1: 0.87643], [q0: 0.96861],\n",
      "          Effect: [ate-q], [train: 0.11464], [test: 0.11486]\n",
      "********************************************************************************\n",
      "epoch: 105 / 500, time cost: 48.05 sec, \n",
      "          Loss: [Train: 2.34727], [Test: 2.36265],\n",
      "          Accuracy: [prop score:  0.94126], [q1: 0.88455], [q0: 0.96438],\n",
      "          Effect: [ate-q], [train: 0.11810], [test: 0.11833]\n",
      "********************************************************************************\n",
      "epoch: 106 / 500, time cost: 45.87 sec, \n",
      "          Loss: [Train: 2.34743], [Test: 2.36176],\n",
      "          Accuracy: [prop score:  0.94124], [q1: 0.88214], [q0: 0.96347],\n",
      "          Effect: [ate-q], [train: 0.11720], [test: 0.11742]\n",
      "********************************************************************************\n",
      "epoch: 107 / 500, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.34682], [Test: 2.36197],\n",
      "          Accuracy: [prop score:  0.94141], [q1: 0.86901], [q0: 0.96170],\n",
      "          Effect: [ate-q], [train: 0.11131], [test: 0.11152]\n",
      "********************************************************************************\n",
      "epoch: 108 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 2.34600], [Test: 2.36167],\n",
      "          Accuracy: [prop score:  0.94140], [q1: 0.87563], [q0: 0.95808],\n",
      "          Effect: [ate-q], [train: 0.11387], [test: 0.11411]\n",
      "********************************************************************************\n",
      "epoch: 109 / 500, time cost: 47.52 sec, \n",
      "          Loss: [Train: 2.34606], [Test: 2.36189],\n",
      "          Accuracy: [prop score:  0.94133], [q1: 0.87004], [q0: 0.96242],\n",
      "          Effect: [ate-q], [train: 0.11344], [test: 0.11368]\n",
      "********************************************************************************\n",
      "epoch: 110 / 500, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.34567], [Test: 2.36092],\n",
      "          Accuracy: [prop score:  0.94151], [q1: 0.86873], [q0: 0.96505],\n",
      "          Effect: [ate-q], [train: 0.11433], [test: 0.11456]\n",
      "********************************************************************************\n",
      "epoch: 111 / 500, time cost: 47.74 sec, \n",
      "          Loss: [Train: 2.34538], [Test: 2.36088],\n",
      "          Accuracy: [prop score:  0.94147], [q1: 0.86645], [q0: 0.96013],\n",
      "          Effect: [ate-q], [train: 0.11264], [test: 0.11287]\n",
      "********************************************************************************\n",
      "epoch: 112 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.34469], [Test: 2.36096],\n",
      "          Accuracy: [prop score:  0.94156], [q1: 0.86405], [q0: 0.95607],\n",
      "          Effect: [ate-q], [train: 0.11099], [test: 0.11123]\n",
      "********************************************************************************\n",
      "epoch: 113 / 500, time cost: 47.91 sec, \n",
      "          Loss: [Train: 2.34471], [Test: 2.36057],\n",
      "          Accuracy: [prop score:  0.94158], [q1: 0.85978], [q0: 0.96096],\n",
      "          Effect: [ate-q], [train: 0.11135], [test: 0.11157]\n",
      "********************************************************************************\n",
      "epoch: 114 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.34419], [Test: 2.36064],\n",
      "          Accuracy: [prop score:  0.94163], [q1: 0.86431], [q0: 0.95177],\n",
      "          Effect: [ate-q], [train: 0.11119], [test: 0.11143]\n",
      "********************************************************************************\n",
      "epoch: 115 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.34357], [Test: 2.36036],\n",
      "          Accuracy: [prop score:  0.94171], [q1: 0.86141], [q0: 0.95264],\n",
      "          Effect: [ate-q], [train: 0.11066], [test: 0.11090]\n",
      "********************************************************************************\n",
      "epoch: 116 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.34352], [Test: 2.35975],\n",
      "          Accuracy: [prop score:  0.94172], [q1: 0.86442], [q0: 0.95941],\n",
      "          Effect: [ate-q], [train: 0.11452], [test: 0.11475]\n",
      "********************************************************************************\n",
      "epoch: 117 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.34283], [Test: 2.35996],\n",
      "          Accuracy: [prop score:  0.94187], [q1: 0.86792], [q0: 0.95357],\n",
      "          Effect: [ate-q], [train: 0.11491], [test: 0.11517]\n",
      "********************************************************************************\n",
      "epoch: 118 / 500, time cost: 46.28 sec, \n",
      "          Loss: [Train: 2.34230], [Test: 2.35961],\n",
      "          Accuracy: [prop score:  0.94187], [q1: 0.87159], [q0: 0.95228],\n",
      "          Effect: [ate-q], [train: 0.11654], [test: 0.11678]\n",
      "********************************************************************************\n",
      "epoch: 119 / 500, time cost: 48.43 sec, \n",
      "          Loss: [Train: 2.34234], [Test: 2.35914],\n",
      "          Accuracy: [prop score:  0.94195], [q1: 0.86012], [q0: 0.95549],\n",
      "          Effect: [ate-q], [train: 0.11319], [test: 0.11345]\n",
      "********************************************************************************\n",
      "epoch: 120 / 500, time cost: 46.54 sec, \n",
      "          Loss: [Train: 2.34188], [Test: 2.35920],\n",
      "          Accuracy: [prop score:  0.94197], [q1: 0.87100], [q0: 0.95060],\n",
      "          Effect: [ate-q], [train: 0.11683], [test: 0.11708]\n",
      "********************************************************************************\n",
      "epoch: 121 / 500, time cost: 48.52 sec, \n",
      "          Loss: [Train: 2.34138], [Test: 2.35915],\n",
      "          Accuracy: [prop score:  0.94203], [q1: 0.86738], [q0: 0.95266],\n",
      "          Effect: [ate-q], [train: 0.11633], [test: 0.11655]\n",
      "********************************************************************************\n",
      "epoch: 122 / 500, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.34082], [Test: 2.35854],\n",
      "          Accuracy: [prop score:  0.94216], [q1: 0.85423], [q0: 0.94223],\n",
      "          Effect: [ate-q], [train: 0.10913], [test: 0.10938]\n",
      "********************************************************************************\n",
      "epoch: 123 / 500, time cost: 48.14 sec, \n",
      "          Loss: [Train: 2.34053], [Test: 2.35830],\n",
      "          Accuracy: [prop score:  0.94216], [q1: 0.86211], [q0: 0.95139],\n",
      "          Effect: [ate-q], [train: 0.11475], [test: 0.11500]\n",
      "********************************************************************************\n",
      "epoch: 124 / 500, time cost: 46.08 sec, \n",
      "          Loss: [Train: 2.34017], [Test: 2.35855],\n",
      "          Accuracy: [prop score:  0.94222], [q1: 0.85939], [q0: 0.94876],\n",
      "          Effect: [ate-q], [train: 0.11356], [test: 0.11382]\n",
      "********************************************************************************\n",
      "epoch: 125 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.33971], [Test: 2.35840],\n",
      "          Accuracy: [prop score:  0.94242], [q1: 0.85362], [q0: 0.95389],\n",
      "          Effect: [ate-q], [train: 0.11323], [test: 0.11348]\n",
      "********************************************************************************\n",
      "epoch: 126 / 500, time cost: 45.97 sec, \n",
      "          Loss: [Train: 2.33973], [Test: 2.35761],\n",
      "          Accuracy: [prop score:  0.94247], [q1: 0.85689], [q0: 0.94946],\n",
      "          Effect: [ate-q], [train: 0.11357], [test: 0.11384]\n",
      "********************************************************************************\n",
      "epoch: 127 / 500, time cost: 48.05 sec, \n",
      "          Loss: [Train: 2.33911], [Test: 2.35779],\n",
      "          Accuracy: [prop score:  0.94242], [q1: 0.84923], [q0: 0.94714],\n",
      "          Effect: [ate-q], [train: 0.11073], [test: 0.11099]\n",
      "********************************************************************************\n",
      "epoch: 128 / 500, time cost: 46.15 sec, \n",
      "          Loss: [Train: 2.33904], [Test: 2.35768],\n",
      "          Accuracy: [prop score:  0.94240], [q1: 0.85335], [q0: 0.94126],\n",
      "          Effect: [ate-q], [train: 0.11118], [test: 0.11147]\n",
      "********************************************************************************\n",
      "epoch: 129 / 500, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.33838], [Test: 2.35749],\n",
      "          Accuracy: [prop score:  0.94250], [q1: 0.85266], [q0: 0.94495],\n",
      "          Effect: [ate-q], [train: 0.11224], [test: 0.11249]\n",
      "********************************************************************************\n",
      "epoch: 130 / 500, time cost: 45.43 sec, \n",
      "          Loss: [Train: 2.33794], [Test: 2.35708],\n",
      "          Accuracy: [prop score:  0.94255], [q1: 0.85849], [q0: 0.94631],\n",
      "          Effect: [ate-q], [train: 0.11497], [test: 0.11521]\n",
      "********************************************************************************\n",
      "epoch: 131 / 500, time cost: 48.12 sec, \n",
      "          Loss: [Train: 2.33750], [Test: 2.35708],\n",
      "          Accuracy: [prop score:  0.94269], [q1: 0.83563], [q0: 0.94459],\n",
      "          Effect: [ate-q], [train: 0.10677], [test: 0.10704]\n",
      "********************************************************************************\n",
      "epoch: 132 / 500, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.33712], [Test: 2.35671],\n",
      "          Accuracy: [prop score:  0.94272], [q1: 0.84395], [q0: 0.94540],\n",
      "          Effect: [ate-q], [train: 0.10990], [test: 0.11017]\n",
      "********************************************************************************\n",
      "epoch: 133 / 500, time cost: 48.07 sec, \n",
      "          Loss: [Train: 2.33707], [Test: 2.35685],\n",
      "          Accuracy: [prop score:  0.94281], [q1: 0.84158], [q0: 0.94219],\n",
      "          Effect: [ate-q], [train: 0.10867], [test: 0.10893]\n",
      "********************************************************************************\n",
      "epoch: 134 / 500, time cost: 45.99 sec, \n",
      "          Loss: [Train: 2.33644], [Test: 2.35649],\n",
      "          Accuracy: [prop score:  0.94290], [q1: 0.84536], [q0: 0.94265],\n",
      "          Effect: [ate-q], [train: 0.11016], [test: 0.11042]\n",
      "********************************************************************************\n",
      "epoch: 135 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.33621], [Test: 2.35631],\n",
      "          Accuracy: [prop score:  0.94290], [q1: 0.84770], [q0: 0.94080],\n",
      "          Effect: [ate-q], [train: 0.11108], [test: 0.11134]\n",
      "********************************************************************************\n",
      "epoch: 136 / 500, time cost: 46.00 sec, \n",
      "          Loss: [Train: 2.33554], [Test: 2.35613],\n",
      "          Accuracy: [prop score:  0.94298], [q1: 0.84793], [q0: 0.94724],\n",
      "          Effect: [ate-q], [train: 0.11281], [test: 0.11312]\n",
      "********************************************************************************\n",
      "epoch: 137 / 500, time cost: 48.52 sec, \n",
      "          Loss: [Train: 2.33532], [Test: 2.35605],\n",
      "          Accuracy: [prop score:  0.94319], [q1: 0.84459], [q0: 0.93814],\n",
      "          Effect: [ate-q], [train: 0.10959], [test: 0.10985]\n",
      "********************************************************************************\n",
      "epoch: 138 / 500, time cost: 52.14 sec, \n",
      "          Loss: [Train: 2.33546], [Test: 2.35586],\n",
      "          Accuracy: [prop score:  0.94314], [q1: 0.84680], [q0: 0.93875],\n",
      "          Effect: [ate-q], [train: 0.11066], [test: 0.11094]\n",
      "********************************************************************************\n",
      "epoch: 139 / 500, time cost: 53.22 sec, \n",
      "          Loss: [Train: 2.33479], [Test: 2.35531],\n",
      "          Accuracy: [prop score:  0.94324], [q1: 0.84855], [q0: 0.93908],\n",
      "          Effect: [ate-q], [train: 0.11182], [test: 0.11211]\n",
      "********************************************************************************\n",
      "epoch: 140 / 500, time cost: 48.70 sec, \n",
      "          Loss: [Train: 2.33466], [Test: 2.35529],\n",
      "          Accuracy: [prop score:  0.94334], [q1: 0.83155], [q0: 0.94375],\n",
      "          Effect: [ate-q], [train: 0.10726], [test: 0.10757]\n",
      "********************************************************************************\n",
      "epoch: 141 / 500, time cost: 49.05 sec, \n",
      "          Loss: [Train: 2.33372], [Test: 2.35519],\n",
      "          Accuracy: [prop score:  0.94340], [q1: 0.83810], [q0: 0.94403],\n",
      "          Effect: [ate-q], [train: 0.10963], [test: 0.10992]\n",
      "********************************************************************************\n",
      "epoch: 142 / 500, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.33396], [Test: 2.35500],\n",
      "          Accuracy: [prop score:  0.94328], [q1: 0.83476], [q0: 0.93409],\n",
      "          Effect: [ate-q], [train: 0.10623], [test: 0.10654]\n",
      "********************************************************************************\n",
      "epoch: 143 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.33338], [Test: 2.35447],\n",
      "          Accuracy: [prop score:  0.94338], [q1: 0.84324], [q0: 0.94891],\n",
      "          Effect: [ate-q], [train: 0.11301], [test: 0.11332]\n",
      "********************************************************************************\n",
      "epoch: 144 / 500, time cost: 46.24 sec, \n",
      "          Loss: [Train: 2.33300], [Test: 2.35466],\n",
      "          Accuracy: [prop score:  0.94351], [q1: 0.84170], [q0: 0.94020],\n",
      "          Effect: [ate-q], [train: 0.11020], [test: 0.11049]\n",
      "********************************************************************************\n",
      "epoch: 145 / 500, time cost: 47.99 sec, \n",
      "          Loss: [Train: 2.33262], [Test: 2.35489],\n",
      "          Accuracy: [prop score:  0.94355], [q1: 0.82577], [q0: 0.91880],\n",
      "          Effect: [ate-q], [train: 0.10050], [test: 0.10082]\n",
      "********************************************************************************\n",
      "epoch: 146 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.33233], [Test: 2.35454],\n",
      "          Accuracy: [prop score:  0.94353], [q1: 0.83963], [q0: 0.93451],\n",
      "          Effect: [ate-q], [train: 0.10837], [test: 0.10870]\n",
      "********************************************************************************\n",
      "epoch: 147 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.33201], [Test: 2.35409],\n",
      "          Accuracy: [prop score:  0.94368], [q1: 0.83448], [q0: 0.93941],\n",
      "          Effect: [ate-q], [train: 0.10800], [test: 0.10830]\n",
      "********************************************************************************\n",
      "epoch: 148 / 500, time cost: 46.66 sec, \n",
      "          Loss: [Train: 2.33161], [Test: 2.35404],\n",
      "          Accuracy: [prop score:  0.94369], [q1: 0.83116], [q0: 0.93818],\n",
      "          Effect: [ate-q], [train: 0.10676], [test: 0.10705]\n",
      "********************************************************************************\n",
      "epoch: 149 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.33096], [Test: 2.35376],\n",
      "          Accuracy: [prop score:  0.94372], [q1: 0.84555], [q0: 0.93902],\n",
      "          Effect: [ate-q], [train: 0.11204], [test: 0.11235]\n",
      "********************************************************************************\n",
      "epoch: 150 / 500, time cost: 46.79 sec, \n",
      "          Loss: [Train: 2.33075], [Test: 2.35371],\n",
      "          Accuracy: [prop score:  0.94395], [q1: 0.84643], [q0: 0.94639],\n",
      "          Effect: [ate-q], [train: 0.11463], [test: 0.11495]\n",
      "********************************************************************************\n",
      "epoch: 151 / 500, time cost: 48.97 sec, \n",
      "          Loss: [Train: 2.33058], [Test: 2.35337],\n",
      "          Accuracy: [prop score:  0.94385], [q1: 0.83694], [q0: 0.94570],\n",
      "          Effect: [ate-q], [train: 0.11140], [test: 0.11173]\n",
      "********************************************************************************\n",
      "epoch: 152 / 500, time cost: 46.81 sec, \n",
      "          Loss: [Train: 2.33018], [Test: 2.35336],\n",
      "          Accuracy: [prop score:  0.94401], [q1: 0.82344], [q0: 0.93621],\n",
      "          Effect: [ate-q], [train: 0.10439], [test: 0.10472]\n",
      "********************************************************************************\n",
      "epoch: 153 / 500, time cost: 48.62 sec, \n",
      "          Loss: [Train: 2.32984], [Test: 2.35339],\n",
      "          Accuracy: [prop score:  0.94399], [q1: 0.84136], [q0: 0.91640],\n",
      "          Effect: [ate-q], [train: 0.10666], [test: 0.10698]\n",
      "********************************************************************************\n",
      "epoch: 154 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.32946], [Test: 2.35323],\n",
      "          Accuracy: [prop score:  0.94399], [q1: 0.84092], [q0: 0.93555],\n",
      "          Effect: [ate-q], [train: 0.11059], [test: 0.11092]\n",
      "********************************************************************************\n",
      "epoch: 155 / 500, time cost: 48.69 sec, \n",
      "          Loss: [Train: 2.32920], [Test: 2.35307],\n",
      "          Accuracy: [prop score:  0.94410], [q1: 0.83370], [q0: 0.93744],\n",
      "          Effect: [ate-q], [train: 0.10853], [test: 0.10884]\n",
      "********************************************************************************\n",
      "epoch: 156 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.32910], [Test: 2.35298],\n",
      "          Accuracy: [prop score:  0.94418], [q1: 0.81843], [q0: 0.92107],\n",
      "          Effect: [ate-q], [train: 0.10048], [test: 0.10078]\n",
      "********************************************************************************\n",
      "epoch: 157 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.32851], [Test: 2.35258],\n",
      "          Accuracy: [prop score:  0.94419], [q1: 0.82903], [q0: 0.93042],\n",
      "          Effect: [ate-q], [train: 0.10587], [test: 0.10620]\n",
      "********************************************************************************\n",
      "epoch: 158 / 500, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.32815], [Test: 2.35233],\n",
      "          Accuracy: [prop score:  0.94423], [q1: 0.82171], [q0: 0.92961],\n",
      "          Effect: [ate-q], [train: 0.10355], [test: 0.10392]\n",
      "********************************************************************************\n",
      "epoch: 159 / 500, time cost: 48.62 sec, \n",
      "          Loss: [Train: 2.32776], [Test: 2.35272],\n",
      "          Accuracy: [prop score:  0.94429], [q1: 0.82967], [q0: 0.92925],\n",
      "          Effect: [ate-q], [train: 0.10599], [test: 0.10633]\n",
      "********************************************************************************\n",
      "epoch: 160 / 500, time cost: 53.41 sec, \n",
      "          Loss: [Train: 2.32776], [Test: 2.35262],\n",
      "          Accuracy: [prop score:  0.94435], [q1: 0.82299], [q0: 0.92203],\n",
      "          Effect: [ate-q], [train: 0.10269], [test: 0.10303]\n",
      "********************************************************************************\n",
      "epoch: 161 / 500, time cost: 50.62 sec, \n",
      "          Loss: [Train: 2.32701], [Test: 2.35220],\n",
      "          Accuracy: [prop score:  0.94437], [q1: 0.82944], [q0: 0.93480],\n",
      "          Effect: [ate-q], [train: 0.10763], [test: 0.10798]\n",
      "********************************************************************************\n",
      "epoch: 162 / 500, time cost: 46.35 sec, \n",
      "          Loss: [Train: 2.32678], [Test: 2.35225],\n",
      "          Accuracy: [prop score:  0.94444], [q1: 0.83377], [q0: 0.93046],\n",
      "          Effect: [ate-q], [train: 0.10835], [test: 0.10868]\n",
      "********************************************************************************\n",
      "epoch: 163 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.32671], [Test: 2.35227],\n",
      "          Accuracy: [prop score:  0.94445], [q1: 0.82097], [q0: 0.92866],\n",
      "          Effect: [ate-q], [train: 0.10380], [test: 0.10416]\n",
      "********************************************************************************\n",
      "epoch: 164 / 500, time cost: 46.57 sec, \n",
      "          Loss: [Train: 2.32624], [Test: 2.35163],\n",
      "          Accuracy: [prop score:  0.94469], [q1: 0.82133], [q0: 0.93980],\n",
      "          Effect: [ate-q], [train: 0.10691], [test: 0.10726]\n",
      "********************************************************************************\n",
      "epoch: 165 / 500, time cost: 48.71 sec, \n",
      "          Loss: [Train: 2.32568], [Test: 2.35176],\n",
      "          Accuracy: [prop score:  0.94470], [q1: 0.83506], [q0: 0.93005],\n",
      "          Effect: [ate-q], [train: 0.10916], [test: 0.10950]\n",
      "********************************************************************************\n",
      "epoch: 166 / 500, time cost: 46.87 sec, \n",
      "          Loss: [Train: 2.32558], [Test: 2.35142],\n",
      "          Accuracy: [prop score:  0.94479], [q1: 0.82368], [q0: 0.93055],\n",
      "          Effect: [ate-q], [train: 0.10568], [test: 0.10605]\n",
      "********************************************************************************\n",
      "epoch: 167 / 500, time cost: 48.76 sec, \n",
      "          Loss: [Train: 2.32510], [Test: 2.35091],\n",
      "          Accuracy: [prop score:  0.94487], [q1: 0.81884], [q0: 0.93669],\n",
      "          Effect: [ate-q], [train: 0.10585], [test: 0.10624]\n",
      "********************************************************************************\n",
      "epoch: 168 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.32449], [Test: 2.35141],\n",
      "          Accuracy: [prop score:  0.94486], [q1: 0.81877], [q0: 0.93220],\n",
      "          Effect: [ate-q], [train: 0.10485], [test: 0.10521]\n",
      "********************************************************************************\n",
      "epoch: 169 / 500, time cost: 48.46 sec, \n",
      "          Loss: [Train: 2.32474], [Test: 2.35112],\n",
      "          Accuracy: [prop score:  0.94499], [q1: 0.82295], [q0: 0.93021],\n",
      "          Effect: [ate-q], [train: 0.10583], [test: 0.10619]\n",
      "********************************************************************************\n",
      "epoch: 170 / 500, time cost: 46.34 sec, \n",
      "          Loss: [Train: 2.32382], [Test: 2.35117],\n",
      "          Accuracy: [prop score:  0.94489], [q1: 0.80930], [q0: 0.92918],\n",
      "          Effect: [ate-q], [train: 0.10145], [test: 0.10184]\n",
      "********************************************************************************\n",
      "epoch: 171 / 500, time cost: 48.35 sec, \n",
      "          Loss: [Train: 2.32388], [Test: 2.35097],\n",
      "          Accuracy: [prop score:  0.94501], [q1: 0.79626], [q0: 0.93662],\n",
      "          Effect: [ate-q], [train: 0.09967], [test: 0.10003]\n",
      "********************************************************************************\n",
      "epoch: 172 / 500, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.32332], [Test: 2.35047],\n",
      "          Accuracy: [prop score:  0.94505], [q1: 0.81486], [q0: 0.91748],\n",
      "          Effect: [ate-q], [train: 0.10093], [test: 0.10131]\n",
      "********************************************************************************\n",
      "epoch: 173 / 500, time cost: 48.49 sec, \n",
      "          Loss: [Train: 2.32274], [Test: 2.35063],\n",
      "          Accuracy: [prop score:  0.94511], [q1: 0.82816], [q0: 0.93103],\n",
      "          Effect: [ate-q], [train: 0.10839], [test: 0.10875]\n",
      "********************************************************************************\n",
      "epoch: 174 / 500, time cost: 45.76 sec, \n",
      "          Loss: [Train: 2.32257], [Test: 2.35060],\n",
      "          Accuracy: [prop score:  0.94518], [q1: 0.81222], [q0: 0.92157],\n",
      "          Effect: [ate-q], [train: 0.10123], [test: 0.10162]\n",
      "********************************************************************************\n",
      "epoch: 175 / 500, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.32225], [Test: 2.35052],\n",
      "          Accuracy: [prop score:  0.94522], [q1: 0.81772], [q0: 0.92726],\n",
      "          Effect: [ate-q], [train: 0.10431], [test: 0.10471]\n",
      "********************************************************************************\n",
      "epoch: 176 / 500, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.32196], [Test: 2.35064],\n",
      "          Accuracy: [prop score:  0.94526], [q1: 0.80824], [q0: 0.92575],\n",
      "          Effect: [ate-q], [train: 0.10118], [test: 0.10157]\n",
      "********************************************************************************\n",
      "epoch: 177 / 500, time cost: 48.28 sec, \n",
      "          Loss: [Train: 2.32130], [Test: 2.34996],\n",
      "          Accuracy: [prop score:  0.94535], [q1: 0.81697], [q0: 0.92891],\n",
      "          Effect: [ate-q], [train: 0.10485], [test: 0.10524]\n",
      "********************************************************************************\n",
      "epoch: 178 / 500, time cost: 45.88 sec, \n",
      "          Loss: [Train: 2.32086], [Test: 2.35002],\n",
      "          Accuracy: [prop score:  0.94529], [q1: 0.78530], [q0: 0.92023],\n",
      "          Effect: [ate-q], [train: 0.09411], [test: 0.09454]\n",
      "********************************************************************************\n",
      "epoch: 179 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.32070], [Test: 2.35002],\n",
      "          Accuracy: [prop score:  0.94554], [q1: 0.82329], [q0: 0.93391],\n",
      "          Effect: [ate-q], [train: 0.10856], [test: 0.10895]\n",
      "********************************************************************************\n",
      "epoch: 180 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.32081], [Test: 2.35004],\n",
      "          Accuracy: [prop score:  0.94544], [q1: 0.81641], [q0: 0.93560],\n",
      "          Effect: [ate-q], [train: 0.10681], [test: 0.10721]\n",
      "********************************************************************************\n",
      "epoch: 181 / 500, time cost: 48.38 sec, \n",
      "          Loss: [Train: 2.32030], [Test: 2.34945],\n",
      "          Accuracy: [prop score:  0.94551], [q1: 0.79655], [q0: 0.92459],\n",
      "          Effect: [ate-q], [train: 0.09828], [test: 0.09869]\n",
      "********************************************************************************\n",
      "epoch: 182 / 500, time cost: 46.16 sec, \n",
      "          Loss: [Train: 2.31987], [Test: 2.34953],\n",
      "          Accuracy: [prop score:  0.94556], [q1: 0.81616], [q0: 0.92699],\n",
      "          Effect: [ate-q], [train: 0.10488], [test: 0.10531]\n",
      "********************************************************************************\n",
      "epoch: 183 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.31945], [Test: 2.34958],\n",
      "          Accuracy: [prop score:  0.94567], [q1: 0.80466], [q0: 0.93348],\n",
      "          Effect: [ate-q], [train: 0.10308], [test: 0.10351]\n",
      "********************************************************************************\n",
      "epoch: 184 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.31885], [Test: 2.34968],\n",
      "          Accuracy: [prop score:  0.94555], [q1: 0.81066], [q0: 0.93669],\n",
      "          Effect: [ate-q], [train: 0.10591], [test: 0.10634]\n",
      "********************************************************************************\n",
      "epoch: 185 / 500, time cost: 48.74 sec, \n",
      "          Loss: [Train: 2.31867], [Test: 2.34965],\n",
      "          Accuracy: [prop score:  0.94583], [q1: 0.81236], [q0: 0.92596],\n",
      "          Effect: [ate-q], [train: 0.10389], [test: 0.10430]\n",
      "********************************************************************************\n",
      "epoch: 186 / 500, time cost: 46.24 sec, \n",
      "          Loss: [Train: 2.31865], [Test: 2.34912],\n",
      "          Accuracy: [prop score:  0.94586], [q1: 0.78866], [q0: 0.93413],\n",
      "          Effect: [ate-q], [train: 0.09918], [test: 0.09962]\n",
      "********************************************************************************\n",
      "epoch: 187 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.31791], [Test: 2.34907],\n",
      "          Accuracy: [prop score:  0.94581], [q1: 0.80959], [q0: 0.92146],\n",
      "          Effect: [ate-q], [train: 0.10235], [test: 0.10279]\n",
      "********************************************************************************\n",
      "epoch: 188 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.31744], [Test: 2.34895],\n",
      "          Accuracy: [prop score:  0.94587], [q1: 0.80720], [q0: 0.91282],\n",
      "          Effect: [ate-q], [train: 0.10008], [test: 0.10052]\n",
      "********************************************************************************\n",
      "epoch: 189 / 500, time cost: 48.02 sec, \n",
      "          Loss: [Train: 2.31708], [Test: 2.34893],\n",
      "          Accuracy: [prop score:  0.94586], [q1: 0.78846], [q0: 0.92547],\n",
      "          Effect: [ate-q], [train: 0.09742], [test: 0.09786]\n",
      "********************************************************************************\n",
      "epoch: 190 / 500, time cost: 45.97 sec, \n",
      "          Loss: [Train: 2.31677], [Test: 2.34885],\n",
      "          Accuracy: [prop score:  0.94600], [q1: 0.78583], [q0: 0.93034],\n",
      "          Effect: [ate-q], [train: 0.09795], [test: 0.09841]\n",
      "********************************************************************************\n",
      "epoch: 191 / 500, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.31652], [Test: 2.34874],\n",
      "          Accuracy: [prop score:  0.94600], [q1: 0.82028], [q0: 0.92360],\n",
      "          Effect: [ate-q], [train: 0.10686], [test: 0.10731]\n",
      "********************************************************************************\n",
      "epoch: 192 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.31638], [Test: 2.34818],\n",
      "          Accuracy: [prop score:  0.94608], [q1: 0.80867], [q0: 0.93963],\n",
      "          Effect: [ate-q], [train: 0.10751], [test: 0.10797]\n",
      "********************************************************************************\n",
      "epoch: 193 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.31564], [Test: 2.34834],\n",
      "          Accuracy: [prop score:  0.94623], [q1: 0.81135], [q0: 0.92296],\n",
      "          Effect: [ate-q], [train: 0.10413], [test: 0.10459]\n",
      "********************************************************************************\n",
      "epoch: 194 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.31574], [Test: 2.34849],\n",
      "          Accuracy: [prop score:  0.94628], [q1: 0.81644], [q0: 0.92382],\n",
      "          Effect: [ate-q], [train: 0.10609], [test: 0.10654]\n",
      "********************************************************************************\n",
      "epoch: 195 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.31524], [Test: 2.34845],\n",
      "          Accuracy: [prop score:  0.94632], [q1: 0.79769], [q0: 0.91282],\n",
      "          Effect: [ate-q], [train: 0.09814], [test: 0.09860]\n",
      "********************************************************************************\n",
      "epoch: 196 / 500, time cost: 46.04 sec, \n",
      "          Loss: [Train: 2.31499], [Test: 2.34864],\n",
      "          Accuracy: [prop score:  0.94632], [q1: 0.80753], [q0: 0.92742],\n",
      "          Effect: [ate-q], [train: 0.10446], [test: 0.10491]\n",
      "********************************************************************************\n",
      "epoch: 197 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.31400], [Test: 2.34838],\n",
      "          Accuracy: [prop score:  0.94645], [q1: 0.79376], [q0: 0.92351],\n",
      "          Effect: [ate-q], [train: 0.09937], [test: 0.09982]\n",
      "********************************************************************************\n",
      "epoch: 198 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.31410], [Test: 2.34851],\n",
      "          Accuracy: [prop score:  0.94655], [q1: 0.79652], [q0: 0.92157],\n",
      "          Effect: [ate-q], [train: 0.09989], [test: 0.10036]\n",
      "********************************************************************************\n",
      "epoch: 199 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.31358], [Test: 2.34797],\n",
      "          Accuracy: [prop score:  0.94670], [q1: 0.78585], [q0: 0.91251],\n",
      "          Effect: [ate-q], [train: 0.09519], [test: 0.09568]\n",
      "********************************************************************************\n",
      "epoch: 200 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.31322], [Test: 2.34777],\n",
      "          Accuracy: [prop score:  0.94669], [q1: 0.79042], [q0: 0.92366],\n",
      "          Effect: [ate-q], [train: 0.09901], [test: 0.09952]\n",
      "********************************************************************************\n",
      "epoch: 201 / 500, time cost: 48.11 sec, \n",
      "          Loss: [Train: 2.31344], [Test: 2.34818],\n",
      "          Accuracy: [prop score:  0.94677], [q1: 0.79891], [q0: 0.91677],\n",
      "          Effect: [ate-q], [train: 0.10037], [test: 0.10087]\n",
      "********************************************************************************\n",
      "epoch: 202 / 500, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.31254], [Test: 2.34815],\n",
      "          Accuracy: [prop score:  0.94679], [q1: 0.79436], [q0: 0.91490],\n",
      "          Effect: [ate-q], [train: 0.09854], [test: 0.09901]\n",
      "********************************************************************************\n",
      "epoch: 203 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.31223], [Test: 2.34766],\n",
      "          Accuracy: [prop score:  0.94688], [q1: 0.79962], [q0: 0.92602],\n",
      "          Effect: [ate-q], [train: 0.10272], [test: 0.10325]\n",
      "********************************************************************************\n",
      "epoch: 204 / 500, time cost: 44.63 sec, \n",
      "          Loss: [Train: 2.31165], [Test: 2.34752],\n",
      "          Accuracy: [prop score:  0.94688], [q1: 0.80348], [q0: 0.91771],\n",
      "          Effect: [ate-q], [train: 0.10238], [test: 0.10289]\n",
      "********************************************************************************\n",
      "epoch: 205 / 500, time cost: 46.87 sec, \n",
      "          Loss: [Train: 2.31164], [Test: 2.34765],\n",
      "          Accuracy: [prop score:  0.94688], [q1: 0.78621], [q0: 0.91079],\n",
      "          Effect: [ate-q], [train: 0.09600], [test: 0.09650]\n",
      "********************************************************************************\n",
      "epoch: 206 / 500, time cost: 44.69 sec, \n",
      "          Loss: [Train: 2.31126], [Test: 2.34752],\n",
      "          Accuracy: [prop score:  0.94697], [q1: 0.77960], [q0: 0.91039],\n",
      "          Effect: [ate-q], [train: 0.09403], [test: 0.09453]\n",
      "********************************************************************************\n",
      "epoch: 207 / 500, time cost: 54.33 sec, \n",
      "          Loss: [Train: 2.31093], [Test: 2.34717],\n",
      "          Accuracy: [prop score:  0.94701], [q1: 0.77363], [q0: 0.91080],\n",
      "          Effect: [ate-q], [train: 0.09259], [test: 0.09311]\n",
      "********************************************************************************\n",
      "epoch: 208 / 500, time cost: 46.97 sec, \n",
      "          Loss: [Train: 2.31047], [Test: 2.34783],\n",
      "          Accuracy: [prop score:  0.94708], [q1: 0.76620], [q0: 0.91476],\n",
      "          Effect: [ate-q], [train: 0.09161], [test: 0.09213]\n",
      "********************************************************************************\n",
      "epoch: 209 / 500, time cost: 48.14 sec, \n",
      "          Loss: [Train: 2.30973], [Test: 2.34733],\n",
      "          Accuracy: [prop score:  0.94695], [q1: 0.80524], [q0: 0.92229],\n",
      "          Effect: [ate-q], [train: 0.10503], [test: 0.10554]\n",
      "********************************************************************************\n",
      "epoch: 210 / 500, time cost: 45.87 sec, \n",
      "          Loss: [Train: 2.30994], [Test: 2.34720],\n",
      "          Accuracy: [prop score:  0.94703], [q1: 0.77998], [q0: 0.90827],\n",
      "          Effect: [ate-q], [train: 0.09427], [test: 0.09482]\n",
      "********************************************************************************\n",
      "epoch: 211 / 500, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.30940], [Test: 2.34733],\n",
      "          Accuracy: [prop score:  0.94709], [q1: 0.79888], [q0: 0.92245],\n",
      "          Effect: [ate-q], [train: 0.10317], [test: 0.10370]\n",
      "********************************************************************************\n",
      "epoch: 212 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.30872], [Test: 2.34715],\n",
      "          Accuracy: [prop score:  0.94712], [q1: 0.77724], [q0: 0.92612],\n",
      "          Effect: [ate-q], [train: 0.09775], [test: 0.09831]\n",
      "********************************************************************************\n",
      "epoch: 213 / 500, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.30876], [Test: 2.34733],\n",
      "          Accuracy: [prop score:  0.94721], [q1: 0.78791], [q0: 0.91880],\n",
      "          Effect: [ate-q], [train: 0.09922], [test: 0.09977]\n",
      "********************************************************************************\n",
      "epoch: 214 / 500, time cost: 45.14 sec, \n",
      "          Loss: [Train: 2.30827], [Test: 2.34680],\n",
      "          Accuracy: [prop score:  0.94735], [q1: 0.78716], [q0: 0.92071],\n",
      "          Effect: [ate-q], [train: 0.09955], [test: 0.10009]\n",
      "********************************************************************************\n",
      "epoch: 215 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.30765], [Test: 2.34689],\n",
      "          Accuracy: [prop score:  0.94733], [q1: 0.77930], [q0: 0.92365],\n",
      "          Effect: [ate-q], [train: 0.09815], [test: 0.09869]\n",
      "********************************************************************************\n",
      "epoch: 216 / 500, time cost: 45.20 sec, \n",
      "          Loss: [Train: 2.30727], [Test: 2.34722],\n",
      "          Accuracy: [prop score:  0.94742], [q1: 0.77545], [q0: 0.91063],\n",
      "          Effect: [ate-q], [train: 0.09429], [test: 0.09485]\n",
      "********************************************************************************\n",
      "epoch: 217 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.30719], [Test: 2.34716],\n",
      "          Accuracy: [prop score:  0.94741], [q1: 0.77529], [q0: 0.91401],\n",
      "          Effect: [ate-q], [train: 0.09498], [test: 0.09556]\n",
      "********************************************************************************\n",
      "epoch: 218 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.30666], [Test: 2.34681],\n",
      "          Accuracy: [prop score:  0.94761], [q1: 0.78959], [q0: 0.92039],\n",
      "          Effect: [ate-q], [train: 0.10091], [test: 0.10148]\n",
      "********************************************************************************\n",
      "epoch: 219 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.30640], [Test: 2.34667],\n",
      "          Accuracy: [prop score:  0.94750], [q1: 0.79117], [q0: 0.91252],\n",
      "          Effect: [ate-q], [train: 0.09984], [test: 0.10040]\n",
      "********************************************************************************\n",
      "epoch: 220 / 500, time cost: 45.72 sec, \n",
      "          Loss: [Train: 2.30570], [Test: 2.34674],\n",
      "          Accuracy: [prop score:  0.94756], [q1: 0.78998], [q0: 0.90953],\n",
      "          Effect: [ate-q], [train: 0.09882], [test: 0.09940]\n",
      "********************************************************************************\n",
      "epoch: 221 / 500, time cost: 47.84 sec, \n",
      "          Loss: [Train: 2.30537], [Test: 2.34662],\n",
      "          Accuracy: [prop score:  0.94766], [q1: 0.78759], [q0: 0.91373],\n",
      "          Effect: [ate-q], [train: 0.09911], [test: 0.09970]\n",
      "********************************************************************************\n",
      "epoch: 222 / 500, time cost: 45.65 sec, \n",
      "          Loss: [Train: 2.30522], [Test: 2.34682],\n",
      "          Accuracy: [prop score:  0.94775], [q1: 0.78798], [q0: 0.92800],\n",
      "          Effect: [ate-q], [train: 0.10271], [test: 0.10327]\n",
      "********************************************************************************\n",
      "epoch: 223 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.30486], [Test: 2.34593],\n",
      "          Accuracy: [prop score:  0.94789], [q1: 0.76786], [q0: 0.92506],\n",
      "          Effect: [ate-q], [train: 0.09620], [test: 0.09679]\n",
      "********************************************************************************\n",
      "epoch: 224 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.30424], [Test: 2.34623],\n",
      "          Accuracy: [prop score:  0.94785], [q1: 0.76790], [q0: 0.89827],\n",
      "          Effect: [ate-q], [train: 0.09073], [test: 0.09132]\n",
      "********************************************************************************\n",
      "epoch: 225 / 500, time cost: 47.91 sec, \n",
      "          Loss: [Train: 2.30388], [Test: 2.34667],\n",
      "          Accuracy: [prop score:  0.94785], [q1: 0.77770], [q0: 0.92925],\n",
      "          Effect: [ate-q], [train: 0.10050], [test: 0.10112]\n",
      "********************************************************************************\n",
      "epoch: 226 / 500, time cost: 53.65 sec, \n",
      "          Loss: [Train: 2.30359], [Test: 2.34626],\n",
      "          Accuracy: [prop score:  0.94793], [q1: 0.77417], [q0: 0.91312],\n",
      "          Effect: [ate-q], [train: 0.09561], [test: 0.09622]\n",
      "********************************************************************************\n",
      "epoch: 227 / 500, time cost: 55.34 sec, \n",
      "          Loss: [Train: 2.30337], [Test: 2.34663],\n",
      "          Accuracy: [prop score:  0.94802], [q1: 0.77410], [q0: 0.91357],\n",
      "          Effect: [ate-q], [train: 0.09588], [test: 0.09650]\n",
      "********************************************************************************\n",
      "epoch: 228 / 500, time cost: 53.72 sec, \n",
      "          Loss: [Train: 2.30275], [Test: 2.34672],\n",
      "          Accuracy: [prop score:  0.94807], [q1: 0.76021], [q0: 0.90601],\n",
      "          Effect: [ate-q], [train: 0.09064], [test: 0.09127]\n",
      "********************************************************************************\n",
      "epoch: 229 / 500, time cost: 55.71 sec, \n",
      "          Loss: [Train: 2.30214], [Test: 2.34638],\n",
      "          Accuracy: [prop score:  0.94810], [q1: 0.80130], [q0: 0.92844],\n",
      "          Effect: [ate-q], [train: 0.10860], [test: 0.10926]\n",
      "********************************************************************************\n",
      "epoch: 230 / 500, time cost: 49.47 sec, \n",
      "          Loss: [Train: 2.30181], [Test: 2.34595],\n",
      "          Accuracy: [prop score:  0.94816], [q1: 0.76221], [q0: 0.92970],\n",
      "          Effect: [ate-q], [train: 0.09688], [test: 0.09751]\n",
      "********************************************************************************\n",
      "epoch: 231 / 500, time cost: 48.48 sec, \n",
      "          Loss: [Train: 2.30191], [Test: 2.34606],\n",
      "          Accuracy: [prop score:  0.94821], [q1: 0.78363], [q0: 0.91699],\n",
      "          Effect: [ate-q], [train: 0.10018], [test: 0.10080]\n",
      "********************************************************************************\n",
      "epoch: 232 / 500, time cost: 46.58 sec, \n",
      "          Loss: [Train: 2.30148], [Test: 2.34614],\n",
      "          Accuracy: [prop score:  0.94829], [q1: 0.75081], [q0: 0.91358],\n",
      "          Effect: [ate-q], [train: 0.09030], [test: 0.09094]\n",
      "********************************************************************************\n",
      "epoch: 233 / 500, time cost: 48.76 sec, \n",
      "          Loss: [Train: 2.30117], [Test: 2.34630],\n",
      "          Accuracy: [prop score:  0.94833], [q1: 0.76051], [q0: 0.92957],\n",
      "          Effect: [ate-q], [train: 0.09687], [test: 0.09749]\n",
      "********************************************************************************\n",
      "epoch: 234 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.30076], [Test: 2.34585],\n",
      "          Accuracy: [prop score:  0.94843], [q1: 0.74712], [q0: 0.91601],\n",
      "          Effect: [ate-q], [train: 0.09009], [test: 0.09072]\n",
      "********************************************************************************\n",
      "epoch: 235 / 500, time cost: 48.39 sec, \n",
      "          Loss: [Train: 2.29976], [Test: 2.34601],\n",
      "          Accuracy: [prop score:  0.94852], [q1: 0.79307], [q0: 0.92244],\n",
      "          Effect: [ate-q], [train: 0.10522], [test: 0.10587]\n",
      "********************************************************************************\n",
      "epoch: 236 / 500, time cost: 46.15 sec, \n",
      "          Loss: [Train: 2.29981], [Test: 2.34581],\n",
      "          Accuracy: [prop score:  0.94861], [q1: 0.77022], [q0: 0.91166],\n",
      "          Effect: [ate-q], [train: 0.09552], [test: 0.09619]\n",
      "********************************************************************************\n",
      "epoch: 237 / 500, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.29925], [Test: 2.34614],\n",
      "          Accuracy: [prop score:  0.94874], [q1: 0.74679], [q0: 0.92036],\n",
      "          Effect: [ate-q], [train: 0.09121], [test: 0.09187]\n",
      "********************************************************************************\n",
      "epoch: 238 / 500, time cost: 46.19 sec, \n",
      "          Loss: [Train: 2.29871], [Test: 2.34633],\n",
      "          Accuracy: [prop score:  0.94873], [q1: 0.72280], [q0: 0.91933],\n",
      "          Effect: [ate-q], [train: 0.08570], [test: 0.08637]\n",
      "********************************************************************************\n",
      "epoch: 239 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.29862], [Test: 2.34574],\n",
      "          Accuracy: [prop score:  0.94885], [q1: 0.76575], [q0: 0.92312],\n",
      "          Effect: [ate-q], [train: 0.09737], [test: 0.09804]\n",
      "********************************************************************************\n",
      "epoch: 240 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.29781], [Test: 2.34544],\n",
      "          Accuracy: [prop score:  0.94892], [q1: 0.76704], [q0: 0.91839],\n",
      "          Effect: [ate-q], [train: 0.09675], [test: 0.09743]\n",
      "********************************************************************************\n",
      "epoch: 241 / 500, time cost: 47.89 sec, \n",
      "          Loss: [Train: 2.29755], [Test: 2.34613],\n",
      "          Accuracy: [prop score:  0.94892], [q1: 0.75730], [q0: 0.89443],\n",
      "          Effect: [ate-q], [train: 0.08934], [test: 0.09005]\n",
      "********************************************************************************\n",
      "epoch: 242 / 500, time cost: 45.82 sec, \n",
      "          Loss: [Train: 2.29727], [Test: 2.34618],\n",
      "          Accuracy: [prop score:  0.94908], [q1: 0.77985], [q0: 0.91494],\n",
      "          Effect: [ate-q], [train: 0.10020], [test: 0.10086]\n",
      "********************************************************************************\n",
      "epoch: 243 / 500, time cost: 48.26 sec, \n",
      "          Loss: [Train: 2.29653], [Test: 2.34553],\n",
      "          Accuracy: [prop score:  0.94911], [q1: 0.75714], [q0: 0.90997],\n",
      "          Effect: [ate-q], [train: 0.09263], [test: 0.09334]\n",
      "********************************************************************************\n",
      "epoch: 244 / 500, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.29665], [Test: 2.34573],\n",
      "          Accuracy: [prop score:  0.94915], [q1: 0.75285], [q0: 0.92338],\n",
      "          Effect: [ate-q], [train: 0.09469], [test: 0.09539]\n",
      "********************************************************************************\n",
      "epoch: 245 / 500, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.29623], [Test: 2.34608],\n",
      "          Accuracy: [prop score:  0.94915], [q1: 0.77432], [q0: 0.92201],\n",
      "          Effect: [ate-q], [train: 0.10073], [test: 0.10146]\n",
      "********************************************************************************\n",
      "epoch: 246 / 500, time cost: 46.30 sec, \n",
      "          Loss: [Train: 2.29593], [Test: 2.34618],\n",
      "          Accuracy: [prop score:  0.94927], [q1: 0.72477], [q0: 0.90372],\n",
      "          Effect: [ate-q], [train: 0.08381], [test: 0.08452]\n",
      "********************************************************************************\n",
      "epoch: 247 / 500, time cost: 48.13 sec, \n",
      "          Loss: [Train: 2.29539], [Test: 2.34559],\n",
      "          Accuracy: [prop score:  0.94938], [q1: 0.75881], [q0: 0.91340],\n",
      "          Effect: [ate-q], [train: 0.09445], [test: 0.09517]\n",
      "********************************************************************************\n",
      "epoch: 248 / 500, time cost: 46.28 sec, \n",
      "          Loss: [Train: 2.29490], [Test: 2.34567],\n",
      "          Accuracy: [prop score:  0.94938], [q1: 0.76444], [q0: 0.92016],\n",
      "          Effect: [ate-q], [train: 0.09807], [test: 0.09880]\n",
      "********************************************************************************\n",
      "epoch: 249 / 500, time cost: 48.59 sec, \n",
      "          Loss: [Train: 2.29395], [Test: 2.34576],\n",
      "          Accuracy: [prop score:  0.94951], [q1: 0.76785], [q0: 0.92549],\n",
      "          Effect: [ate-q], [train: 0.10051], [test: 0.10124]\n",
      "********************************************************************************\n",
      "epoch: 250 / 500, time cost: 46.68 sec, \n",
      "          Loss: [Train: 2.29404], [Test: 2.34542],\n",
      "          Accuracy: [prop score:  0.94957], [q1: 0.75344], [q0: 0.90483],\n",
      "          Effect: [ate-q], [train: 0.09165], [test: 0.09239]\n",
      "********************************************************************************\n",
      "epoch: 251 / 500, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.29388], [Test: 2.34533],\n",
      "          Accuracy: [prop score:  0.94963], [q1: 0.75649], [q0: 0.91926],\n",
      "          Effect: [ate-q], [train: 0.09596], [test: 0.09671]\n",
      "********************************************************************************\n",
      "epoch: 252 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.29344], [Test: 2.34574],\n",
      "          Accuracy: [prop score:  0.94965], [q1: 0.75731], [q0: 0.91642],\n",
      "          Effect: [ate-q], [train: 0.09569], [test: 0.09645]\n",
      "********************************************************************************\n",
      "epoch: 253 / 500, time cost: 47.25 sec, \n",
      "          Loss: [Train: 2.29305], [Test: 2.34591],\n",
      "          Accuracy: [prop score:  0.94969], [q1: 0.73197], [q0: 0.93175],\n",
      "          Effect: [ate-q], [train: 0.09326], [test: 0.09398]\n",
      "********************************************************************************\n",
      "epoch: 254 / 500, time cost: 45.27 sec, \n",
      "          Loss: [Train: 2.29213], [Test: 2.34582],\n",
      "          Accuracy: [prop score:  0.94983], [q1: 0.78889], [q0: 0.91267],\n",
      "          Effect: [ate-q], [train: 0.10556], [test: 0.10633]\n",
      "********************************************************************************\n",
      "epoch: 255 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.29190], [Test: 2.34571],\n",
      "          Accuracy: [prop score:  0.94988], [q1: 0.74042], [q0: 0.90568],\n",
      "          Effect: [ate-q], [train: 0.08915], [test: 0.08992]\n",
      "********************************************************************************\n",
      "epoch: 256 / 500, time cost: 45.38 sec, \n",
      "          Loss: [Train: 2.29138], [Test: 2.34575],\n",
      "          Accuracy: [prop score:  0.94994], [q1: 0.72595], [q0: 0.90670],\n",
      "          Effect: [ate-q], [train: 0.08593], [test: 0.08669]\n",
      "********************************************************************************\n",
      "epoch: 257 / 500, time cost: 48.09 sec, \n",
      "          Loss: [Train: 2.29131], [Test: 2.34587],\n",
      "          Accuracy: [prop score:  0.94990], [q1: 0.76125], [q0: 0.91668],\n",
      "          Effect: [ate-q], [train: 0.09767], [test: 0.09845]\n",
      "********************************************************************************\n",
      "epoch: 258 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.29055], [Test: 2.34621],\n",
      "          Accuracy: [prop score:  0.94999], [q1: 0.70850], [q0: 0.92532],\n",
      "          Effect: [ate-q], [train: 0.08647], [test: 0.08724]\n",
      "********************************************************************************\n",
      "epoch: 259 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.29040], [Test: 2.34676],\n",
      "          Accuracy: [prop score:  0.94996], [q1: 0.72797], [q0: 0.91570],\n",
      "          Effect: [ate-q], [train: 0.08880], [test: 0.08955]\n",
      "********************************************************************************\n",
      "epoch: 260 / 500, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.29002], [Test: 2.34629],\n",
      "          Accuracy: [prop score:  0.95005], [q1: 0.75443], [q0: 0.89306],\n",
      "          Effect: [ate-q], [train: 0.09142], [test: 0.09223]\n",
      "********************************************************************************\n",
      "epoch: 261 / 500, time cost: 48.37 sec, \n",
      "          Loss: [Train: 2.28983], [Test: 2.34606],\n",
      "          Accuracy: [prop score:  0.95012], [q1: 0.73303], [q0: 0.92449],\n",
      "          Effect: [ate-q], [train: 0.09216], [test: 0.09296]\n",
      "********************************************************************************\n",
      "epoch: 262 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.28906], [Test: 2.34619],\n",
      "          Accuracy: [prop score:  0.95005], [q1: 0.75279], [q0: 0.90704],\n",
      "          Effect: [ate-q], [train: 0.09408], [test: 0.09488]\n",
      "********************************************************************************\n",
      "epoch: 263 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.28832], [Test: 2.34645],\n",
      "          Accuracy: [prop score:  0.95021], [q1: 0.71381], [q0: 0.90351],\n",
      "          Effect: [ate-q], [train: 0.08308], [test: 0.08391]\n",
      "********************************************************************************\n",
      "epoch: 264 / 500, time cost: 46.61 sec, \n",
      "          Loss: [Train: 2.28771], [Test: 2.34592],\n",
      "          Accuracy: [prop score:  0.95028], [q1: 0.72062], [q0: 0.91538],\n",
      "          Effect: [ate-q], [train: 0.08755], [test: 0.08835]\n",
      "********************************************************************************\n",
      "epoch: 265 / 500, time cost: 47.86 sec, \n",
      "          Loss: [Train: 2.28820], [Test: 2.34587],\n",
      "          Accuracy: [prop score:  0.95028], [q1: 0.75575], [q0: 0.90863],\n",
      "          Effect: [ate-q], [train: 0.09588], [test: 0.09672]\n",
      "********************************************************************************\n",
      "epoch: 266 / 500, time cost: 46.75 sec, \n",
      "          Loss: [Train: 2.28740], [Test: 2.34634],\n",
      "          Accuracy: [prop score:  0.95034], [q1: 0.72113], [q0: 0.91088],\n",
      "          Effect: [ate-q], [train: 0.08680], [test: 0.08763]\n",
      "********************************************************************************\n",
      "epoch: 267 / 500, time cost: 48.73 sec, \n",
      "          Loss: [Train: 2.28703], [Test: 2.34595],\n",
      "          Accuracy: [prop score:  0.95046], [q1: 0.71631], [q0: 0.91236],\n",
      "          Effect: [ate-q], [train: 0.08628], [test: 0.08711]\n",
      "********************************************************************************\n",
      "epoch: 268 / 500, time cost: 49.26 sec, \n",
      "          Loss: [Train: 2.28654], [Test: 2.34615],\n",
      "          Accuracy: [prop score:  0.95041], [q1: 0.75368], [q0: 0.91054],\n",
      "          Effect: [ate-q], [train: 0.09623], [test: 0.09710]\n",
      "********************************************************************************\n",
      "epoch: 269 / 500, time cost: 55.21 sec, \n",
      "          Loss: [Train: 2.28606], [Test: 2.34640],\n",
      "          Accuracy: [prop score:  0.95042], [q1: 0.71325], [q0: 0.89941],\n",
      "          Effect: [ate-q], [train: 0.08310], [test: 0.08399]\n",
      "********************************************************************************\n",
      "epoch: 270 / 500, time cost: 48.31 sec, \n",
      "          Loss: [Train: 2.28507], [Test: 2.34613],\n",
      "          Accuracy: [prop score:  0.95056], [q1: 0.74415], [q0: 0.91263],\n",
      "          Effect: [ate-q], [train: 0.09423], [test: 0.09509]\n",
      "********************************************************************************\n",
      "epoch: 271 / 500, time cost: 47.70 sec, \n",
      "          Loss: [Train: 2.28533], [Test: 2.34655],\n",
      "          Accuracy: [prop score:  0.95060], [q1: 0.75145], [q0: 0.88311],\n",
      "          Effect: [ate-q], [train: 0.09089], [test: 0.09175]\n",
      "********************************************************************************\n",
      "epoch: 272 / 500, time cost: 45.19 sec, \n",
      "          Loss: [Train: 2.28479], [Test: 2.34619],\n",
      "          Accuracy: [prop score:  0.95065], [q1: 0.73553], [q0: 0.92351],\n",
      "          Effect: [ate-q], [train: 0.09462], [test: 0.09546]\n",
      "********************************************************************************\n",
      "epoch: 273 / 500, time cost: 47.38 sec, \n",
      "          Loss: [Train: 2.28404], [Test: 2.34634],\n",
      "          Accuracy: [prop score:  0.95071], [q1: 0.71877], [q0: 0.91615],\n",
      "          Effect: [ate-q], [train: 0.08855], [test: 0.08941]\n",
      "********************************************************************************\n",
      "epoch: 274 / 500, time cost: 44.63 sec, \n",
      "          Loss: [Train: 2.28404], [Test: 2.34663],\n",
      "          Accuracy: [prop score:  0.95072], [q1: 0.75802], [q0: 0.90929],\n",
      "          Effect: [ate-q], [train: 0.09850], [test: 0.09940]\n",
      "********************************************************************************\n",
      "epoch: 275 / 500, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.28358], [Test: 2.34671],\n",
      "          Accuracy: [prop score:  0.95079], [q1: 0.75974], [q0: 0.90522],\n",
      "          Effect: [ate-q], [train: 0.09836], [test: 0.09923]\n",
      "********************************************************************************\n",
      "epoch: 276 / 500, time cost: 45.00 sec, \n",
      "          Loss: [Train: 2.28300], [Test: 2.34660],\n",
      "          Accuracy: [prop score:  0.95081], [q1: 0.74342], [q0: 0.91307],\n",
      "          Effect: [ate-q], [train: 0.09539], [test: 0.09629]\n",
      "********************************************************************************\n",
      "epoch: 277 / 500, time cost: 47.27 sec, \n",
      "          Loss: [Train: 2.28221], [Test: 2.34670],\n",
      "          Accuracy: [prop score:  0.95078], [q1: 0.70420], [q0: 0.89646],\n",
      "          Effect: [ate-q], [train: 0.08178], [test: 0.08270]\n",
      "********************************************************************************\n",
      "epoch: 278 / 500, time cost: 45.48 sec, \n",
      "          Loss: [Train: 2.28249], [Test: 2.34686],\n",
      "          Accuracy: [prop score:  0.95091], [q1: 0.74373], [q0: 0.89880],\n",
      "          Effect: [ate-q], [train: 0.09272], [test: 0.09365]\n",
      "********************************************************************************\n",
      "epoch: 279 / 500, time cost: 47.85 sec, \n",
      "          Loss: [Train: 2.28137], [Test: 2.34690],\n",
      "          Accuracy: [prop score:  0.95096], [q1: 0.70602], [q0: 0.88929],\n",
      "          Effect: [ate-q], [train: 0.08097], [test: 0.08187]\n",
      "********************************************************************************\n",
      "epoch: 280 / 500, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.28102], [Test: 2.34632],\n",
      "          Accuracy: [prop score:  0.95092], [q1: 0.72822], [q0: 0.90378],\n",
      "          Effect: [ate-q], [train: 0.08943], [test: 0.09035]\n",
      "********************************************************************************\n",
      "epoch: 281 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.28071], [Test: 2.34683],\n",
      "          Accuracy: [prop score:  0.95101], [q1: 0.72950], [q0: 0.91182],\n",
      "          Effect: [ate-q], [train: 0.09177], [test: 0.09270]\n",
      "********************************************************************************\n",
      "epoch: 282 / 500, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.28015], [Test: 2.34666],\n",
      "          Accuracy: [prop score:  0.95108], [q1: 0.73627], [q0: 0.90621],\n",
      "          Effect: [ate-q], [train: 0.09266], [test: 0.09359]\n",
      "********************************************************************************\n",
      "epoch: 283 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.27924], [Test: 2.34684],\n",
      "          Accuracy: [prop score:  0.95114], [q1: 0.71946], [q0: 0.90936],\n",
      "          Effect: [ate-q], [train: 0.08897], [test: 0.08992]\n",
      "********************************************************************************\n",
      "epoch: 284 / 500, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.27919], [Test: 2.34731],\n",
      "          Accuracy: [prop score:  0.95120], [q1: 0.73119], [q0: 0.90538],\n",
      "          Effect: [ate-q], [train: 0.09170], [test: 0.09262]\n",
      "********************************************************************************\n",
      "epoch: 285 / 500, time cost: 47.64 sec, \n",
      "          Loss: [Train: 2.27933], [Test: 2.34740],\n",
      "          Accuracy: [prop score:  0.95119], [q1: 0.71348], [q0: 0.89978],\n",
      "          Effect: [ate-q], [train: 0.08554], [test: 0.08649]\n",
      "********************************************************************************\n",
      "epoch: 286 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.27878], [Test: 2.34745],\n",
      "          Accuracy: [prop score:  0.95127], [q1: 0.71861], [q0: 0.89525],\n",
      "          Effect: [ate-q], [train: 0.08627], [test: 0.08724]\n",
      "********************************************************************************\n",
      "epoch: 287 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.27774], [Test: 2.34734],\n",
      "          Accuracy: [prop score:  0.95128], [q1: 0.74042], [q0: 0.91444],\n",
      "          Effect: [ate-q], [train: 0.09680], [test: 0.09778]\n",
      "********************************************************************************\n",
      "epoch: 288 / 500, time cost: 46.23 sec, \n",
      "          Loss: [Train: 2.27778], [Test: 2.34744],\n",
      "          Accuracy: [prop score:  0.95133], [q1: 0.69866], [q0: 0.91672],\n",
      "          Effect: [ate-q], [train: 0.08613], [test: 0.08708]\n",
      "********************************************************************************\n",
      "epoch: 289 / 500, time cost: 48.43 sec, \n",
      "          Loss: [Train: 2.27719], [Test: 2.34757],\n",
      "          Accuracy: [prop score:  0.95137], [q1: 0.69324], [q0: 0.90900],\n",
      "          Effect: [ate-q], [train: 0.08353], [test: 0.08453]\n",
      "********************************************************************************\n",
      "epoch: 290 / 500, time cost: 45.46 sec, \n",
      "          Loss: [Train: 2.27693], [Test: 2.34770],\n",
      "          Accuracy: [prop score:  0.95143], [q1: 0.70003], [q0: 0.88033],\n",
      "          Effect: [ate-q], [train: 0.07958], [test: 0.08057]\n",
      "********************************************************************************\n",
      "epoch: 291 / 500, time cost: 47.75 sec, \n",
      "          Loss: [Train: 2.27611], [Test: 2.34740],\n",
      "          Accuracy: [prop score:  0.95142], [q1: 0.70413], [q0: 0.92830],\n",
      "          Effect: [ate-q], [train: 0.09134], [test: 0.09236]\n",
      "********************************************************************************\n",
      "epoch: 292 / 500, time cost: 46.88 sec, \n",
      "          Loss: [Train: 2.27536], [Test: 2.34792],\n",
      "          Accuracy: [prop score:  0.95157], [q1: 0.71499], [q0: 0.90711],\n",
      "          Effect: [ate-q], [train: 0.08858], [test: 0.08957]\n",
      "********************************************************************************\n",
      "epoch: 293 / 500, time cost: 48.49 sec, \n",
      "          Loss: [Train: 2.27502], [Test: 2.34792],\n",
      "          Accuracy: [prop score:  0.95158], [q1: 0.70502], [q0: 0.90704],\n",
      "          Effect: [ate-q], [train: 0.08612], [test: 0.08715]\n",
      "********************************************************************************\n",
      "epoch: 294 / 500, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.27471], [Test: 2.34794],\n",
      "          Accuracy: [prop score:  0.95162], [q1: 0.69428], [q0: 0.88777],\n",
      "          Effect: [ate-q], [train: 0.07973], [test: 0.08070]\n",
      "********************************************************************************\n",
      "epoch: 295 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.27448], [Test: 2.34835],\n",
      "          Accuracy: [prop score:  0.95170], [q1: 0.73760], [q0: 0.90317],\n",
      "          Effect: [ate-q], [train: 0.09478], [test: 0.09579]\n",
      "********************************************************************************\n",
      "epoch: 296 / 500, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.27409], [Test: 2.34784],\n",
      "          Accuracy: [prop score:  0.95176], [q1: 0.72369], [q0: 0.91750],\n",
      "          Effect: [ate-q], [train: 0.09429], [test: 0.09531]\n",
      "********************************************************************************\n",
      "epoch: 297 / 500, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.27353], [Test: 2.34844],\n",
      "          Accuracy: [prop score:  0.95181], [q1: 0.68889], [q0: 0.90871],\n",
      "          Effect: [ate-q], [train: 0.08310], [test: 0.08412]\n",
      "********************************************************************************\n",
      "epoch: 298 / 500, time cost: 45.91 sec, \n",
      "          Loss: [Train: 2.27322], [Test: 2.34883],\n",
      "          Accuracy: [prop score:  0.95182], [q1: 0.67317], [q0: 0.87543],\n",
      "          Effect: [ate-q], [train: 0.07336], [test: 0.07440]\n",
      "********************************************************************************\n",
      "epoch: 299 / 500, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.27234], [Test: 2.34884],\n",
      "          Accuracy: [prop score:  0.95183], [q1: 0.67296], [q0: 0.89378],\n",
      "          Effect: [ate-q], [train: 0.07668], [test: 0.07770]\n",
      "********************************************************************************\n",
      "epoch: 300 / 500, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.27260], [Test: 2.34849],\n",
      "          Accuracy: [prop score:  0.95188], [q1: 0.67168], [q0: 0.89601],\n",
      "          Effect: [ate-q], [train: 0.07728], [test: 0.07832]\n",
      "********************************************************************************\n",
      "epoch: 301 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.27194], [Test: 2.34893],\n",
      "          Accuracy: [prop score:  0.95189], [q1: 0.66470], [q0: 0.90045],\n",
      "          Effect: [ate-q], [train: 0.07758], [test: 0.07862]\n",
      "********************************************************************************\n",
      "epoch: 302 / 500, time cost: 45.62 sec, \n",
      "          Loss: [Train: 2.27145], [Test: 2.34855],\n",
      "          Accuracy: [prop score:  0.95194], [q1: 0.69476], [q0: 0.90579],\n",
      "          Effect: [ate-q], [train: 0.08487], [test: 0.08594]\n",
      "********************************************************************************\n",
      "epoch: 303 / 500, time cost: 48.28 sec, \n",
      "          Loss: [Train: 2.27107], [Test: 2.34864],\n",
      "          Accuracy: [prop score:  0.95201], [q1: 0.71384], [q0: 0.90641],\n",
      "          Effect: [ate-q], [train: 0.09008], [test: 0.09116]\n",
      "********************************************************************************\n",
      "epoch: 304 / 500, time cost: 46.06 sec, \n",
      "          Loss: [Train: 2.27051], [Test: 2.34926],\n",
      "          Accuracy: [prop score:  0.95212], [q1: 0.68887], [q0: 0.91073],\n",
      "          Effect: [ate-q], [train: 0.08455], [test: 0.08561]\n",
      "********************************************************************************\n",
      "epoch: 305 / 500, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.26964], [Test: 2.34920],\n",
      "          Accuracy: [prop score:  0.95212], [q1: 0.70792], [q0: 0.89241],\n",
      "          Effect: [ate-q], [train: 0.08590], [test: 0.08699]\n",
      "********************************************************************************\n",
      "epoch: 306 / 500, time cost: 45.88 sec, \n",
      "          Loss: [Train: 2.26948], [Test: 2.34940],\n",
      "          Accuracy: [prop score:  0.95218], [q1: 0.69690], [q0: 0.88540],\n",
      "          Effect: [ate-q], [train: 0.08186], [test: 0.08294]\n",
      "********************************************************************************\n",
      "epoch: 307 / 500, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.26901], [Test: 2.34908],\n",
      "          Accuracy: [prop score:  0.95233], [q1: 0.71691], [q0: 0.88073],\n",
      "          Effect: [ate-q], [train: 0.08671], [test: 0.08783]\n",
      "********************************************************************************\n",
      "epoch: 308 / 500, time cost: 45.81 sec, \n",
      "          Loss: [Train: 2.26846], [Test: 2.34929],\n",
      "          Accuracy: [prop score:  0.95225], [q1: 0.68125], [q0: 0.90308],\n",
      "          Effect: [ate-q], [train: 0.08157], [test: 0.08269]\n",
      "********************************************************************************\n",
      "epoch: 309 / 500, time cost: 47.91 sec, \n",
      "          Loss: [Train: 2.26805], [Test: 2.34933],\n",
      "          Accuracy: [prop score:  0.95243], [q1: 0.67269], [q0: 0.91507],\n",
      "          Effect: [ate-q], [train: 0.08288], [test: 0.08394]\n",
      "********************************************************************************\n",
      "epoch: 310 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.26729], [Test: 2.34971],\n",
      "          Accuracy: [prop score:  0.95242], [q1: 0.72995], [q0: 0.89269],\n",
      "          Effect: [ate-q], [train: 0.09334], [test: 0.09444]\n",
      "********************************************************************************\n",
      "epoch: 311 / 500, time cost: 48.00 sec, \n",
      "          Loss: [Train: 2.26674], [Test: 2.34977],\n",
      "          Accuracy: [prop score:  0.95238], [q1: 0.71269], [q0: 0.90323],\n",
      "          Effect: [ate-q], [train: 0.09068], [test: 0.09182]\n",
      "********************************************************************************\n",
      "epoch: 312 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.26651], [Test: 2.35046],\n",
      "          Accuracy: [prop score:  0.95236], [q1: 0.66519], [q0: 0.89240],\n",
      "          Effect: [ate-q], [train: 0.07693], [test: 0.07806]\n",
      "********************************************************************************\n",
      "epoch: 313 / 500, time cost: 47.73 sec, \n",
      "          Loss: [Train: 2.26637], [Test: 2.34967],\n",
      "          Accuracy: [prop score:  0.95249], [q1: 0.71423], [q0: 0.91471],\n",
      "          Effect: [ate-q], [train: 0.09410], [test: 0.09526]\n",
      "********************************************************************************\n",
      "epoch: 314 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.26564], [Test: 2.34938],\n",
      "          Accuracy: [prop score:  0.95251], [q1: 0.70377], [q0: 0.89816],\n",
      "          Effect: [ate-q], [train: 0.08748], [test: 0.08863]\n",
      "********************************************************************************\n",
      "epoch: 315 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.26552], [Test: 2.35008],\n",
      "          Accuracy: [prop score:  0.95251], [q1: 0.70400], [q0: 0.88827],\n",
      "          Effect: [ate-q], [train: 0.08590], [test: 0.08707]\n",
      "********************************************************************************\n",
      "epoch: 316 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 2.26446], [Test: 2.35070],\n",
      "          Accuracy: [prop score:  0.95252], [q1: 0.65374], [q0: 0.91115],\n",
      "          Effect: [ate-q], [train: 0.07859], [test: 0.07973]\n",
      "********************************************************************************\n",
      "epoch: 317 / 500, time cost: 47.63 sec, \n",
      "          Loss: [Train: 2.26425], [Test: 2.35125],\n",
      "          Accuracy: [prop score:  0.95259], [q1: 0.64886], [q0: 0.86372],\n",
      "          Effect: [ate-q], [train: 0.06931], [test: 0.07040]\n",
      "********************************************************************************\n",
      "epoch: 318 / 500, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.26408], [Test: 2.35058],\n",
      "          Accuracy: [prop score:  0.95263], [q1: 0.71201], [q0: 0.89496],\n",
      "          Effect: [ate-q], [train: 0.09003], [test: 0.09124]\n",
      "********************************************************************************\n",
      "epoch: 319 / 500, time cost: 48.65 sec, \n",
      "          Loss: [Train: 2.26361], [Test: 2.35119],\n",
      "          Accuracy: [prop score:  0.95256], [q1: 0.70497], [q0: 0.90623],\n",
      "          Effect: [ate-q], [train: 0.09039], [test: 0.09154]\n",
      "********************************************************************************\n",
      "epoch: 320 / 500, time cost: 46.38 sec, \n",
      "          Loss: [Train: 2.26301], [Test: 2.35105],\n",
      "          Accuracy: [prop score:  0.95268], [q1: 0.69304], [q0: 0.87947],\n",
      "          Effect: [ate-q], [train: 0.08244], [test: 0.08361]\n",
      "********************************************************************************\n",
      "epoch: 321 / 500, time cost: 48.09 sec, \n",
      "          Loss: [Train: 2.26262], [Test: 2.35137],\n",
      "          Accuracy: [prop score:  0.95252], [q1: 0.70206], [q0: 0.88229],\n",
      "          Effect: [ate-q], [train: 0.08561], [test: 0.08684]\n",
      "********************************************************************************\n",
      "epoch: 322 / 500, time cost: 45.59 sec, \n",
      "          Loss: [Train: 2.26220], [Test: 2.35119],\n",
      "          Accuracy: [prop score:  0.95261], [q1: 0.69354], [q0: 0.90240],\n",
      "          Effect: [ate-q], [train: 0.08710], [test: 0.08833]\n",
      "********************************************************************************\n",
      "epoch: 323 / 500, time cost: 47.99 sec, \n",
      "          Loss: [Train: 2.26120], [Test: 2.35132],\n",
      "          Accuracy: [prop score:  0.95269], [q1: 0.70727], [q0: 0.89754],\n",
      "          Effect: [ate-q], [train: 0.09025], [test: 0.09144]\n",
      "********************************************************************************\n",
      "epoch: 324 / 500, time cost: 45.60 sec, \n",
      "          Loss: [Train: 2.26085], [Test: 2.35138],\n",
      "          Accuracy: [prop score:  0.95275], [q1: 0.69493], [q0: 0.88581],\n",
      "          Effect: [ate-q], [train: 0.08477], [test: 0.08594]\n",
      "********************************************************************************\n",
      "epoch: 325 / 500, time cost: 47.95 sec, \n",
      "          Loss: [Train: 2.26087], [Test: 2.35222],\n",
      "          Accuracy: [prop score:  0.95274], [q1: 0.69005], [q0: 0.91620],\n",
      "          Effect: [ate-q], [train: 0.08962], [test: 0.09086]\n",
      "********************************************************************************\n",
      "epoch: 326 / 500, time cost: 46.32 sec, \n",
      "          Loss: [Train: 2.26004], [Test: 2.35220],\n",
      "          Accuracy: [prop score:  0.95278], [q1: 0.70315], [q0: 0.90191],\n",
      "          Effect: [ate-q], [train: 0.09038], [test: 0.09160]\n",
      "********************************************************************************\n",
      "epoch: 327 / 500, time cost: 48.43 sec, \n",
      "          Loss: [Train: 2.25983], [Test: 2.35230],\n",
      "          Accuracy: [prop score:  0.95279], [q1: 0.68445], [q0: 0.87803],\n",
      "          Effect: [ate-q], [train: 0.08070], [test: 0.08191]\n",
      "********************************************************************************\n",
      "epoch: 328 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.25911], [Test: 2.35209],\n",
      "          Accuracy: [prop score:  0.95284], [q1: 0.70279], [q0: 0.91230],\n",
      "          Effect: [ate-q], [train: 0.09277], [test: 0.09400]\n",
      "********************************************************************************\n",
      "epoch: 329 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.25913], [Test: 2.35310],\n",
      "          Accuracy: [prop score:  0.95292], [q1: 0.63832], [q0: 0.87950],\n",
      "          Effect: [ate-q], [train: 0.07082], [test: 0.07203]\n",
      "********************************************************************************\n",
      "epoch: 330 / 500, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.25818], [Test: 2.35271],\n",
      "          Accuracy: [prop score:  0.95287], [q1: 0.65110], [q0: 0.88028],\n",
      "          Effect: [ate-q], [train: 0.07373], [test: 0.07495]\n",
      "********************************************************************************\n",
      "epoch: 331 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.25782], [Test: 2.35256],\n",
      "          Accuracy: [prop score:  0.95289], [q1: 0.69347], [q0: 0.90245],\n",
      "          Effect: [ate-q], [train: 0.08829], [test: 0.08952]\n",
      "********************************************************************************\n",
      "epoch: 332 / 500, time cost: 46.16 sec, \n",
      "          Loss: [Train: 2.25762], [Test: 2.35223],\n",
      "          Accuracy: [prop score:  0.95295], [q1: 0.71300], [q0: 0.88961],\n",
      "          Effect: [ate-q], [train: 0.09169], [test: 0.09296]\n",
      "********************************************************************************\n",
      "epoch: 333 / 500, time cost: 48.52 sec, \n",
      "          Loss: [Train: 2.25672], [Test: 2.35314],\n",
      "          Accuracy: [prop score:  0.95302], [q1: 0.69259], [q0: 0.88757],\n",
      "          Effect: [ate-q], [train: 0.08594], [test: 0.08717]\n",
      "********************************************************************************\n",
      "epoch: 334 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.25629], [Test: 2.35326],\n",
      "          Accuracy: [prop score:  0.95297], [q1: 0.66593], [q0: 0.86451],\n",
      "          Effect: [ate-q], [train: 0.07463], [test: 0.07585]\n",
      "********************************************************************************\n",
      "epoch: 335 / 500, time cost: 48.72 sec, \n",
      "          Loss: [Train: 2.25559], [Test: 2.35357],\n",
      "          Accuracy: [prop score:  0.95302], [q1: 0.66106], [q0: 0.88006],\n",
      "          Effect: [ate-q], [train: 0.07673], [test: 0.07799]\n",
      "********************************************************************************\n",
      "epoch: 336 / 500, time cost: 46.53 sec, \n",
      "          Loss: [Train: 2.25550], [Test: 2.35284],\n",
      "          Accuracy: [prop score:  0.95304], [q1: 0.66713], [q0: 0.89368],\n",
      "          Effect: [ate-q], [train: 0.08064], [test: 0.08190]\n",
      "********************************************************************************\n",
      "epoch: 337 / 500, time cost: 55.74 sec, \n",
      "          Loss: [Train: 2.25492], [Test: 2.35339],\n",
      "          Accuracy: [prop score:  0.95303], [q1: 0.69354], [q0: 0.89682],\n",
      "          Effect: [ate-q], [train: 0.08851], [test: 0.08977]\n",
      "********************************************************************************\n",
      "epoch: 338 / 500, time cost: 47.15 sec, \n",
      "          Loss: [Train: 2.25423], [Test: 2.35367],\n",
      "          Accuracy: [prop score:  0.95309], [q1: 0.67180], [q0: 0.90777],\n",
      "          Effect: [ate-q], [train: 0.08519], [test: 0.08652]\n",
      "********************************************************************************\n",
      "epoch: 339 / 500, time cost: 47.90 sec, \n",
      "          Loss: [Train: 2.25390], [Test: 2.35425],\n",
      "          Accuracy: [prop score:  0.95313], [q1: 0.64039], [q0: 0.86205],\n",
      "          Effect: [ate-q], [train: 0.06892], [test: 0.07021]\n",
      "********************************************************************************\n",
      "epoch: 340 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.25346], [Test: 2.35398],\n",
      "          Accuracy: [prop score:  0.95316], [q1: 0.69635], [q0: 0.87694],\n",
      "          Effect: [ate-q], [train: 0.08620], [test: 0.08751]\n",
      "********************************************************************************\n",
      "epoch: 341 / 500, time cost: 47.55 sec, \n",
      "          Loss: [Train: 2.25355], [Test: 2.35424],\n",
      "          Accuracy: [prop score:  0.95317], [q1: 0.66404], [q0: 0.90950],\n",
      "          Effect: [ate-q], [train: 0.08470], [test: 0.08597]\n",
      "********************************************************************************\n",
      "epoch: 342 / 500, time cost: 45.43 sec, \n",
      "          Loss: [Train: 2.25311], [Test: 2.35470],\n",
      "          Accuracy: [prop score:  0.95317], [q1: 0.65791], [q0: 0.88109],\n",
      "          Effect: [ate-q], [train: 0.07640], [test: 0.07775]\n",
      "********************************************************************************\n",
      "epoch: 343 / 500, time cost: 47.68 sec, \n",
      "          Loss: [Train: 2.25195], [Test: 2.35491],\n",
      "          Accuracy: [prop score:  0.95319], [q1: 0.66167], [q0: 0.87785],\n",
      "          Effect: [ate-q], [train: 0.07729], [test: 0.07856]\n",
      "********************************************************************************\n",
      "epoch: 344 / 500, time cost: 45.45 sec, \n",
      "          Loss: [Train: 2.25169], [Test: 2.35492],\n",
      "          Accuracy: [prop score:  0.95326], [q1: 0.65219], [q0: 0.90458],\n",
      "          Effect: [ate-q], [train: 0.08000], [test: 0.08132]\n",
      "********************************************************************************\n",
      "epoch: 345 / 500, time cost: 47.86 sec, \n",
      "          Loss: [Train: 2.25124], [Test: 2.35484],\n",
      "          Accuracy: [prop score:  0.95326], [q1: 0.65159], [q0: 0.89009],\n",
      "          Effect: [ate-q], [train: 0.07762], [test: 0.07893]\n",
      "********************************************************************************\n",
      "epoch: 346 / 500, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.25057], [Test: 2.35551],\n",
      "          Accuracy: [prop score:  0.95339], [q1: 0.67782], [q0: 0.89331],\n",
      "          Effect: [ate-q], [train: 0.08472], [test: 0.08605]\n",
      "********************************************************************************\n",
      "epoch: 347 / 500, time cost: 47.63 sec, \n",
      "          Loss: [Train: 2.24970], [Test: 2.35520],\n",
      "          Accuracy: [prop score:  0.95332], [q1: 0.66485], [q0: 0.86747],\n",
      "          Effect: [ate-q], [train: 0.07678], [test: 0.07812]\n",
      "********************************************************************************\n",
      "epoch: 348 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.24981], [Test: 2.35546],\n",
      "          Accuracy: [prop score:  0.95347], [q1: 0.70696], [q0: 0.88212],\n",
      "          Effect: [ate-q], [train: 0.09153], [test: 0.09291]\n",
      "********************************************************************************\n",
      "epoch: 349 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.24957], [Test: 2.35565],\n",
      "          Accuracy: [prop score:  0.95348], [q1: 0.65926], [q0: 0.90187],\n",
      "          Effect: [ate-q], [train: 0.08204], [test: 0.08337]\n",
      "********************************************************************************\n",
      "epoch: 350 / 500, time cost: 45.93 sec, \n",
      "          Loss: [Train: 2.24928], [Test: 2.35643],\n",
      "          Accuracy: [prop score:  0.95351], [q1: 0.63575], [q0: 0.88300],\n",
      "          Effect: [ate-q], [train: 0.07322], [test: 0.07456]\n",
      "********************************************************************************\n",
      "epoch: 351 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.24842], [Test: 2.35620],\n",
      "          Accuracy: [prop score:  0.95347], [q1: 0.65332], [q0: 0.89724],\n",
      "          Effect: [ate-q], [train: 0.07920], [test: 0.08053]\n",
      "********************************************************************************\n",
      "epoch: 352 / 500, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.24777], [Test: 2.35705],\n",
      "          Accuracy: [prop score:  0.95353], [q1: 0.64804], [q0: 0.88961],\n",
      "          Effect: [ate-q], [train: 0.07689], [test: 0.07824]\n",
      "********************************************************************************\n",
      "epoch: 353 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.24721], [Test: 2.35677],\n",
      "          Accuracy: [prop score:  0.95345], [q1: 0.64380], [q0: 0.88323],\n",
      "          Effect: [ate-q], [train: 0.07488], [test: 0.07633]\n",
      "********************************************************************************\n",
      "epoch: 354 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.24696], [Test: 2.35694],\n",
      "          Accuracy: [prop score:  0.95350], [q1: 0.67474], [q0: 0.87700],\n",
      "          Effect: [ate-q], [train: 0.08166], [test: 0.08305]\n",
      "********************************************************************************\n",
      "epoch: 355 / 500, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.24648], [Test: 2.35686],\n",
      "          Accuracy: [prop score:  0.95350], [q1: 0.66964], [q0: 0.86934],\n",
      "          Effect: [ate-q], [train: 0.07977], [test: 0.08118]\n",
      "********************************************************************************\n",
      "epoch: 356 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.24613], [Test: 2.35771],\n",
      "          Accuracy: [prop score:  0.95352], [q1: 0.66762], [q0: 0.84566],\n",
      "          Effect: [ate-q], [train: 0.07495], [test: 0.07638]\n",
      "********************************************************************************\n",
      "epoch: 357 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.24562], [Test: 2.35763],\n",
      "          Accuracy: [prop score:  0.95354], [q1: 0.63158], [q0: 0.87303],\n",
      "          Effect: [ate-q], [train: 0.07065], [test: 0.07205]\n",
      "********************************************************************************\n",
      "epoch: 358 / 500, time cost: 46.18 sec, \n",
      "          Loss: [Train: 2.24448], [Test: 2.35801],\n",
      "          Accuracy: [prop score:  0.95359], [q1: 0.65968], [q0: 0.88332],\n",
      "          Effect: [ate-q], [train: 0.08042], [test: 0.08186]\n",
      "********************************************************************************\n",
      "epoch: 359 / 500, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.24461], [Test: 2.35730],\n",
      "          Accuracy: [prop score:  0.95367], [q1: 0.67803], [q0: 0.88688],\n",
      "          Effect: [ate-q], [train: 0.08541], [test: 0.08688]\n",
      "********************************************************************************\n",
      "epoch: 360 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.24393], [Test: 2.35793],\n",
      "          Accuracy: [prop score:  0.95372], [q1: 0.66572], [q0: 0.84978],\n",
      "          Effect: [ate-q], [train: 0.07594], [test: 0.07736]\n",
      "********************************************************************************\n",
      "epoch: 361 / 500, time cost: 48.42 sec, \n",
      "          Loss: [Train: 2.24340], [Test: 2.35801],\n",
      "          Accuracy: [prop score:  0.95361], [q1: 0.67034], [q0: 0.86571],\n",
      "          Effect: [ate-q], [train: 0.07989], [test: 0.08138]\n",
      "********************************************************************************\n",
      "epoch: 362 / 500, time cost: 46.35 sec, \n",
      "          Loss: [Train: 2.24286], [Test: 2.35892],\n",
      "          Accuracy: [prop score:  0.95374], [q1: 0.62643], [q0: 0.88591],\n",
      "          Effect: [ate-q], [train: 0.07273], [test: 0.07416]\n",
      "********************************************************************************\n",
      "epoch: 363 / 500, time cost: 48.50 sec, \n",
      "          Loss: [Train: 2.24233], [Test: 2.35867],\n",
      "          Accuracy: [prop score:  0.95375], [q1: 0.66588], [q0: 0.89077],\n",
      "          Effect: [ate-q], [train: 0.08358], [test: 0.08504]\n",
      "********************************************************************************\n",
      "epoch: 364 / 500, time cost: 46.75 sec, \n",
      "          Loss: [Train: 2.24207], [Test: 2.35905],\n",
      "          Accuracy: [prop score:  0.95374], [q1: 0.65280], [q0: 0.86111],\n",
      "          Effect: [ate-q], [train: 0.07595], [test: 0.07740]\n",
      "********************************************************************************\n",
      "epoch: 365 / 500, time cost: 48.65 sec, \n",
      "          Loss: [Train: 2.24165], [Test: 2.35954],\n",
      "          Accuracy: [prop score:  0.95382], [q1: 0.62029], [q0: 0.88226],\n",
      "          Effect: [ate-q], [train: 0.07151], [test: 0.07295]\n",
      "********************************************************************************\n",
      "epoch: 366 / 500, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.24143], [Test: 2.35963],\n",
      "          Accuracy: [prop score:  0.95376], [q1: 0.66295], [q0: 0.88009],\n",
      "          Effect: [ate-q], [train: 0.08152], [test: 0.08298]\n",
      "********************************************************************************\n",
      "epoch: 367 / 500, time cost: 48.06 sec, \n",
      "          Loss: [Train: 2.24073], [Test: 2.35988],\n",
      "          Accuracy: [prop score:  0.95383], [q1: 0.65790], [q0: 0.89051],\n",
      "          Effect: [ate-q], [train: 0.08115], [test: 0.08264]\n",
      "********************************************************************************\n",
      "epoch: 368 / 500, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.24035], [Test: 2.36023],\n",
      "          Accuracy: [prop score:  0.95391], [q1: 0.66029], [q0: 0.87852],\n",
      "          Effect: [ate-q], [train: 0.07926], [test: 0.08078]\n",
      "********************************************************************************\n",
      "epoch: 369 / 500, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.23922], [Test: 2.35997],\n",
      "          Accuracy: [prop score:  0.95397], [q1: 0.65038], [q0: 0.86600],\n",
      "          Effect: [ate-q], [train: 0.07395], [test: 0.07539]\n",
      "********************************************************************************\n",
      "epoch: 370 / 500, time cost: 45.88 sec, \n",
      "          Loss: [Train: 2.23895], [Test: 2.36028],\n",
      "          Accuracy: [prop score:  0.95395], [q1: 0.67538], [q0: 0.85084],\n",
      "          Effect: [ate-q], [train: 0.07990], [test: 0.08139]\n",
      "********************************************************************************\n",
      "epoch: 371 / 500, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.23867], [Test: 2.36004],\n",
      "          Accuracy: [prop score:  0.95398], [q1: 0.68584], [q0: 0.90742],\n",
      "          Effect: [ate-q], [train: 0.09441], [test: 0.09588]\n",
      "********************************************************************************\n",
      "epoch: 372 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.23782], [Test: 2.36063],\n",
      "          Accuracy: [prop score:  0.95398], [q1: 0.68011], [q0: 0.85421],\n",
      "          Effect: [ate-q], [train: 0.08213], [test: 0.08363]\n",
      "********************************************************************************\n",
      "epoch: 373 / 500, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.23765], [Test: 2.36084],\n",
      "          Accuracy: [prop score:  0.95404], [q1: 0.64780], [q0: 0.86950],\n",
      "          Effect: [ate-q], [train: 0.07541], [test: 0.07690]\n",
      "********************************************************************************\n",
      "epoch: 374 / 500, time cost: 45.60 sec, \n",
      "          Loss: [Train: 2.23789], [Test: 2.36134],\n",
      "          Accuracy: [prop score:  0.95410], [q1: 0.64666], [q0: 0.90522],\n",
      "          Effect: [ate-q], [train: 0.08377], [test: 0.08526]\n",
      "********************************************************************************\n",
      "epoch: 375 / 500, time cost: 48.09 sec, \n",
      "          Loss: [Train: 2.23684], [Test: 2.36121],\n",
      "          Accuracy: [prop score:  0.95404], [q1: 0.63853], [q0: 0.88444],\n",
      "          Effect: [ate-q], [train: 0.07655], [test: 0.07812]\n",
      "********************************************************************************\n",
      "epoch: 376 / 500, time cost: 45.89 sec, \n",
      "          Loss: [Train: 2.23617], [Test: 2.36169],\n",
      "          Accuracy: [prop score:  0.95410], [q1: 0.67765], [q0: 0.88971],\n",
      "          Effect: [ate-q], [train: 0.08830], [test: 0.08985]\n",
      "********************************************************************************\n",
      "epoch: 377 / 500, time cost: 47.48 sec, \n",
      "          Loss: [Train: 2.23522], [Test: 2.36192],\n",
      "          Accuracy: [prop score:  0.95418], [q1: 0.69985], [q0: 0.89009],\n",
      "          Effect: [ate-q], [train: 0.09598], [test: 0.09756]\n",
      "********************************************************************************\n",
      "epoch: 378 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.23542], [Test: 2.36203],\n",
      "          Accuracy: [prop score:  0.95413], [q1: 0.63488], [q0: 0.87150],\n",
      "          Effect: [ate-q], [train: 0.07377], [test: 0.07529]\n",
      "********************************************************************************\n",
      "epoch: 379 / 500, time cost: 48.02 sec, \n",
      "          Loss: [Train: 2.23441], [Test: 2.36197],\n",
      "          Accuracy: [prop score:  0.95419], [q1: 0.63429], [q0: 0.89928],\n",
      "          Effect: [ate-q], [train: 0.08084], [test: 0.08238]\n",
      "********************************************************************************\n",
      "epoch: 380 / 500, time cost: 45.94 sec, \n",
      "          Loss: [Train: 2.23476], [Test: 2.36292],\n",
      "          Accuracy: [prop score:  0.95420], [q1: 0.67532], [q0: 0.86341],\n",
      "          Effect: [ate-q], [train: 0.08374], [test: 0.08534]\n",
      "********************************************************************************\n",
      "epoch: 381 / 500, time cost: 47.91 sec, \n",
      "          Loss: [Train: 2.23407], [Test: 2.36270],\n",
      "          Accuracy: [prop score:  0.95425], [q1: 0.66483], [q0: 0.87170],\n",
      "          Effect: [ate-q], [train: 0.08163], [test: 0.08319]\n",
      "********************************************************************************\n",
      "epoch: 382 / 500, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.23342], [Test: 2.36300],\n",
      "          Accuracy: [prop score:  0.95420], [q1: 0.68625], [q0: 0.86485],\n",
      "          Effect: [ate-q], [train: 0.08728], [test: 0.08888]\n",
      "********************************************************************************\n",
      "epoch: 383 / 500, time cost: 47.39 sec, \n",
      "          Loss: [Train: 2.23290], [Test: 2.36326],\n",
      "          Accuracy: [prop score:  0.95421], [q1: 0.63463], [q0: 0.87266],\n",
      "          Effect: [ate-q], [train: 0.07339], [test: 0.07495]\n",
      "********************************************************************************\n",
      "epoch: 384 / 500, time cost: 45.74 sec, \n",
      "          Loss: [Train: 2.23268], [Test: 2.36377],\n",
      "          Accuracy: [prop score:  0.95425], [q1: 0.62875], [q0: 0.86398],\n",
      "          Effect: [ate-q], [train: 0.07091], [test: 0.07251]\n",
      "********************************************************************************\n",
      "epoch: 385 / 500, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.23195], [Test: 2.36382],\n",
      "          Accuracy: [prop score:  0.95425], [q1: 0.63048], [q0: 0.85929],\n",
      "          Effect: [ate-q], [train: 0.07070], [test: 0.07227]\n",
      "********************************************************************************\n",
      "epoch: 386 / 500, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.23164], [Test: 2.36451],\n",
      "          Accuracy: [prop score:  0.95439], [q1: 0.59913], [q0: 0.85920],\n",
      "          Effect: [ate-q], [train: 0.06471], [test: 0.06627]\n",
      "********************************************************************************\n",
      "epoch: 387 / 500, time cost: 48.13 sec, \n",
      "          Loss: [Train: 2.23083], [Test: 2.36486],\n",
      "          Accuracy: [prop score:  0.95433], [q1: 0.60521], [q0: 0.87800],\n",
      "          Effect: [ate-q], [train: 0.06943], [test: 0.07106]\n",
      "********************************************************************************\n",
      "epoch: 388 / 500, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.23076], [Test: 2.36444],\n",
      "          Accuracy: [prop score:  0.95443], [q1: 0.64167], [q0: 0.89036],\n",
      "          Effect: [ate-q], [train: 0.08079], [test: 0.08237]\n",
      "********************************************************************************\n",
      "epoch: 389 / 500, time cost: 48.20 sec, \n",
      "          Loss: [Train: 2.23018], [Test: 2.36520],\n",
      "          Accuracy: [prop score:  0.95442], [q1: 0.63501], [q0: 0.84813],\n",
      "          Effect: [ate-q], [train: 0.07172], [test: 0.07336]\n",
      "********************************************************************************\n",
      "epoch: 390 / 500, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.22918], [Test: 2.36647],\n",
      "          Accuracy: [prop score:  0.95453], [q1: 0.56450], [q0: 0.86223],\n",
      "          Effect: [ate-q], [train: 0.05756], [test: 0.05911]\n",
      "********************************************************************************\n",
      "epoch: 391 / 500, time cost: 48.32 sec, \n",
      "          Loss: [Train: 2.22932], [Test: 2.36556],\n",
      "          Accuracy: [prop score:  0.95444], [q1: 0.64019], [q0: 0.89253],\n",
      "          Effect: [ate-q], [train: 0.08220], [test: 0.08384]\n",
      "********************************************************************************\n",
      "epoch: 392 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.22872], [Test: 2.36547],\n",
      "          Accuracy: [prop score:  0.95452], [q1: 0.64517], [q0: 0.85616],\n",
      "          Effect: [ate-q], [train: 0.07542], [test: 0.07706]\n",
      "********************************************************************************\n",
      "epoch: 393 / 500, time cost: 48.24 sec, \n",
      "          Loss: [Train: 2.22780], [Test: 2.36571],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.69524], [q0: 0.86415],\n",
      "          Effect: [ate-q], [train: 0.09322], [test: 0.09488]\n",
      "********************************************************************************\n",
      "epoch: 394 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.22753], [Test: 2.36558],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.65855], [q0: 0.88287],\n",
      "          Effect: [ate-q], [train: 0.08398], [test: 0.08561]\n",
      "********************************************************************************\n",
      "epoch: 395 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.22676], [Test: 2.36661],\n",
      "          Accuracy: [prop score:  0.95454], [q1: 0.62545], [q0: 0.87192],\n",
      "          Effect: [ate-q], [train: 0.07344], [test: 0.07508]\n",
      "********************************************************************************\n",
      "epoch: 396 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.22650], [Test: 2.36619],\n",
      "          Accuracy: [prop score:  0.95460], [q1: 0.67548], [q0: 0.86124],\n",
      "          Effect: [ate-q], [train: 0.08623], [test: 0.08789]\n",
      "********************************************************************************\n",
      "epoch: 397 / 500, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.22643], [Test: 2.36685],\n",
      "          Accuracy: [prop score:  0.95461], [q1: 0.63582], [q0: 0.85490],\n",
      "          Effect: [ate-q], [train: 0.07408], [test: 0.07577]\n",
      "********************************************************************************\n",
      "epoch: 398 / 500, time cost: 45.49 sec, \n",
      "          Loss: [Train: 2.22589], [Test: 2.36843],\n",
      "          Accuracy: [prop score:  0.95467], [q1: 0.60956], [q0: 0.88428],\n",
      "          Effect: [ate-q], [train: 0.07052], [test: 0.07216]\n",
      "********************************************************************************\n",
      "epoch: 399 / 500, time cost: 47.21 sec, \n",
      "          Loss: [Train: 2.22446], [Test: 2.36726],\n",
      "          Accuracy: [prop score:  0.95473], [q1: 0.61752], [q0: 0.85583],\n",
      "          Effect: [ate-q], [train: 0.06869], [test: 0.07034]\n",
      "********************************************************************************\n",
      "epoch: 400 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.22436], [Test: 2.36825],\n",
      "          Accuracy: [prop score:  0.95468], [q1: 0.63466], [q0: 0.86403],\n",
      "          Effect: [ate-q], [train: 0.07455], [test: 0.07623]\n",
      "********************************************************************************\n",
      "epoch: 401 / 500, time cost: 47.52 sec, \n",
      "          Loss: [Train: 2.22391], [Test: 2.36938],\n",
      "          Accuracy: [prop score:  0.95479], [q1: 0.55231], [q0: 0.83943],\n",
      "          Effect: [ate-q], [train: 0.05044], [test: 0.05207]\n",
      "********************************************************************************\n",
      "epoch: 402 / 500, time cost: 45.06 sec, \n",
      "          Loss: [Train: 2.22358], [Test: 2.36828],\n",
      "          Accuracy: [prop score:  0.95477], [q1: 0.62040], [q0: 0.86403],\n",
      "          Effect: [ate-q], [train: 0.07209], [test: 0.07375]\n",
      "********************************************************************************\n",
      "epoch: 403 / 500, time cost: 47.49 sec, \n",
      "          Loss: [Train: 2.22306], [Test: 2.36919],\n",
      "          Accuracy: [prop score:  0.95481], [q1: 0.59318], [q0: 0.84338],\n",
      "          Effect: [ate-q], [train: 0.06237], [test: 0.06407]\n",
      "********************************************************************************\n",
      "epoch: 404 / 500, time cost: 45.52 sec, \n",
      "          Loss: [Train: 2.22279], [Test: 2.36893],\n",
      "          Accuracy: [prop score:  0.95485], [q1: 0.64496], [q0: 0.87819],\n",
      "          Effect: [ate-q], [train: 0.08063], [test: 0.08232]\n",
      "********************************************************************************\n",
      "epoch: 405 / 500, time cost: 47.86 sec, \n",
      "          Loss: [Train: 2.22215], [Test: 2.36882],\n",
      "          Accuracy: [prop score:  0.95479], [q1: 0.63187], [q0: 0.87030],\n",
      "          Effect: [ate-q], [train: 0.07601], [test: 0.07769]\n",
      "********************************************************************************\n",
      "epoch: 406 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.22160], [Test: 2.36992],\n",
      "          Accuracy: [prop score:  0.95486], [q1: 0.60096], [q0: 0.86482],\n",
      "          Effect: [ate-q], [train: 0.06751], [test: 0.06920]\n",
      "********************************************************************************\n",
      "epoch: 407 / 500, time cost: 48.54 sec, \n",
      "          Loss: [Train: 2.22136], [Test: 2.36943],\n",
      "          Accuracy: [prop score:  0.95488], [q1: 0.61931], [q0: 0.83917],\n",
      "          Effect: [ate-q], [train: 0.06819], [test: 0.06988]\n",
      "********************************************************************************\n",
      "epoch: 408 / 500, time cost: 46.42 sec, \n",
      "          Loss: [Train: 2.22034], [Test: 2.37023],\n",
      "          Accuracy: [prop score:  0.95491], [q1: 0.67501], [q0: 0.85061],\n",
      "          Effect: [ate-q], [train: 0.08572], [test: 0.08755]\n",
      "********************************************************************************\n",
      "epoch: 409 / 500, time cost: 48.44 sec, \n",
      "          Loss: [Train: 2.22056], [Test: 2.37075],\n",
      "          Accuracy: [prop score:  0.95494], [q1: 0.61713], [q0: 0.83249],\n",
      "          Effect: [ate-q], [train: 0.06618], [test: 0.06787]\n",
      "********************************************************************************\n",
      "epoch: 410 / 500, time cost: 46.48 sec, \n",
      "          Loss: [Train: 2.21949], [Test: 2.37068],\n",
      "          Accuracy: [prop score:  0.95490], [q1: 0.61777], [q0: 0.83306],\n",
      "          Effect: [ate-q], [train: 0.06584], [test: 0.06756]\n",
      "********************************************************************************\n",
      "epoch: 411 / 500, time cost: 48.79 sec, \n",
      "          Loss: [Train: 2.21905], [Test: 2.37098],\n",
      "          Accuracy: [prop score:  0.95490], [q1: 0.62635], [q0: 0.85879],\n",
      "          Effect: [ate-q], [train: 0.07296], [test: 0.07470]\n",
      "********************************************************************************\n",
      "epoch: 412 / 500, time cost: 46.31 sec, \n",
      "          Loss: [Train: 2.21850], [Test: 2.37184],\n",
      "          Accuracy: [prop score:  0.95498], [q1: 0.58834], [q0: 0.85891],\n",
      "          Effect: [ate-q], [train: 0.06289], [test: 0.06466]\n",
      "********************************************************************************\n",
      "epoch: 413 / 500, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.21827], [Test: 2.37116],\n",
      "          Accuracy: [prop score:  0.95486], [q1: 0.64704], [q0: 0.84852],\n",
      "          Effect: [ate-q], [train: 0.07764], [test: 0.07941]\n",
      "********************************************************************************\n",
      "epoch: 414 / 500, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.21755], [Test: 2.37176],\n",
      "          Accuracy: [prop score:  0.95493], [q1: 0.62744], [q0: 0.87718],\n",
      "          Effect: [ate-q], [train: 0.07863], [test: 0.08042]\n",
      "********************************************************************************\n",
      "epoch: 415 / 500, time cost: 52.33 sec, \n",
      "          Loss: [Train: 2.21713], [Test: 2.37149],\n",
      "          Accuracy: [prop score:  0.95494], [q1: 0.65473], [q0: 0.87704],\n",
      "          Effect: [ate-q], [train: 0.08634], [test: 0.08813]\n",
      "********************************************************************************\n",
      "epoch: 416 / 500, time cost: 53.33 sec, \n",
      "          Loss: [Train: 2.21608], [Test: 2.37184],\n",
      "          Accuracy: [prop score:  0.95498], [q1: 0.65840], [q0: 0.82329],\n",
      "          Effect: [ate-q], [train: 0.07862], [test: 0.08048]\n",
      "********************************************************************************\n",
      "epoch: 417 / 500, time cost: 55.97 sec, \n",
      "          Loss: [Train: 2.21648], [Test: 2.37279],\n",
      "          Accuracy: [prop score:  0.95498], [q1: 0.63975], [q0: 0.85503],\n",
      "          Effect: [ate-q], [train: 0.07765], [test: 0.07941]\n",
      "********************************************************************************\n",
      "epoch: 418 / 500, time cost: 47.71 sec, \n",
      "          Loss: [Train: 2.21580], [Test: 2.37283],\n",
      "          Accuracy: [prop score:  0.95496], [q1: 0.63896], [q0: 0.86170],\n",
      "          Effect: [ate-q], [train: 0.07764], [test: 0.07943]\n",
      "********************************************************************************\n",
      "epoch: 419 / 500, time cost: 48.45 sec, \n",
      "          Loss: [Train: 2.21566], [Test: 2.37319],\n",
      "          Accuracy: [prop score:  0.95508], [q1: 0.61598], [q0: 0.83265],\n",
      "          Effect: [ate-q], [train: 0.06677], [test: 0.06856]\n",
      "********************************************************************************\n",
      "epoch: 420 / 500, time cost: 46.54 sec, \n",
      "          Loss: [Train: 2.21479], [Test: 2.37287],\n",
      "          Accuracy: [prop score:  0.95491], [q1: 0.62706], [q0: 0.85724],\n",
      "          Effect: [ate-q], [train: 0.07517], [test: 0.07703]\n",
      "********************************************************************************\n",
      "epoch: 421 / 500, time cost: 48.58 sec, \n",
      "          Loss: [Train: 2.21420], [Test: 2.37383],\n",
      "          Accuracy: [prop score:  0.95506], [q1: 0.60976], [q0: 0.85251],\n",
      "          Effect: [ate-q], [train: 0.06708], [test: 0.06885]\n",
      "********************************************************************************\n",
      "epoch: 422 / 500, time cost: 46.62 sec, \n",
      "          Loss: [Train: 2.21388], [Test: 2.37399],\n",
      "          Accuracy: [prop score:  0.95505], [q1: 0.64036], [q0: 0.82768],\n",
      "          Effect: [ate-q], [train: 0.07425], [test: 0.07609]\n",
      "********************************************************************************\n",
      "epoch: 423 / 500, time cost: 48.64 sec, \n",
      "          Loss: [Train: 2.21340], [Test: 2.37498],\n",
      "          Accuracy: [prop score:  0.95503], [q1: 0.63479], [q0: 0.84283],\n",
      "          Effect: [ate-q], [train: 0.07346], [test: 0.07526]\n",
      "********************************************************************************\n",
      "epoch: 424 / 500, time cost: 46.33 sec, \n",
      "          Loss: [Train: 2.21278], [Test: 2.37393],\n",
      "          Accuracy: [prop score:  0.95500], [q1: 0.65594], [q0: 0.85324],\n",
      "          Effect: [ate-q], [train: 0.08249], [test: 0.08437]\n",
      "********************************************************************************\n",
      "epoch: 425 / 500, time cost: 48.58 sec, \n",
      "          Loss: [Train: 2.21211], [Test: 2.37499],\n",
      "          Accuracy: [prop score:  0.95510], [q1: 0.66386], [q0: 0.84497],\n",
      "          Effect: [ate-q], [train: 0.08326], [test: 0.08506]\n",
      "********************************************************************************\n",
      "epoch: 426 / 500, time cost: 46.37 sec, \n",
      "          Loss: [Train: 2.21200], [Test: 2.37533],\n",
      "          Accuracy: [prop score:  0.95503], [q1: 0.65675], [q0: 0.85903],\n",
      "          Effect: [ate-q], [train: 0.08478], [test: 0.08663]\n",
      "********************************************************************************\n",
      "epoch: 427 / 500, time cost: 48.53 sec, \n",
      "          Loss: [Train: 2.21086], [Test: 2.37552],\n",
      "          Accuracy: [prop score:  0.95502], [q1: 0.60842], [q0: 0.86842],\n",
      "          Effect: [ate-q], [train: 0.07203], [test: 0.07388]\n",
      "********************************************************************************\n",
      "epoch: 428 / 500, time cost: 46.16 sec, \n",
      "          Loss: [Train: 2.21006], [Test: 2.37550],\n",
      "          Accuracy: [prop score:  0.95505], [q1: 0.64140], [q0: 0.81837],\n",
      "          Effect: [ate-q], [train: 0.07365], [test: 0.07547]\n",
      "********************************************************************************\n",
      "epoch: 429 / 500, time cost: 48.10 sec, \n",
      "          Loss: [Train: 2.21052], [Test: 2.37660],\n",
      "          Accuracy: [prop score:  0.95501], [q1: 0.59722], [q0: 0.86368],\n",
      "          Effect: [ate-q], [train: 0.06985], [test: 0.07166]\n",
      "********************************************************************************\n",
      "epoch: 430 / 500, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.20955], [Test: 2.37808],\n",
      "          Accuracy: [prop score:  0.95511], [q1: 0.55012], [q0: 0.84637],\n",
      "          Effect: [ate-q], [train: 0.05614], [test: 0.05795]\n",
      "********************************************************************************\n",
      "epoch: 431 / 500, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.20910], [Test: 2.37694],\n",
      "          Accuracy: [prop score:  0.95507], [q1: 0.61427], [q0: 0.88647],\n",
      "          Effect: [ate-q], [train: 0.08022], [test: 0.08208]\n",
      "********************************************************************************\n",
      "epoch: 432 / 500, time cost: 46.54 sec, \n",
      "          Loss: [Train: 2.20876], [Test: 2.37704],\n",
      "          Accuracy: [prop score:  0.95512], [q1: 0.64357], [q0: 0.87934],\n",
      "          Effect: [ate-q], [train: 0.08544], [test: 0.08734]\n",
      "********************************************************************************\n",
      "epoch: 433 / 500, time cost: 47.51 sec, \n",
      "          Loss: [Train: 2.20815], [Test: 2.37743],\n",
      "          Accuracy: [prop score:  0.95509], [q1: 0.66871], [q0: 0.80769],\n",
      "          Effect: [ate-q], [train: 0.08038], [test: 0.08228]\n",
      "********************************************************************************\n",
      "epoch: 434 / 500, time cost: 45.06 sec, \n",
      "          Loss: [Train: 2.20756], [Test: 2.37735],\n",
      "          Accuracy: [prop score:  0.95508], [q1: 0.65987], [q0: 0.83948],\n",
      "          Effect: [ate-q], [train: 0.08349], [test: 0.08538]\n",
      "********************************************************************************\n",
      "epoch: 435 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.20730], [Test: 2.37851],\n",
      "          Accuracy: [prop score:  0.95501], [q1: 0.59433], [q0: 0.81203],\n",
      "          Effect: [ate-q], [train: 0.06068], [test: 0.06255]\n",
      "********************************************************************************\n",
      "epoch: 436 / 500, time cost: 45.99 sec, \n",
      "          Loss: [Train: 2.20695], [Test: 2.37850],\n",
      "          Accuracy: [prop score:  0.95516], [q1: 0.58578], [q0: 0.82998],\n",
      "          Effect: [ate-q], [train: 0.06202], [test: 0.06392]\n",
      "********************************************************************************\n",
      "epoch: 437 / 500, time cost: 47.85 sec, \n",
      "          Loss: [Train: 2.20607], [Test: 2.37851],\n",
      "          Accuracy: [prop score:  0.95515], [q1: 0.63007], [q0: 0.80899],\n",
      "          Effect: [ate-q], [train: 0.06930], [test: 0.07114]\n",
      "********************************************************************************\n",
      "epoch: 438 / 500, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.20585], [Test: 2.37894],\n",
      "          Accuracy: [prop score:  0.95516], [q1: 0.60589], [q0: 0.86153],\n",
      "          Effect: [ate-q], [train: 0.07276], [test: 0.07463]\n",
      "********************************************************************************\n",
      "epoch: 439 / 500, time cost: 47.44 sec, \n",
      "          Loss: [Train: 2.20503], [Test: 2.37938],\n",
      "          Accuracy: [prop score:  0.95516], [q1: 0.63524], [q0: 0.82532],\n",
      "          Effect: [ate-q], [train: 0.07356], [test: 0.07554]\n",
      "********************************************************************************\n",
      "epoch: 440 / 500, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.20468], [Test: 2.37923],\n",
      "          Accuracy: [prop score:  0.95515], [q1: 0.61783], [q0: 0.85407],\n",
      "          Effect: [ate-q], [train: 0.07350], [test: 0.07538]\n",
      "********************************************************************************\n",
      "epoch: 441 / 500, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.20407], [Test: 2.38004],\n",
      "          Accuracy: [prop score:  0.95514], [q1: 0.62965], [q0: 0.83351],\n",
      "          Effect: [ate-q], [train: 0.07330], [test: 0.07521]\n",
      "********************************************************************************\n",
      "epoch: 442 / 500, time cost: 45.63 sec, \n",
      "          Loss: [Train: 2.20344], [Test: 2.38091],\n",
      "          Accuracy: [prop score:  0.95522], [q1: 0.58785], [q0: 0.84440],\n",
      "          Effect: [ate-q], [train: 0.06682], [test: 0.06869]\n",
      "********************************************************************************\n",
      "epoch: 443 / 500, time cost: 47.95 sec, \n",
      "          Loss: [Train: 2.20314], [Test: 2.38063],\n",
      "          Accuracy: [prop score:  0.95525], [q1: 0.62342], [q0: 0.84339],\n",
      "          Effect: [ate-q], [train: 0.07338], [test: 0.07529]\n",
      "********************************************************************************\n",
      "epoch: 444 / 500, time cost: 52.11 sec, \n",
      "          Loss: [Train: 2.20267], [Test: 2.38264],\n",
      "          Accuracy: [prop score:  0.95519], [q1: 0.54920], [q0: 0.83172],\n",
      "          Effect: [ate-q], [train: 0.05290], [test: 0.05476]\n",
      "********************************************************************************\n",
      "epoch: 445 / 500, time cost: 51.56 sec, \n",
      "          Loss: [Train: 2.20258], [Test: 2.38270],\n",
      "          Accuracy: [prop score:  0.95526], [q1: 0.56987], [q0: 0.82172],\n",
      "          Effect: [ate-q], [train: 0.05629], [test: 0.05819]\n",
      "********************************************************************************\n",
      "epoch: 446 / 500, time cost: 46.34 sec, \n",
      "          Loss: [Train: 2.20179], [Test: 2.38147],\n",
      "          Accuracy: [prop score:  0.95520], [q1: 0.65725], [q0: 0.85216],\n",
      "          Effect: [ate-q], [train: 0.08659], [test: 0.08859]\n",
      "********************************************************************************\n",
      "epoch: 447 / 500, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.20129], [Test: 2.38240],\n",
      "          Accuracy: [prop score:  0.95526], [q1: 0.63722], [q0: 0.84737],\n",
      "          Effect: [ate-q], [train: 0.07838], [test: 0.08035]\n",
      "********************************************************************************\n",
      "epoch: 448 / 500, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.20092], [Test: 2.38247],\n",
      "          Accuracy: [prop score:  0.95527], [q1: 0.62788], [q0: 0.85126],\n",
      "          Effect: [ate-q], [train: 0.07715], [test: 0.07915]\n",
      "********************************************************************************\n",
      "epoch: 449 / 500, time cost: 47.88 sec, \n",
      "          Loss: [Train: 2.19989], [Test: 2.38237],\n",
      "          Accuracy: [prop score:  0.95516], [q1: 0.64992], [q0: 0.79776],\n",
      "          Effect: [ate-q], [train: 0.07460], [test: 0.07656]\n",
      "********************************************************************************\n",
      "epoch: 450 / 500, time cost: 45.87 sec, \n",
      "          Loss: [Train: 2.19934], [Test: 2.38311],\n",
      "          Accuracy: [prop score:  0.95523], [q1: 0.63295], [q0: 0.83044],\n",
      "          Effect: [ate-q], [train: 0.07576], [test: 0.07770]\n",
      "********************************************************************************\n",
      "epoch: 451 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.19937], [Test: 2.38402],\n",
      "          Accuracy: [prop score:  0.95518], [q1: 0.59138], [q0: 0.86804],\n",
      "          Effect: [ate-q], [train: 0.07138], [test: 0.07331]\n",
      "********************************************************************************\n",
      "epoch: 452 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.19876], [Test: 2.38425],\n",
      "          Accuracy: [prop score:  0.95527], [q1: 0.60389], [q0: 0.80510],\n",
      "          Effect: [ate-q], [train: 0.06376], [test: 0.06569]\n",
      "********************************************************************************\n",
      "epoch: 453 / 500, time cost: 48.33 sec, \n",
      "          Loss: [Train: 2.19889], [Test: 2.38440],\n",
      "          Accuracy: [prop score:  0.95534], [q1: 0.61605], [q0: 0.87105],\n",
      "          Effect: [ate-q], [train: 0.07781], [test: 0.07981]\n",
      "********************************************************************************\n",
      "epoch: 454 / 500, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.19797], [Test: 2.38571],\n",
      "          Accuracy: [prop score:  0.95530], [q1: 0.58311], [q0: 0.81808],\n",
      "          Effect: [ate-q], [train: 0.05966], [test: 0.06163]\n",
      "********************************************************************************\n",
      "epoch: 455 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.19664], [Test: 2.38533],\n",
      "          Accuracy: [prop score:  0.95523], [q1: 0.58949], [q0: 0.81939],\n",
      "          Effect: [ate-q], [train: 0.06225], [test: 0.06422]\n",
      "********************************************************************************\n",
      "epoch: 456 / 500, time cost: 45.29 sec, \n",
      "          Loss: [Train: 2.19688], [Test: 2.38538],\n",
      "          Accuracy: [prop score:  0.95525], [q1: 0.61435], [q0: 0.80178],\n",
      "          Effect: [ate-q], [train: 0.06545], [test: 0.06739]\n",
      "********************************************************************************\n",
      "epoch: 457 / 500, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.19629], [Test: 2.38555],\n",
      "          Accuracy: [prop score:  0.95525], [q1: 0.60762], [q0: 0.83710],\n",
      "          Effect: [ate-q], [train: 0.06934], [test: 0.07125]\n",
      "********************************************************************************\n",
      "epoch: 458 / 500, time cost: 45.87 sec, \n",
      "          Loss: [Train: 2.19568], [Test: 2.38612],\n",
      "          Accuracy: [prop score:  0.95525], [q1: 0.59888], [q0: 0.82203],\n",
      "          Effect: [ate-q], [train: 0.06417], [test: 0.06614]\n",
      "********************************************************************************\n",
      "epoch: 459 / 500, time cost: 48.01 sec, \n",
      "          Loss: [Train: 2.19509], [Test: 2.38648],\n",
      "          Accuracy: [prop score:  0.95525], [q1: 0.62096], [q0: 0.84528],\n",
      "          Effect: [ate-q], [train: 0.07412], [test: 0.07611]\n",
      "********************************************************************************\n",
      "epoch: 460 / 500, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.19471], [Test: 2.38658],\n",
      "          Accuracy: [prop score:  0.95532], [q1: 0.61109], [q0: 0.84321],\n",
      "          Effect: [ate-q], [train: 0.07366], [test: 0.07568]\n",
      "********************************************************************************\n",
      "epoch: 461 / 500, time cost: 47.92 sec, \n",
      "          Loss: [Train: 2.19394], [Test: 2.38736],\n",
      "          Accuracy: [prop score:  0.95532], [q1: 0.61710], [q0: 0.80896],\n",
      "          Effect: [ate-q], [train: 0.06729], [test: 0.06925]\n",
      "********************************************************************************\n",
      "epoch: 462 / 500, time cost: 45.80 sec, \n",
      "          Loss: [Train: 2.19359], [Test: 2.38752],\n",
      "          Accuracy: [prop score:  0.95528], [q1: 0.62340], [q0: 0.80856],\n",
      "          Effect: [ate-q], [train: 0.06957], [test: 0.07157]\n",
      "********************************************************************************\n",
      "epoch: 463 / 500, time cost: 48.04 sec, \n",
      "          Loss: [Train: 2.19298], [Test: 2.38793],\n",
      "          Accuracy: [prop score:  0.95530], [q1: 0.58380], [q0: 0.81663],\n",
      "          Effect: [ate-q], [train: 0.06026], [test: 0.06224]\n",
      "********************************************************************************\n",
      "epoch: 464 / 500, time cost: 45.46 sec, \n",
      "          Loss: [Train: 2.19308], [Test: 2.38844],\n",
      "          Accuracy: [prop score:  0.95519], [q1: 0.62699], [q0: 0.82131],\n",
      "          Effect: [ate-q], [train: 0.07369], [test: 0.07573]\n",
      "********************************************************************************\n",
      "epoch: 465 / 500, time cost: 46.40 sec, \n",
      "          Loss: [Train: 2.19204], [Test: 2.38815],\n",
      "          Accuracy: [prop score:  0.95531], [q1: 0.60765], [q0: 0.85447],\n",
      "          Effect: [ate-q], [train: 0.07535], [test: 0.07731]\n",
      "********************************************************************************\n",
      "epoch: 466 / 500, time cost: 44.17 sec, \n",
      "          Loss: [Train: 2.19162], [Test: 2.39051],\n",
      "          Accuracy: [prop score:  0.95527], [q1: 0.64065], [q0: 0.85169],\n",
      "          Effect: [ate-q], [train: 0.08361], [test: 0.08572]\n",
      "********************************************************************************\n",
      "epoch: 467 / 500, time cost: 46.13 sec, \n",
      "          Loss: [Train: 2.19101], [Test: 2.39087],\n",
      "          Accuracy: [prop score:  0.95538], [q1: 0.62494], [q0: 0.82986],\n",
      "          Effect: [ate-q], [train: 0.07403], [test: 0.07603]\n",
      "********************************************************************************\n",
      "epoch: 468 / 500, time cost: 44.22 sec, \n",
      "          Loss: [Train: 2.19116], [Test: 2.39135],\n",
      "          Accuracy: [prop score:  0.95531], [q1: 0.62255], [q0: 0.80525],\n",
      "          Effect: [ate-q], [train: 0.06945], [test: 0.07145]\n",
      "********************************************************************************\n",
      "epoch: 469 / 500, time cost: 46.21 sec, \n",
      "          Loss: [Train: 2.18965], [Test: 2.39220],\n",
      "          Accuracy: [prop score:  0.95532], [q1: 0.57775], [q0: 0.82931],\n",
      "          Effect: [ate-q], [train: 0.06053], [test: 0.06255]\n",
      "********************************************************************************\n",
      "epoch: 470 / 500, time cost: 44.24 sec, \n",
      "          Loss: [Train: 2.19021], [Test: 2.39216],\n",
      "          Accuracy: [prop score:  0.95527], [q1: 0.63284], [q0: 0.80151],\n",
      "          Effect: [ate-q], [train: 0.07192], [test: 0.07399]\n",
      "********************************************************************************\n",
      "epoch: 471 / 500, time cost: 46.55 sec, \n",
      "          Loss: [Train: 2.18970], [Test: 2.39251],\n",
      "          Accuracy: [prop score:  0.95528], [q1: 0.61729], [q0: 0.83272],\n",
      "          Effect: [ate-q], [train: 0.07150], [test: 0.07354]\n",
      "********************************************************************************\n",
      "epoch: 472 / 500, time cost: 44.36 sec, \n",
      "          Loss: [Train: 2.18889], [Test: 2.39445],\n",
      "          Accuracy: [prop score:  0.95535], [q1: 0.56531], [q0: 0.79585],\n",
      "          Effect: [ate-q], [train: 0.05368], [test: 0.05571]\n",
      "********************************************************************************\n",
      "epoch: 473 / 500, time cost: 46.69 sec, \n",
      "          Loss: [Train: 2.18862], [Test: 2.39356],\n",
      "          Accuracy: [prop score:  0.95537], [q1: 0.60022], [q0: 0.85669],\n",
      "          Effect: [ate-q], [train: 0.07278], [test: 0.07486]\n",
      "********************************************************************************\n",
      "epoch: 474 / 500, time cost: 44.39 sec, \n",
      "          Loss: [Train: 2.18803], [Test: 2.39463],\n",
      "          Accuracy: [prop score:  0.95536], [q1: 0.59234], [q0: 0.83009],\n",
      "          Effect: [ate-q], [train: 0.06432], [test: 0.06633]\n",
      "********************************************************************************\n",
      "epoch: 475 / 500, time cost: 46.74 sec, \n",
      "          Loss: [Train: 2.18745], [Test: 2.39408],\n",
      "          Accuracy: [prop score:  0.95538], [q1: 0.65154], [q0: 0.81171],\n",
      "          Effect: [ate-q], [train: 0.08093], [test: 0.08298]\n",
      "********************************************************************************\n",
      "epoch: 476 / 500, time cost: 44.59 sec, \n",
      "          Loss: [Train: 2.18715], [Test: 2.39507],\n",
      "          Accuracy: [prop score:  0.95541], [q1: 0.57040], [q0: 0.80457],\n",
      "          Effect: [ate-q], [train: 0.05548], [test: 0.05748]\n",
      "********************************************************************************\n",
      "epoch: 477 / 500, time cost: 47.00 sec, \n",
      "          Loss: [Train: 2.18601], [Test: 2.39507],\n",
      "          Accuracy: [prop score:  0.95529], [q1: 0.67028], [q0: 0.80971],\n",
      "          Effect: [ate-q], [train: 0.08867], [test: 0.09087]\n",
      "********************************************************************************\n",
      "epoch: 478 / 500, time cost: 44.70 sec, \n",
      "          Loss: [Train: 2.18538], [Test: 2.39595],\n",
      "          Accuracy: [prop score:  0.95529], [q1: 0.63098], [q0: 0.75314],\n",
      "          Effect: [ate-q], [train: 0.06478], [test: 0.06687]\n",
      "********************************************************************************\n",
      "epoch: 479 / 500, time cost: 47.07 sec, \n",
      "          Loss: [Train: 2.18558], [Test: 2.39647],\n",
      "          Accuracy: [prop score:  0.95529], [q1: 0.59673], [q0: 0.83643],\n",
      "          Effect: [ate-q], [train: 0.06940], [test: 0.07146]\n",
      "********************************************************************************\n",
      "epoch: 480 / 500, time cost: 44.73 sec, \n",
      "          Loss: [Train: 2.18517], [Test: 2.39656],\n",
      "          Accuracy: [prop score:  0.95539], [q1: 0.59815], [q0: 0.83859],\n",
      "          Effect: [ate-q], [train: 0.07078], [test: 0.07289]\n",
      "********************************************************************************\n",
      "epoch: 481 / 500, time cost: 48.35 sec, \n",
      "          Loss: [Train: 2.18419], [Test: 2.39736],\n",
      "          Accuracy: [prop score:  0.95543], [q1: 0.64634], [q0: 0.72959],\n",
      "          Effect: [ate-q], [train: 0.06504], [test: 0.06721]\n",
      "********************************************************************************\n",
      "epoch: 482 / 500, time cost: 54.20 sec, \n",
      "          Loss: [Train: 2.18373], [Test: 2.39769],\n",
      "          Accuracy: [prop score:  0.95531], [q1: 0.63616], [q0: 0.72625],\n",
      "          Effect: [ate-q], [train: 0.06310], [test: 0.06523]\n",
      "********************************************************************************\n",
      "epoch: 483 / 500, time cost: 55.72 sec, \n",
      "          Loss: [Train: 2.18341], [Test: 2.39757],\n",
      "          Accuracy: [prop score:  0.95538], [q1: 0.60100], [q0: 0.77930],\n",
      "          Effect: [ate-q], [train: 0.05970], [test: 0.06175]\n",
      "********************************************************************************\n",
      "epoch: 484 / 500, time cost: 48.49 sec, \n",
      "          Loss: [Train: 2.18289], [Test: 2.39778],\n",
      "          Accuracy: [prop score:  0.95532], [q1: 0.63997], [q0: 0.80175],\n",
      "          Effect: [ate-q], [train: 0.07673], [test: 0.07886]\n",
      "********************************************************************************\n",
      "epoch: 485 / 500, time cost: 48.90 sec, \n",
      "          Loss: [Train: 2.18269], [Test: 2.39808],\n",
      "          Accuracy: [prop score:  0.95539], [q1: 0.62920], [q0: 0.81867],\n",
      "          Effect: [ate-q], [train: 0.07604], [test: 0.07809]\n",
      "********************************************************************************\n",
      "epoch: 486 / 500, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.18143], [Test: 2.39864],\n",
      "          Accuracy: [prop score:  0.95543], [q1: 0.60606], [q0: 0.79249],\n",
      "          Effect: [ate-q], [train: 0.06374], [test: 0.06580]\n",
      "********************************************************************************\n",
      "epoch: 487 / 500, time cost: 47.98 sec, \n",
      "          Loss: [Train: 2.18130], [Test: 2.39962],\n",
      "          Accuracy: [prop score:  0.95546], [q1: 0.61151], [q0: 0.78134],\n",
      "          Effect: [ate-q], [train: 0.06282], [test: 0.06486]\n",
      "********************************************************************************\n",
      "epoch: 488 / 500, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.18120], [Test: 2.39981],\n",
      "          Accuracy: [prop score:  0.95545], [q1: 0.59389], [q0: 0.78896],\n",
      "          Effect: [ate-q], [train: 0.06225], [test: 0.06436]\n",
      "********************************************************************************\n",
      "epoch: 489 / 500, time cost: 48.87 sec, \n",
      "          Loss: [Train: 2.18109], [Test: 2.39985],\n",
      "          Accuracy: [prop score:  0.95549], [q1: 0.59489], [q0: 0.80931],\n",
      "          Effect: [ate-q], [train: 0.06448], [test: 0.06659]\n",
      "********************************************************************************\n",
      "epoch: 490 / 500, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.17968], [Test: 2.40231],\n",
      "          Accuracy: [prop score:  0.95540], [q1: 0.51587], [q0: 0.83361],\n",
      "          Effect: [ate-q], [train: 0.04920], [test: 0.05119]\n",
      "********************************************************************************\n",
      "epoch: 491 / 500, time cost: 48.16 sec, \n",
      "          Loss: [Train: 2.17977], [Test: 2.40123],\n",
      "          Accuracy: [prop score:  0.95543], [q1: 0.60006], [q0: 0.81640],\n",
      "          Effect: [ate-q], [train: 0.06705], [test: 0.06911]\n",
      "********************************************************************************\n",
      "epoch: 492 / 500, time cost: 45.47 sec, \n",
      "          Loss: [Train: 2.17915], [Test: 2.40062],\n",
      "          Accuracy: [prop score:  0.95538], [q1: 0.61219], [q0: 0.79214],\n",
      "          Effect: [ate-q], [train: 0.06684], [test: 0.06889]\n",
      "********************************************************************************\n",
      "epoch: 493 / 500, time cost: 48.38 sec, \n",
      "          Loss: [Train: 2.17881], [Test: 2.40281],\n",
      "          Accuracy: [prop score:  0.95536], [q1: 0.56589], [q0: 0.82430],\n",
      "          Effect: [ate-q], [train: 0.05883], [test: 0.06096]\n",
      "********************************************************************************\n",
      "epoch: 494 / 500, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.17779], [Test: 2.40227],\n",
      "          Accuracy: [prop score:  0.95534], [q1: 0.59386], [q0: 0.82969],\n",
      "          Effect: [ate-q], [train: 0.07095], [test: 0.07303]\n",
      "********************************************************************************\n",
      "epoch: 495 / 500, time cost: 48.23 sec, \n",
      "          Loss: [Train: 2.17753], [Test: 2.40244],\n",
      "          Accuracy: [prop score:  0.95538], [q1: 0.64061], [q0: 0.78144],\n",
      "          Effect: [ate-q], [train: 0.07252], [test: 0.07464]\n",
      "********************************************************************************\n",
      "epoch: 496 / 500, time cost: 45.47 sec, \n",
      "          Loss: [Train: 2.17714], [Test: 2.40349],\n",
      "          Accuracy: [prop score:  0.95536], [q1: 0.57984], [q0: 0.77448],\n",
      "          Effect: [ate-q], [train: 0.05514], [test: 0.05728]\n",
      "********************************************************************************\n",
      "epoch: 497 / 500, time cost: 47.69 sec, \n",
      "          Loss: [Train: 2.17667], [Test: 2.40393],\n",
      "          Accuracy: [prop score:  0.95540], [q1: 0.60150], [q0: 0.78881],\n",
      "          Effect: [ate-q], [train: 0.06167], [test: 0.06379]\n",
      "********************************************************************************\n",
      "epoch: 498 / 500, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.17583], [Test: 2.40338],\n",
      "          Accuracy: [prop score:  0.95540], [q1: 0.64582], [q0: 0.81768],\n",
      "          Effect: [ate-q], [train: 0.08323], [test: 0.08540]\n",
      "********************************************************************************\n",
      "epoch: 499 / 500, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.17579], [Test: 2.40502],\n",
      "          Accuracy: [prop score:  0.95541], [q1: 0.56599], [q0: 0.81065],\n",
      "          Effect: [ate-q], [train: 0.05802], [test: 0.06016]\n",
      "********************************************************************************\n",
      "epoch: 500 / 500, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.17545], [Test: 2.40459],\n",
      "          Accuracy: [prop score:  0.95544], [q1: 0.66203], [q0: 0.78354],\n",
      "          Effect: [ate-q], [train: 0.08087], [test: 0.08307]\n",
      "********************************************************************************\n",
      "Finish training...\n"
     ]
    }
   ],
   "source": [
    "rs_loss, rq1_loss, rq0_loss = [0.] * 3\n",
    "\n",
    "train_loss_hist, test_loss_hist, est_effect = [], [], []\n",
    "for e in range(1, epoch + 1):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        run_loss += loss.item()\n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / {epoch}, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW in 500 epochs，lr=5e-6, no scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 501 / 800, time cost: 49.07 sec, \n",
      "          Loss: [Train: 2.17496], [Test: 2.40462],\n",
      "          Accuracy: [prop score:  0.95546], [q1: 0.62851], [q0: 0.82109],\n",
      "          Effect: [ate-q], [train: 0.07702], [test: 0.07920]\n",
      "********************************************************************************\n",
      "epoch: 502 / 800, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.17431], [Test: 2.40531],\n",
      "          Accuracy: [prop score:  0.95542], [q1: 0.60986], [q0: 0.75659],\n",
      "          Effect: [ate-q], [train: 0.05990], [test: 0.06210]\n",
      "********************************************************************************\n",
      "epoch: 503 / 800, time cost: 46.23 sec, \n",
      "          Loss: [Train: 2.17500], [Test: 2.40599],\n",
      "          Accuracy: [prop score:  0.95550], [q1: 0.59937], [q0: 0.80214],\n",
      "          Effect: [ate-q], [train: 0.06488], [test: 0.06702]\n",
      "********************************************************************************\n",
      "epoch: 504 / 800, time cost: 48.39 sec, \n",
      "          Loss: [Train: 2.17590], [Test: 2.40629],\n",
      "          Accuracy: [prop score:  0.95549], [q1: 0.58608], [q0: 0.77049],\n",
      "          Effect: [ate-q], [train: 0.05648], [test: 0.05861]\n",
      "********************************************************************************\n",
      "epoch: 505 / 800, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.17449], [Test: 2.40624],\n",
      "          Accuracy: [prop score:  0.95561], [q1: 0.64020], [q0: 0.80220],\n",
      "          Effect: [ate-q], [train: 0.07709], [test: 0.07931]\n",
      "********************************************************************************\n",
      "epoch: 506 / 800, time cost: 48.54 sec, \n",
      "          Loss: [Train: 2.17391], [Test: 2.40716],\n",
      "          Accuracy: [prop score:  0.95547], [q1: 0.59879], [q0: 0.77781],\n",
      "          Effect: [ate-q], [train: 0.06323], [test: 0.06542]\n",
      "********************************************************************************\n",
      "epoch: 507 / 800, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.17365], [Test: 2.40765],\n",
      "          Accuracy: [prop score:  0.95544], [q1: 0.58341], [q0: 0.80619],\n",
      "          Effect: [ate-q], [train: 0.06141], [test: 0.06361]\n",
      "********************************************************************************\n",
      "epoch: 508 / 800, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.17398], [Test: 2.40829],\n",
      "          Accuracy: [prop score:  0.95558], [q1: 0.62047], [q0: 0.79228],\n",
      "          Effect: [ate-q], [train: 0.06970], [test: 0.07188]\n",
      "********************************************************************************\n",
      "epoch: 509 / 800, time cost: 48.27 sec, \n",
      "          Loss: [Train: 2.17306], [Test: 2.40810],\n",
      "          Accuracy: [prop score:  0.95549], [q1: 0.64278], [q0: 0.82344],\n",
      "          Effect: [ate-q], [train: 0.08541], [test: 0.08768]\n",
      "********************************************************************************\n",
      "epoch: 510 / 800, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.17228], [Test: 2.40965],\n",
      "          Accuracy: [prop score:  0.95551], [q1: 0.57292], [q0: 0.79273],\n",
      "          Effect: [ate-q], [train: 0.05565], [test: 0.05775]\n",
      "********************************************************************************\n",
      "epoch: 511 / 800, time cost: 48.36 sec, \n",
      "          Loss: [Train: 2.17177], [Test: 2.40991],\n",
      "          Accuracy: [prop score:  0.95545], [q1: 0.58857], [q0: 0.75894],\n",
      "          Effect: [ate-q], [train: 0.05529], [test: 0.05743]\n",
      "********************************************************************************\n",
      "epoch: 512 / 800, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.17131], [Test: 2.41040],\n",
      "          Accuracy: [prop score:  0.95552], [q1: 0.60354], [q0: 0.78708],\n",
      "          Effect: [ate-q], [train: 0.06496], [test: 0.06713]\n",
      "********************************************************************************\n",
      "epoch: 513 / 800, time cost: 46.22 sec, \n",
      "          Loss: [Train: 2.17028], [Test: 2.41050],\n",
      "          Accuracy: [prop score:  0.95555], [q1: 0.67721], [q0: 0.81283],\n",
      "          Effect: [ate-q], [train: 0.09496], [test: 0.09730]\n",
      "********************************************************************************\n",
      "epoch: 514 / 800, time cost: 50.52 sec, \n",
      "          Loss: [Train: 2.17047], [Test: 2.41099],\n",
      "          Accuracy: [prop score:  0.95554], [q1: 0.59869], [q0: 0.78163],\n",
      "          Effect: [ate-q], [train: 0.06320], [test: 0.06534]\n",
      "********************************************************************************\n",
      "epoch: 515 / 800, time cost: 57.63 sec, \n",
      "          Loss: [Train: 2.17008], [Test: 2.41270],\n",
      "          Accuracy: [prop score:  0.95563], [q1: 0.56981], [q0: 0.77353],\n",
      "          Effect: [ate-q], [train: 0.05298], [test: 0.05508]\n",
      "********************************************************************************\n",
      "epoch: 516 / 800, time cost: 50.55 sec, \n",
      "          Loss: [Train: 2.17015], [Test: 2.41204],\n",
      "          Accuracy: [prop score:  0.95562], [q1: 0.57900], [q0: 0.77772],\n",
      "          Effect: [ate-q], [train: 0.05569], [test: 0.05785]\n",
      "********************************************************************************\n",
      "epoch: 517 / 800, time cost: 51.89 sec, \n",
      "          Loss: [Train: 2.16870], [Test: 2.41222],\n",
      "          Accuracy: [prop score:  0.95568], [q1: 0.61055], [q0: 0.78212],\n",
      "          Effect: [ate-q], [train: 0.06647], [test: 0.06864]\n",
      "********************************************************************************\n",
      "epoch: 518 / 800, time cost: 46.71 sec, \n",
      "          Loss: [Train: 2.16874], [Test: 2.41268],\n",
      "          Accuracy: [prop score:  0.95561], [q1: 0.58219], [q0: 0.77608],\n",
      "          Effect: [ate-q], [train: 0.05593], [test: 0.05809]\n",
      "********************************************************************************\n",
      "epoch: 519 / 800, time cost: 48.30 sec, \n",
      "          Loss: [Train: 2.16824], [Test: 2.41316],\n",
      "          Accuracy: [prop score:  0.95567], [q1: 0.60062], [q0: 0.81601],\n",
      "          Effect: [ate-q], [train: 0.06972], [test: 0.07181]\n",
      "********************************************************************************\n",
      "epoch: 520 / 800, time cost: 46.59 sec, \n",
      "          Loss: [Train: 2.16735], [Test: 2.41381],\n",
      "          Accuracy: [prop score:  0.95563], [q1: 0.59116], [q0: 0.78552],\n",
      "          Effect: [ate-q], [train: 0.06034], [test: 0.06258]\n",
      "********************************************************************************\n",
      "epoch: 521 / 800, time cost: 48.29 sec, \n",
      "          Loss: [Train: 2.16732], [Test: 2.41321],\n",
      "          Accuracy: [prop score:  0.95561], [q1: 0.62777], [q0: 0.79054],\n",
      "          Effect: [ate-q], [train: 0.07426], [test: 0.07654]\n",
      "********************************************************************************\n",
      "epoch: 522 / 800, time cost: 47.32 sec, \n",
      "          Loss: [Train: 2.16688], [Test: 2.41418],\n",
      "          Accuracy: [prop score:  0.95565], [q1: 0.61705], [q0: 0.79880],\n",
      "          Effect: [ate-q], [train: 0.07223], [test: 0.07445]\n",
      "********************************************************************************\n",
      "epoch: 523 / 800, time cost: 45.27 sec, \n",
      "          Loss: [Train: 2.16597], [Test: 2.41628],\n",
      "          Accuracy: [prop score:  0.95562], [q1: 0.52744], [q0: 0.80438],\n",
      "          Effect: [ate-q], [train: 0.04824], [test: 0.05031]\n",
      "********************************************************************************\n",
      "epoch: 524 / 800, time cost: 47.45 sec, \n",
      "          Loss: [Train: 2.16570], [Test: 2.41512],\n",
      "          Accuracy: [prop score:  0.95558], [q1: 0.62100], [q0: 0.76060],\n",
      "          Effect: [ate-q], [train: 0.06799], [test: 0.07016]\n",
      "********************************************************************************\n",
      "epoch: 525 / 800, time cost: 45.65 sec, \n",
      "          Loss: [Train: 2.16523], [Test: 2.41585],\n",
      "          Accuracy: [prop score:  0.95561], [q1: 0.59032], [q0: 0.76417],\n",
      "          Effect: [ate-q], [train: 0.05764], [test: 0.05982]\n",
      "********************************************************************************\n",
      "epoch: 526 / 800, time cost: 47.83 sec, \n",
      "          Loss: [Train: 2.16476], [Test: 2.41658],\n",
      "          Accuracy: [prop score:  0.95564], [q1: 0.60311], [q0: 0.74625],\n",
      "          Effect: [ate-q], [train: 0.05834], [test: 0.06057]\n",
      "********************************************************************************\n",
      "epoch: 527 / 800, time cost: 45.33 sec, \n",
      "          Loss: [Train: 2.16417], [Test: 2.41741],\n",
      "          Accuracy: [prop score:  0.95568], [q1: 0.58321], [q0: 0.73903],\n",
      "          Effect: [ate-q], [train: 0.05298], [test: 0.05514]\n",
      "********************************************************************************\n",
      "epoch: 528 / 800, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.16415], [Test: 2.41732],\n",
      "          Accuracy: [prop score:  0.95564], [q1: 0.62223], [q0: 0.75944],\n",
      "          Effect: [ate-q], [train: 0.06560], [test: 0.06782]\n",
      "********************************************************************************\n",
      "epoch: 529 / 800, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.16413], [Test: 2.41815],\n",
      "          Accuracy: [prop score:  0.95558], [q1: 0.58048], [q0: 0.79000],\n",
      "          Effect: [ate-q], [train: 0.06054], [test: 0.06274]\n",
      "********************************************************************************\n",
      "epoch: 530 / 800, time cost: 47.43 sec, \n",
      "          Loss: [Train: 2.16339], [Test: 2.41753],\n",
      "          Accuracy: [prop score:  0.95555], [q1: 0.60416], [q0: 0.81031],\n",
      "          Effect: [ate-q], [train: 0.07234], [test: 0.07461]\n",
      "********************************************************************************\n",
      "epoch: 531 / 800, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.16322], [Test: 2.41816],\n",
      "          Accuracy: [prop score:  0.95559], [q1: 0.61691], [q0: 0.76246],\n",
      "          Effect: [ate-q], [train: 0.06731], [test: 0.06952]\n",
      "********************************************************************************\n",
      "epoch: 532 / 800, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.16203], [Test: 2.41838],\n",
      "          Accuracy: [prop score:  0.95575], [q1: 0.62045], [q0: 0.77593],\n",
      "          Effect: [ate-q], [train: 0.07043], [test: 0.07263]\n",
      "********************************************************************************\n",
      "epoch: 533 / 800, time cost: 45.76 sec, \n",
      "          Loss: [Train: 2.16148], [Test: 2.41965],\n",
      "          Accuracy: [prop score:  0.95570], [q1: 0.56791], [q0: 0.78010],\n",
      "          Effect: [ate-q], [train: 0.05714], [test: 0.05932]\n",
      "********************************************************************************\n",
      "epoch: 534 / 800, time cost: 47.53 sec, \n",
      "          Loss: [Train: 2.16151], [Test: 2.41933],\n",
      "          Accuracy: [prop score:  0.95575], [q1: 0.63260], [q0: 0.79297],\n",
      "          Effect: [ate-q], [train: 0.07750], [test: 0.07976]\n",
      "********************************************************************************\n",
      "epoch: 535 / 800, time cost: 45.31 sec, \n",
      "          Loss: [Train: 2.16078], [Test: 2.42062],\n",
      "          Accuracy: [prop score:  0.95569], [q1: 0.56908], [q0: 0.77928],\n",
      "          Effect: [ate-q], [train: 0.05533], [test: 0.05750]\n",
      "********************************************************************************\n",
      "epoch: 536 / 800, time cost: 47.39 sec, \n",
      "          Loss: [Train: 2.16073], [Test: 2.42016],\n",
      "          Accuracy: [prop score:  0.95575], [q1: 0.58869], [q0: 0.77828],\n",
      "          Effect: [ate-q], [train: 0.06033], [test: 0.06251]\n",
      "********************************************************************************\n",
      "epoch: 537 / 800, time cost: 45.47 sec, \n",
      "          Loss: [Train: 2.16018], [Test: 2.42142],\n",
      "          Accuracy: [prop score:  0.95570], [q1: 0.58921], [q0: 0.76605],\n",
      "          Effect: [ate-q], [train: 0.05892], [test: 0.06118]\n",
      "********************************************************************************\n",
      "epoch: 538 / 800, time cost: 45.41 sec, \n",
      "          Loss: [Train: 2.15932], [Test: 2.42243],\n",
      "          Accuracy: [prop score:  0.95566], [q1: 0.55305], [q0: 0.73493],\n",
      "          Effect: [ate-q], [train: 0.04335], [test: 0.04558]\n",
      "********************************************************************************\n",
      "epoch: 539 / 800, time cost: 47.81 sec, \n",
      "          Loss: [Train: 2.15988], [Test: 2.42213],\n",
      "          Accuracy: [prop score:  0.95570], [q1: 0.59282], [q0: 0.76155],\n",
      "          Effect: [ate-q], [train: 0.05837], [test: 0.06061]\n",
      "********************************************************************************\n",
      "epoch: 540 / 800, time cost: 45.41 sec, \n",
      "          Loss: [Train: 2.15898], [Test: 2.42362],\n",
      "          Accuracy: [prop score:  0.95563], [q1: 0.56648], [q0: 0.75791],\n",
      "          Effect: [ate-q], [train: 0.05091], [test: 0.05312]\n",
      "********************************************************************************\n",
      "epoch: 541 / 800, time cost: 47.42 sec, \n",
      "          Loss: [Train: 2.15816], [Test: 2.42356],\n",
      "          Accuracy: [prop score:  0.95566], [q1: 0.61882], [q0: 0.80102],\n",
      "          Effect: [ate-q], [train: 0.07509], [test: 0.07731]\n",
      "********************************************************************************\n",
      "epoch: 542 / 800, time cost: 45.39 sec, \n",
      "          Loss: [Train: 2.15729], [Test: 2.42326],\n",
      "          Accuracy: [prop score:  0.95568], [q1: 0.57608], [q0: 0.77211],\n",
      "          Effect: [ate-q], [train: 0.05708], [test: 0.05932]\n",
      "********************************************************************************\n",
      "epoch: 543 / 800, time cost: 45.35 sec, \n",
      "          Loss: [Train: 2.15706], [Test: 2.42354],\n",
      "          Accuracy: [prop score:  0.95571], [q1: 0.58584], [q0: 0.75327],\n",
      "          Effect: [ate-q], [train: 0.05688], [test: 0.05920]\n",
      "********************************************************************************\n",
      "epoch: 544 / 800, time cost: 48.12 sec, \n",
      "          Loss: [Train: 2.15701], [Test: 2.42418],\n",
      "          Accuracy: [prop score:  0.95580], [q1: 0.57968], [q0: 0.76015],\n",
      "          Effect: [ate-q], [train: 0.05496], [test: 0.05716]\n",
      "********************************************************************************\n",
      "epoch: 545 / 800, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.15717], [Test: 2.42434],\n",
      "          Accuracy: [prop score:  0.95577], [q1: 0.63112], [q0: 0.77460],\n",
      "          Effect: [ate-q], [train: 0.07465], [test: 0.07691]\n",
      "********************************************************************************\n",
      "epoch: 546 / 800, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.15607], [Test: 2.42518],\n",
      "          Accuracy: [prop score:  0.95567], [q1: 0.59846], [q0: 0.79229],\n",
      "          Effect: [ate-q], [train: 0.06909], [test: 0.07136]\n",
      "********************************************************************************\n",
      "epoch: 547 / 800, time cost: 45.78 sec, \n",
      "          Loss: [Train: 2.15556], [Test: 2.42615],\n",
      "          Accuracy: [prop score:  0.95578], [q1: 0.60658], [q0: 0.79371],\n",
      "          Effect: [ate-q], [train: 0.07102], [test: 0.07327]\n",
      "********************************************************************************\n",
      "epoch: 548 / 800, time cost: 46.53 sec, \n",
      "          Loss: [Train: 2.15512], [Test: 2.42722],\n",
      "          Accuracy: [prop score:  0.95565], [q1: 0.56469], [q0: 0.72340],\n",
      "          Effect: [ate-q], [train: 0.04566], [test: 0.04792]\n",
      "********************************************************************************\n",
      "epoch: 549 / 800, time cost: 48.90 sec, \n",
      "          Loss: [Train: 2.15442], [Test: 2.42702],\n",
      "          Accuracy: [prop score:  0.95568], [q1: 0.58263], [q0: 0.75484],\n",
      "          Effect: [ate-q], [train: 0.05604], [test: 0.05826]\n",
      "********************************************************************************\n",
      "epoch: 550 / 800, time cost: 46.34 sec, \n",
      "          Loss: [Train: 2.15439], [Test: 2.42686],\n",
      "          Accuracy: [prop score:  0.95577], [q1: 0.57703], [q0: 0.79309],\n",
      "          Effect: [ate-q], [train: 0.06035], [test: 0.06256]\n",
      "********************************************************************************\n",
      "epoch: 551 / 800, time cost: 48.08 sec, \n",
      "          Loss: [Train: 2.15354], [Test: 2.42775],\n",
      "          Accuracy: [prop score:  0.95574], [q1: 0.57185], [q0: 0.75091],\n",
      "          Effect: [ate-q], [train: 0.05172], [test: 0.05391]\n",
      "********************************************************************************\n",
      "epoch: 552 / 800, time cost: 45.83 sec, \n",
      "          Loss: [Train: 2.15271], [Test: 2.42857],\n",
      "          Accuracy: [prop score:  0.95576], [q1: 0.61566], [q0: 0.79081],\n",
      "          Effect: [ate-q], [train: 0.07270], [test: 0.07501]\n",
      "********************************************************************************\n",
      "epoch: 553 / 800, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.15262], [Test: 2.42941],\n",
      "          Accuracy: [prop score:  0.95571], [q1: 0.56640], [q0: 0.78966],\n",
      "          Effect: [ate-q], [train: 0.05943], [test: 0.06172]\n",
      "********************************************************************************\n",
      "epoch: 554 / 800, time cost: 47.88 sec, \n",
      "          Loss: [Train: 2.15320], [Test: 2.42866],\n",
      "          Accuracy: [prop score:  0.95573], [q1: 0.59755], [q0: 0.78253],\n",
      "          Effect: [ate-q], [train: 0.06661], [test: 0.06884]\n",
      "********************************************************************************\n",
      "epoch: 555 / 800, time cost: 45.90 sec, \n",
      "          Loss: [Train: 2.15260], [Test: 2.42927],\n",
      "          Accuracy: [prop score:  0.95578], [q1: 0.62750], [q0: 0.72526],\n",
      "          Effect: [ate-q], [train: 0.06561], [test: 0.06793]\n",
      "********************************************************************************\n",
      "epoch: 556 / 800, time cost: 48.00 sec, \n",
      "          Loss: [Train: 2.15162], [Test: 2.43020],\n",
      "          Accuracy: [prop score:  0.95582], [q1: 0.60195], [q0: 0.76183],\n",
      "          Effect: [ate-q], [train: 0.06358], [test: 0.06582]\n",
      "********************************************************************************\n",
      "epoch: 557 / 800, time cost: 45.76 sec, \n",
      "          Loss: [Train: 2.15179], [Test: 2.43079],\n",
      "          Accuracy: [prop score:  0.95573], [q1: 0.58837], [q0: 0.74250],\n",
      "          Effect: [ate-q], [train: 0.05622], [test: 0.05846]\n",
      "********************************************************************************\n",
      "epoch: 558 / 800, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.15080], [Test: 2.43092],\n",
      "          Accuracy: [prop score:  0.95581], [q1: 0.59331], [q0: 0.79115],\n",
      "          Effect: [ate-q], [train: 0.06743], [test: 0.06968]\n",
      "********************************************************************************\n",
      "epoch: 559 / 800, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.14986], [Test: 2.43066],\n",
      "          Accuracy: [prop score:  0.95566], [q1: 0.58928], [q0: 0.73397],\n",
      "          Effect: [ate-q], [train: 0.05533], [test: 0.05753]\n",
      "********************************************************************************\n",
      "epoch: 560 / 800, time cost: 48.69 sec, \n",
      "          Loss: [Train: 2.15012], [Test: 2.43156],\n",
      "          Accuracy: [prop score:  0.95565], [q1: 0.61203], [q0: 0.76257],\n",
      "          Effect: [ate-q], [train: 0.06857], [test: 0.07081]\n",
      "********************************************************************************\n",
      "epoch: 561 / 800, time cost: 54.97 sec, \n",
      "          Loss: [Train: 2.14993], [Test: 2.43321],\n",
      "          Accuracy: [prop score:  0.95570], [q1: 0.55945], [q0: 0.81709],\n",
      "          Effect: [ate-q], [train: 0.06466], [test: 0.06695]\n",
      "********************************************************************************\n",
      "epoch: 562 / 800, time cost: 53.14 sec, \n",
      "          Loss: [Train: 2.14855], [Test: 2.43258],\n",
      "          Accuracy: [prop score:  0.95561], [q1: 0.59940], [q0: 0.72397],\n",
      "          Effect: [ate-q], [train: 0.05752], [test: 0.05986]\n",
      "********************************************************************************\n",
      "epoch: 563 / 800, time cost: 52.85 sec, \n",
      "          Loss: [Train: 2.14907], [Test: 2.43327],\n",
      "          Accuracy: [prop score:  0.95570], [q1: 0.62196], [q0: 0.75473],\n",
      "          Effect: [ate-q], [train: 0.06805], [test: 0.07026]\n",
      "********************************************************************************\n",
      "epoch: 564 / 800, time cost: 54.97 sec, \n",
      "          Loss: [Train: 2.14774], [Test: 2.43328],\n",
      "          Accuracy: [prop score:  0.95568], [q1: 0.59277], [q0: 0.74409],\n",
      "          Effect: [ate-q], [train: 0.05904], [test: 0.06130]\n",
      "********************************************************************************\n",
      "epoch: 565 / 800, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.14748], [Test: 2.43504],\n",
      "          Accuracy: [prop score:  0.95575], [q1: 0.57681], [q0: 0.78895],\n",
      "          Effect: [ate-q], [train: 0.06186], [test: 0.06412]\n",
      "********************************************************************************\n",
      "epoch: 566 / 800, time cost: 52.50 sec, \n",
      "          Loss: [Train: 2.14696], [Test: 2.43493],\n",
      "          Accuracy: [prop score:  0.95571], [q1: 0.57737], [q0: 0.71993],\n",
      "          Effect: [ate-q], [train: 0.04947], [test: 0.05169]\n",
      "********************************************************************************\n",
      "epoch: 567 / 800, time cost: 55.89 sec, \n",
      "          Loss: [Train: 2.14666], [Test: 2.43515],\n",
      "          Accuracy: [prop score:  0.95574], [q1: 0.59982], [q0: 0.75416],\n",
      "          Effect: [ate-q], [train: 0.06111], [test: 0.06332]\n",
      "********************************************************************************\n",
      "epoch: 568 / 800, time cost: 49.45 sec, \n",
      "          Loss: [Train: 2.14631], [Test: 2.43540],\n",
      "          Accuracy: [prop score:  0.95568], [q1: 0.64122], [q0: 0.75006],\n",
      "          Effect: [ate-q], [train: 0.07560], [test: 0.07794]\n",
      "********************************************************************************\n",
      "epoch: 569 / 800, time cost: 57.44 sec, \n",
      "          Loss: [Train: 2.14596], [Test: 2.43616],\n",
      "          Accuracy: [prop score:  0.95576], [q1: 0.59764], [q0: 0.77438],\n",
      "          Effect: [ate-q], [train: 0.06486], [test: 0.06719]\n",
      "********************************************************************************\n",
      "epoch: 570 / 800, time cost: 46.68 sec, \n",
      "          Loss: [Train: 2.14566], [Test: 2.43607],\n",
      "          Accuracy: [prop score:  0.95570], [q1: 0.60656], [q0: 0.76627],\n",
      "          Effect: [ate-q], [train: 0.06803], [test: 0.07032]\n",
      "********************************************************************************\n",
      "epoch: 571 / 800, time cost: 48.18 sec, \n",
      "          Loss: [Train: 2.14491], [Test: 2.43639],\n",
      "          Accuracy: [prop score:  0.95580], [q1: 0.62336], [q0: 0.78053],\n",
      "          Effect: [ate-q], [train: 0.07694], [test: 0.07927]\n",
      "********************************************************************************\n",
      "epoch: 572 / 800, time cost: 45.47 sec, \n",
      "          Loss: [Train: 2.14503], [Test: 2.43726],\n",
      "          Accuracy: [prop score:  0.95571], [q1: 0.61842], [q0: 0.77963],\n",
      "          Effect: [ate-q], [train: 0.07416], [test: 0.07645]\n",
      "********************************************************************************\n",
      "epoch: 573 / 800, time cost: 45.39 sec, \n",
      "          Loss: [Train: 2.14518], [Test: 2.43852],\n",
      "          Accuracy: [prop score:  0.95575], [q1: 0.57087], [q0: 0.78368],\n",
      "          Effect: [ate-q], [train: 0.06416], [test: 0.06641]\n",
      "********************************************************************************\n",
      "epoch: 574 / 800, time cost: 47.40 sec, \n",
      "          Loss: [Train: 2.14463], [Test: 2.43827],\n",
      "          Accuracy: [prop score:  0.95579], [q1: 0.61245], [q0: 0.78373],\n",
      "          Effect: [ate-q], [train: 0.07218], [test: 0.07447]\n",
      "********************************************************************************\n",
      "epoch: 575 / 800, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.14401], [Test: 2.44051],\n",
      "          Accuracy: [prop score:  0.95582], [q1: 0.53601], [q0: 0.76973],\n",
      "          Effect: [ate-q], [train: 0.04968], [test: 0.05187]\n",
      "********************************************************************************\n",
      "epoch: 576 / 800, time cost: 47.55 sec, \n",
      "          Loss: [Train: 2.14347], [Test: 2.43918],\n",
      "          Accuracy: [prop score:  0.95580], [q1: 0.56219], [q0: 0.76457],\n",
      "          Effect: [ate-q], [train: 0.05578], [test: 0.05799]\n",
      "********************************************************************************\n",
      "epoch: 577 / 800, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.14302], [Test: 2.43878],\n",
      "          Accuracy: [prop score:  0.95576], [q1: 0.61726], [q0: 0.76863],\n",
      "          Effect: [ate-q], [train: 0.07316], [test: 0.07549]\n",
      "********************************************************************************\n",
      "epoch: 578 / 800, time cost: 45.63 sec, \n",
      "          Loss: [Train: 2.14234], [Test: 2.43984],\n",
      "          Accuracy: [prop score:  0.95580], [q1: 0.61868], [q0: 0.73999],\n",
      "          Effect: [ate-q], [train: 0.06718], [test: 0.06952]\n",
      "********************************************************************************\n",
      "epoch: 579 / 800, time cost: 47.69 sec, \n",
      "          Loss: [Train: 2.14243], [Test: 2.44075],\n",
      "          Accuracy: [prop score:  0.95583], [q1: 0.59327], [q0: 0.73203],\n",
      "          Effect: [ate-q], [train: 0.05888], [test: 0.06115]\n",
      "********************************************************************************\n",
      "epoch: 580 / 800, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.14120], [Test: 2.44137],\n",
      "          Accuracy: [prop score:  0.95587], [q1: 0.56887], [q0: 0.75502],\n",
      "          Effect: [ate-q], [train: 0.05715], [test: 0.05942]\n",
      "********************************************************************************\n",
      "epoch: 581 / 800, time cost: 47.99 sec, \n",
      "          Loss: [Train: 2.14089], [Test: 2.44158],\n",
      "          Accuracy: [prop score:  0.95580], [q1: 0.60981], [q0: 0.76159],\n",
      "          Effect: [ate-q], [train: 0.06752], [test: 0.06985]\n",
      "********************************************************************************\n",
      "epoch: 582 / 800, time cost: 46.27 sec, \n",
      "          Loss: [Train: 2.14139], [Test: 2.44310],\n",
      "          Accuracy: [prop score:  0.95587], [q1: 0.55679], [q0: 0.76213],\n",
      "          Effect: [ate-q], [train: 0.05134], [test: 0.05360]\n",
      "********************************************************************************\n",
      "epoch: 583 / 800, time cost: 45.57 sec, \n",
      "          Loss: [Train: 2.14041], [Test: 2.44372],\n",
      "          Accuracy: [prop score:  0.95592], [q1: 0.57092], [q0: 0.71471],\n",
      "          Effect: [ate-q], [train: 0.04718], [test: 0.04941]\n",
      "********************************************************************************\n",
      "epoch: 584 / 800, time cost: 48.26 sec, \n",
      "          Loss: [Train: 2.14022], [Test: 2.44375],\n",
      "          Accuracy: [prop score:  0.95575], [q1: 0.55560], [q0: 0.75046],\n",
      "          Effect: [ate-q], [train: 0.04860], [test: 0.05093]\n",
      "********************************************************************************\n",
      "epoch: 585 / 800, time cost: 45.43 sec, \n",
      "          Loss: [Train: 2.13924], [Test: 2.44318],\n",
      "          Accuracy: [prop score:  0.95577], [q1: 0.62153], [q0: 0.74000],\n",
      "          Effect: [ate-q], [train: 0.06932], [test: 0.07165]\n",
      "********************************************************************************\n",
      "epoch: 586 / 800, time cost: 47.17 sec, \n",
      "          Loss: [Train: 2.13901], [Test: 2.44269],\n",
      "          Accuracy: [prop score:  0.95582], [q1: 0.59901], [q0: 0.75255],\n",
      "          Effect: [ate-q], [train: 0.06427], [test: 0.06654]\n",
      "********************************************************************************\n",
      "epoch: 587 / 800, time cost: 44.89 sec, \n",
      "          Loss: [Train: 2.13842], [Test: 2.44542],\n",
      "          Accuracy: [prop score:  0.95580], [q1: 0.54832], [q0: 0.73503],\n",
      "          Effect: [ate-q], [train: 0.04606], [test: 0.04833]\n",
      "********************************************************************************\n",
      "epoch: 588 / 800, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.13845], [Test: 2.44395],\n",
      "          Accuracy: [prop score:  0.95569], [q1: 0.62782], [q0: 0.73378],\n",
      "          Effect: [ate-q], [train: 0.07032], [test: 0.07271]\n",
      "********************************************************************************\n",
      "epoch: 589 / 800, time cost: 57.60 sec, \n",
      "          Loss: [Train: 2.13786], [Test: 2.44449],\n",
      "          Accuracy: [prop score:  0.95578], [q1: 0.63584], [q0: 0.73037],\n",
      "          Effect: [ate-q], [train: 0.07303], [test: 0.07541]\n",
      "********************************************************************************\n",
      "epoch: 590 / 800, time cost: 51.09 sec, \n",
      "          Loss: [Train: 2.13698], [Test: 2.44716],\n",
      "          Accuracy: [prop score:  0.95579], [q1: 0.56487], [q0: 0.73160],\n",
      "          Effect: [ate-q], [train: 0.04730], [test: 0.04959]\n",
      "********************************************************************************\n",
      "epoch: 591 / 800, time cost: 49.46 sec, \n",
      "          Loss: [Train: 2.13685], [Test: 2.44605],\n",
      "          Accuracy: [prop score:  0.95591], [q1: 0.59942], [q0: 0.70192],\n",
      "          Effect: [ate-q], [train: 0.05397], [test: 0.05623]\n",
      "********************************************************************************\n",
      "epoch: 592 / 800, time cost: 46.46 sec, \n",
      "          Loss: [Train: 2.13667], [Test: 2.44755],\n",
      "          Accuracy: [prop score:  0.95573], [q1: 0.54609], [q0: 0.72753],\n",
      "          Effect: [ate-q], [train: 0.04404], [test: 0.04624]\n",
      "********************************************************************************\n",
      "epoch: 593 / 800, time cost: 46.77 sec, \n",
      "          Loss: [Train: 2.13589], [Test: 2.44855],\n",
      "          Accuracy: [prop score:  0.95590], [q1: 0.54763], [q0: 0.76331],\n",
      "          Effect: [ate-q], [train: 0.04934], [test: 0.05158]\n",
      "********************************************************************************\n",
      "epoch: 594 / 800, time cost: 47.39 sec, \n",
      "          Loss: [Train: 2.13606], [Test: 2.44716],\n",
      "          Accuracy: [prop score:  0.95586], [q1: 0.57272], [q0: 0.71036],\n",
      "          Effect: [ate-q], [train: 0.04771], [test: 0.05004]\n",
      "********************************************************************************\n",
      "epoch: 595 / 800, time cost: 46.79 sec, \n",
      "          Loss: [Train: 2.13488], [Test: 2.44975],\n",
      "          Accuracy: [prop score:  0.95577], [q1: 0.53938], [q0: 0.72583],\n",
      "          Effect: [ate-q], [train: 0.04099], [test: 0.04327]\n",
      "********************************************************************************\n",
      "epoch: 596 / 800, time cost: 49.49 sec, \n",
      "          Loss: [Train: 2.13468], [Test: 2.44917],\n",
      "          Accuracy: [prop score:  0.95589], [q1: 0.59370], [q0: 0.69470],\n",
      "          Effect: [ate-q], [train: 0.05198], [test: 0.05432]\n",
      "********************************************************************************\n",
      "epoch: 597 / 800, time cost: 45.44 sec, \n",
      "          Loss: [Train: 2.13421], [Test: 2.44966],\n",
      "          Accuracy: [prop score:  0.95593], [q1: 0.59088], [q0: 0.74723],\n",
      "          Effect: [ate-q], [train: 0.05944], [test: 0.06172]\n",
      "********************************************************************************\n",
      "epoch: 598 / 800, time cost: 44.83 sec, \n",
      "          Loss: [Train: 2.13406], [Test: 2.45089],\n",
      "          Accuracy: [prop score:  0.95591], [q1: 0.57344], [q0: 0.68620],\n",
      "          Effect: [ate-q], [train: 0.04648], [test: 0.04873]\n",
      "********************************************************************************\n",
      "epoch: 599 / 800, time cost: 47.52 sec, \n",
      "          Loss: [Train: 2.13384], [Test: 2.45035],\n",
      "          Accuracy: [prop score:  0.95589], [q1: 0.59376], [q0: 0.74315],\n",
      "          Effect: [ate-q], [train: 0.06320], [test: 0.06552]\n",
      "********************************************************************************\n",
      "epoch: 600 / 800, time cost: 45.20 sec, \n",
      "          Loss: [Train: 2.13309], [Test: 2.45301],\n",
      "          Accuracy: [prop score:  0.95586], [q1: 0.53604], [q0: 0.75464],\n",
      "          Effect: [ate-q], [train: 0.04703], [test: 0.04938]\n",
      "********************************************************************************\n",
      "epoch: 601 / 800, time cost: 47.69 sec, \n",
      "          Loss: [Train: 2.13307], [Test: 2.45235],\n",
      "          Accuracy: [prop score:  0.95583], [q1: 0.58427], [q0: 0.74140],\n",
      "          Effect: [ate-q], [train: 0.05777], [test: 0.05998]\n",
      "********************************************************************************\n",
      "epoch: 602 / 800, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.13225], [Test: 2.45201],\n",
      "          Accuracy: [prop score:  0.95582], [q1: 0.61484], [q0: 0.67360],\n",
      "          Effect: [ate-q], [train: 0.05661], [test: 0.05901]\n",
      "********************************************************************************\n",
      "epoch: 603 / 800, time cost: 45.39 sec, \n",
      "          Loss: [Train: 2.13194], [Test: 2.45285],\n",
      "          Accuracy: [prop score:  0.95585], [q1: 0.56008], [q0: 0.72490],\n",
      "          Effect: [ate-q], [train: 0.04799], [test: 0.05032]\n",
      "********************************************************************************\n",
      "epoch: 604 / 800, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.13247], [Test: 2.45227],\n",
      "          Accuracy: [prop score:  0.95582], [q1: 0.61886], [q0: 0.76084],\n",
      "          Effect: [ate-q], [train: 0.07527], [test: 0.07760]\n",
      "********************************************************************************\n",
      "epoch: 605 / 800, time cost: 45.23 sec, \n",
      "          Loss: [Train: 2.13127], [Test: 2.45300],\n",
      "          Accuracy: [prop score:  0.95589], [q1: 0.58130], [q0: 0.70010],\n",
      "          Effect: [ate-q], [train: 0.04988], [test: 0.05216]\n",
      "********************************************************************************\n",
      "epoch: 606 / 800, time cost: 47.56 sec, \n",
      "          Loss: [Train: 2.13070], [Test: 2.45357],\n",
      "          Accuracy: [prop score:  0.95593], [q1: 0.61267], [q0: 0.72790],\n",
      "          Effect: [ate-q], [train: 0.06591], [test: 0.06824]\n",
      "********************************************************************************\n",
      "epoch: 607 / 800, time cost: 45.54 sec, \n",
      "          Loss: [Train: 2.13022], [Test: 2.45332],\n",
      "          Accuracy: [prop score:  0.95591], [q1: 0.59178], [q0: 0.73949],\n",
      "          Effect: [ate-q], [train: 0.06225], [test: 0.06458]\n",
      "********************************************************************************\n",
      "epoch: 608 / 800, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.13063], [Test: 2.45438],\n",
      "          Accuracy: [prop score:  0.95593], [q1: 0.59125], [q0: 0.75536],\n",
      "          Effect: [ate-q], [train: 0.06400], [test: 0.06631]\n",
      "********************************************************************************\n",
      "epoch: 609 / 800, time cost: 47.47 sec, \n",
      "          Loss: [Train: 2.12933], [Test: 2.45450],\n",
      "          Accuracy: [prop score:  0.95597], [q1: 0.60758], [q0: 0.75160],\n",
      "          Effect: [ate-q], [train: 0.06912], [test: 0.07136]\n",
      "********************************************************************************\n",
      "epoch: 610 / 800, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.12979], [Test: 2.45485],\n",
      "          Accuracy: [prop score:  0.95590], [q1: 0.59122], [q0: 0.71884],\n",
      "          Effect: [ate-q], [train: 0.05979], [test: 0.06207]\n",
      "********************************************************************************\n",
      "epoch: 611 / 800, time cost: 47.64 sec, \n",
      "          Loss: [Train: 2.12943], [Test: 2.45530],\n",
      "          Accuracy: [prop score:  0.95593], [q1: 0.60511], [q0: 0.72069],\n",
      "          Effect: [ate-q], [train: 0.06197], [test: 0.06429]\n",
      "********************************************************************************\n",
      "epoch: 612 / 800, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.12878], [Test: 2.45622],\n",
      "          Accuracy: [prop score:  0.95597], [q1: 0.62820], [q0: 0.76308],\n",
      "          Effect: [ate-q], [train: 0.07795], [test: 0.08032]\n",
      "********************************************************************************\n",
      "epoch: 613 / 800, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.12808], [Test: 2.45843],\n",
      "          Accuracy: [prop score:  0.95595], [q1: 0.54814], [q0: 0.74879],\n",
      "          Effect: [ate-q], [train: 0.05009], [test: 0.05240]\n",
      "********************************************************************************\n",
      "epoch: 614 / 800, time cost: 47.65 sec, \n",
      "          Loss: [Train: 2.12779], [Test: 2.46068],\n",
      "          Accuracy: [prop score:  0.95592], [q1: 0.50743], [q0: 0.75523],\n",
      "          Effect: [ate-q], [train: 0.04233], [test: 0.04455]\n",
      "********************************************************************************\n",
      "epoch: 615 / 800, time cost: 44.92 sec, \n",
      "          Loss: [Train: 2.12711], [Test: 2.45876],\n",
      "          Accuracy: [prop score:  0.95592], [q1: 0.55745], [q0: 0.73980],\n",
      "          Effect: [ate-q], [train: 0.05200], [test: 0.05429]\n",
      "********************************************************************************\n",
      "epoch: 616 / 800, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.12718], [Test: 2.45766],\n",
      "          Accuracy: [prop score:  0.95611], [q1: 0.61961], [q0: 0.72592],\n",
      "          Effect: [ate-q], [train: 0.06777], [test: 0.07016]\n",
      "********************************************************************************\n",
      "epoch: 617 / 800, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.12686], [Test: 2.45853],\n",
      "          Accuracy: [prop score:  0.95611], [q1: 0.60485], [q0: 0.75618],\n",
      "          Effect: [ate-q], [train: 0.06859], [test: 0.07093]\n",
      "********************************************************************************\n",
      "epoch: 618 / 800, time cost: 46.53 sec, \n",
      "          Loss: [Train: 2.12620], [Test: 2.46097],\n",
      "          Accuracy: [prop score:  0.95610], [q1: 0.56069], [q0: 0.71706],\n",
      "          Effect: [ate-q], [train: 0.04717], [test: 0.04947]\n",
      "********************************************************************************\n",
      "epoch: 619 / 800, time cost: 55.07 sec, \n",
      "          Loss: [Train: 2.12582], [Test: 2.46101],\n",
      "          Accuracy: [prop score:  0.95612], [q1: 0.56484], [q0: 0.73535],\n",
      "          Effect: [ate-q], [train: 0.05221], [test: 0.05453]\n",
      "********************************************************************************\n",
      "epoch: 620 / 800, time cost: 52.91 sec, \n",
      "          Loss: [Train: 2.12569], [Test: 2.46020],\n",
      "          Accuracy: [prop score:  0.95605], [q1: 0.58382], [q0: 0.72434],\n",
      "          Effect: [ate-q], [train: 0.05774], [test: 0.06012]\n",
      "********************************************************************************\n",
      "epoch: 621 / 800, time cost: 53.94 sec, \n",
      "          Loss: [Train: 2.12459], [Test: 2.46404],\n",
      "          Accuracy: [prop score:  0.95606], [q1: 0.52733], [q0: 0.71335],\n",
      "          Effect: [ate-q], [train: 0.03788], [test: 0.04010]\n",
      "********************************************************************************\n",
      "epoch: 622 / 800, time cost: 46.07 sec, \n",
      "          Loss: [Train: 2.12484], [Test: 2.46530],\n",
      "          Accuracy: [prop score:  0.95611], [q1: 0.50816], [q0: 0.73220],\n",
      "          Effect: [ate-q], [train: 0.03476], [test: 0.03694]\n",
      "********************************************************************************\n",
      "epoch: 623 / 800, time cost: 45.52 sec, \n",
      "          Loss: [Train: 2.12465], [Test: 2.46168],\n",
      "          Accuracy: [prop score:  0.95603], [q1: 0.62294], [q0: 0.68357],\n",
      "          Effect: [ate-q], [train: 0.06303], [test: 0.06538]\n",
      "********************************************************************************\n",
      "epoch: 624 / 800, time cost: 47.60 sec, \n",
      "          Loss: [Train: 2.12394], [Test: 2.46382],\n",
      "          Accuracy: [prop score:  0.95608], [q1: 0.56029], [q0: 0.72432],\n",
      "          Effect: [ate-q], [train: 0.04856], [test: 0.05084]\n",
      "********************************************************************************\n",
      "epoch: 625 / 800, time cost: 45.66 sec, \n",
      "          Loss: [Train: 2.12374], [Test: 2.46311],\n",
      "          Accuracy: [prop score:  0.95614], [q1: 0.58610], [q0: 0.71957],\n",
      "          Effect: [ate-q], [train: 0.05429], [test: 0.05656]\n",
      "********************************************************************************\n",
      "epoch: 626 / 800, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.12323], [Test: 2.46265],\n",
      "          Accuracy: [prop score:  0.95607], [q1: 0.59576], [q0: 0.71308],\n",
      "          Effect: [ate-q], [train: 0.05871], [test: 0.06102]\n",
      "********************************************************************************\n",
      "epoch: 627 / 800, time cost: 45.72 sec, \n",
      "          Loss: [Train: 2.12242], [Test: 2.46411],\n",
      "          Accuracy: [prop score:  0.95610], [q1: 0.62383], [q0: 0.66278],\n",
      "          Effect: [ate-q], [train: 0.05693], [test: 0.05932]\n",
      "********************************************************************************\n",
      "epoch: 628 / 800, time cost: 45.73 sec, \n",
      "          Loss: [Train: 2.12199], [Test: 2.46311],\n",
      "          Accuracy: [prop score:  0.95612], [q1: 0.63097], [q0: 0.72141],\n",
      "          Effect: [ate-q], [train: 0.07174], [test: 0.07416]\n",
      "********************************************************************************\n",
      "epoch: 629 / 800, time cost: 47.90 sec, \n",
      "          Loss: [Train: 2.12189], [Test: 2.46541],\n",
      "          Accuracy: [prop score:  0.95614], [q1: 0.58615], [q0: 0.78558],\n",
      "          Effect: [ate-q], [train: 0.07051], [test: 0.07284]\n",
      "********************************************************************************\n",
      "epoch: 630 / 800, time cost: 45.70 sec, \n",
      "          Loss: [Train: 2.12147], [Test: 2.46440],\n",
      "          Accuracy: [prop score:  0.95613], [q1: 0.62782], [q0: 0.74402],\n",
      "          Effect: [ate-q], [train: 0.07618], [test: 0.07856]\n",
      "********************************************************************************\n",
      "epoch: 631 / 800, time cost: 48.26 sec, \n",
      "          Loss: [Train: 2.12032], [Test: 2.46675],\n",
      "          Accuracy: [prop score:  0.95610], [q1: 0.59786], [q0: 0.69368],\n",
      "          Effect: [ate-q], [train: 0.05468], [test: 0.05698]\n",
      "********************************************************************************\n",
      "epoch: 632 / 800, time cost: 45.79 sec, \n",
      "          Loss: [Train: 2.12110], [Test: 2.46657],\n",
      "          Accuracy: [prop score:  0.95621], [q1: 0.59135], [q0: 0.75261],\n",
      "          Effect: [ate-q], [train: 0.06365], [test: 0.06603]\n",
      "********************************************************************************\n",
      "epoch: 633 / 800, time cost: 45.53 sec, \n",
      "          Loss: [Train: 2.12017], [Test: 2.46587],\n",
      "          Accuracy: [prop score:  0.95625], [q1: 0.61996], [q0: 0.73739],\n",
      "          Effect: [ate-q], [train: 0.07045], [test: 0.07285]\n",
      "********************************************************************************\n",
      "epoch: 634 / 800, time cost: 47.62 sec, \n",
      "          Loss: [Train: 2.11993], [Test: 2.46722],\n",
      "          Accuracy: [prop score:  0.95614], [q1: 0.58963], [q0: 0.66172],\n",
      "          Effect: [ate-q], [train: 0.04864], [test: 0.05092]\n",
      "********************************************************************************\n",
      "epoch: 635 / 800, time cost: 45.24 sec, \n",
      "          Loss: [Train: 2.11972], [Test: 2.47136],\n",
      "          Accuracy: [prop score:  0.95614], [q1: 0.50071], [q0: 0.72279],\n",
      "          Effect: [ate-q], [train: 0.03326], [test: 0.03547]\n",
      "********************************************************************************\n",
      "epoch: 636 / 800, time cost: 47.63 sec, \n",
      "          Loss: [Train: 2.12012], [Test: 2.46911],\n",
      "          Accuracy: [prop score:  0.95623], [q1: 0.55924], [q0: 0.74905],\n",
      "          Effect: [ate-q], [train: 0.05440], [test: 0.05661]\n",
      "********************************************************************************\n",
      "epoch: 637 / 800, time cost: 45.82 sec, \n",
      "          Loss: [Train: 2.11861], [Test: 2.47057],\n",
      "          Accuracy: [prop score:  0.95625], [q1: 0.54948], [q0: 0.74236],\n",
      "          Effect: [ate-q], [train: 0.05121], [test: 0.05354]\n",
      "********************************************************************************\n",
      "epoch: 638 / 800, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.11832], [Test: 2.46950],\n",
      "          Accuracy: [prop score:  0.95619], [q1: 0.60632], [q0: 0.71693],\n",
      "          Effect: [ate-q], [train: 0.06148], [test: 0.06381]\n",
      "********************************************************************************\n",
      "epoch: 639 / 800, time cost: 47.58 sec, \n",
      "          Loss: [Train: 2.11736], [Test: 2.46984],\n",
      "          Accuracy: [prop score:  0.95613], [q1: 0.59143], [q0: 0.73781],\n",
      "          Effect: [ate-q], [train: 0.06402], [test: 0.06635]\n",
      "********************************************************************************\n",
      "epoch: 640 / 800, time cost: 45.66 sec, \n",
      "          Loss: [Train: 2.11736], [Test: 2.47110],\n",
      "          Accuracy: [prop score:  0.95619], [q1: 0.56638], [q0: 0.66920],\n",
      "          Effect: [ate-q], [train: 0.04377], [test: 0.04603]\n",
      "********************************************************************************\n",
      "epoch: 641 / 800, time cost: 47.69 sec, \n",
      "          Loss: [Train: 2.11699], [Test: 2.47400],\n",
      "          Accuracy: [prop score:  0.95629], [q1: 0.51357], [q0: 0.74724],\n",
      "          Effect: [ate-q], [train: 0.04002], [test: 0.04228]\n",
      "********************************************************************************\n",
      "epoch: 642 / 800, time cost: 45.48 sec, \n",
      "          Loss: [Train: 2.11693], [Test: 2.47278],\n",
      "          Accuracy: [prop score:  0.95621], [q1: 0.55829], [q0: 0.73089],\n",
      "          Effect: [ate-q], [train: 0.05221], [test: 0.05448]\n",
      "********************************************************************************\n",
      "epoch: 643 / 800, time cost: 45.58 sec, \n",
      "          Loss: [Train: 2.11617], [Test: 2.47257],\n",
      "          Accuracy: [prop score:  0.95622], [q1: 0.55486], [q0: 0.72759],\n",
      "          Effect: [ate-q], [train: 0.04917], [test: 0.05142]\n",
      "********************************************************************************\n",
      "epoch: 644 / 800, time cost: 47.40 sec, \n",
      "          Loss: [Train: 2.11600], [Test: 2.47159],\n",
      "          Accuracy: [prop score:  0.95620], [q1: 0.60147], [q0: 0.69146],\n",
      "          Effect: [ate-q], [train: 0.05638], [test: 0.05863]\n",
      "********************************************************************************\n",
      "epoch: 645 / 800, time cost: 45.36 sec, \n",
      "          Loss: [Train: 2.11598], [Test: 2.47330],\n",
      "          Accuracy: [prop score:  0.95609], [q1: 0.56499], [q0: 0.74797],\n",
      "          Effect: [ate-q], [train: 0.05897], [test: 0.06127]\n",
      "********************************************************************************\n",
      "epoch: 646 / 800, time cost: 47.74 sec, \n",
      "          Loss: [Train: 2.11563], [Test: 2.47216],\n",
      "          Accuracy: [prop score:  0.95625], [q1: 0.62579], [q0: 0.75460],\n",
      "          Effect: [ate-q], [train: 0.08156], [test: 0.08394]\n",
      "********************************************************************************\n",
      "epoch: 647 / 800, time cost: 45.15 sec, \n",
      "          Loss: [Train: 2.11439], [Test: 2.47540],\n",
      "          Accuracy: [prop score:  0.95626], [q1: 0.54257], [q0: 0.78950],\n",
      "          Effect: [ate-q], [train: 0.06257], [test: 0.06487]\n",
      "********************************************************************************\n",
      "epoch: 648 / 800, time cost: 45.17 sec, \n",
      "          Loss: [Train: 2.11494], [Test: 2.47330],\n",
      "          Accuracy: [prop score:  0.95623], [q1: 0.62923], [q0: 0.74531],\n",
      "          Effect: [ate-q], [train: 0.07994], [test: 0.08229]\n",
      "********************************************************************************\n",
      "epoch: 649 / 800, time cost: 47.21 sec, \n",
      "          Loss: [Train: 2.11439], [Test: 2.47610],\n",
      "          Accuracy: [prop score:  0.95623], [q1: 0.56239], [q0: 0.69549],\n",
      "          Effect: [ate-q], [train: 0.04198], [test: 0.04423]\n",
      "********************************************************************************\n",
      "epoch: 650 / 800, time cost: 45.09 sec, \n",
      "          Loss: [Train: 2.11336], [Test: 2.47562],\n",
      "          Accuracy: [prop score:  0.95621], [q1: 0.55685], [q0: 0.71562],\n",
      "          Effect: [ate-q], [train: 0.04956], [test: 0.05189]\n",
      "********************************************************************************\n",
      "epoch: 651 / 800, time cost: 47.30 sec, \n",
      "          Loss: [Train: 2.11323], [Test: 2.47911],\n",
      "          Accuracy: [prop score:  0.95635], [q1: 0.51664], [q0: 0.71533],\n",
      "          Effect: [ate-q], [train: 0.03480], [test: 0.03696]\n",
      "********************************************************************************\n",
      "epoch: 652 / 800, time cost: 45.14 sec, \n",
      "          Loss: [Train: 2.11309], [Test: 2.47688],\n",
      "          Accuracy: [prop score:  0.95617], [q1: 0.57886], [q0: 0.73747],\n",
      "          Effect: [ate-q], [train: 0.05881], [test: 0.06124]\n",
      "********************************************************************************\n",
      "epoch: 653 / 800, time cost: 45.11 sec, \n",
      "          Loss: [Train: 2.11222], [Test: 2.47686],\n",
      "          Accuracy: [prop score:  0.95620], [q1: 0.57307], [q0: 0.76571],\n",
      "          Effect: [ate-q], [train: 0.06511], [test: 0.06743]\n",
      "********************************************************************************\n",
      "epoch: 654 / 800, time cost: 46.86 sec, \n",
      "          Loss: [Train: 2.11184], [Test: 2.47761],\n",
      "          Accuracy: [prop score:  0.95630], [q1: 0.61087], [q0: 0.68402],\n",
      "          Effect: [ate-q], [train: 0.05821], [test: 0.06049]\n",
      "********************************************************************************\n",
      "epoch: 655 / 800, time cost: 44.86 sec, \n",
      "          Loss: [Train: 2.11107], [Test: 2.48018],\n",
      "          Accuracy: [prop score:  0.95623], [q1: 0.53293], [q0: 0.70126],\n",
      "          Effect: [ate-q], [train: 0.03680], [test: 0.03904]\n",
      "********************************************************************************\n",
      "epoch: 656 / 800, time cost: 47.71 sec, \n",
      "          Loss: [Train: 2.11244], [Test: 2.47936],\n",
      "          Accuracy: [prop score:  0.95635], [q1: 0.56993], [q0: 0.72300],\n",
      "          Effect: [ate-q], [train: 0.05216], [test: 0.05441]\n",
      "********************************************************************************\n",
      "epoch: 657 / 800, time cost: 45.18 sec, \n",
      "          Loss: [Train: 2.11085], [Test: 2.47803],\n",
      "          Accuracy: [prop score:  0.95629], [q1: 0.59291], [q0: 0.70059],\n",
      "          Effect: [ate-q], [train: 0.05767], [test: 0.06000]\n",
      "********************************************************************************\n",
      "epoch: 658 / 800, time cost: 45.17 sec, \n",
      "          Loss: [Train: 2.11046], [Test: 2.48093],\n",
      "          Accuracy: [prop score:  0.95636], [q1: 0.53456], [q0: 0.71773],\n",
      "          Effect: [ate-q], [train: 0.04067], [test: 0.04285]\n",
      "********************************************************************************\n",
      "epoch: 659 / 800, time cost: 47.32 sec, \n",
      "          Loss: [Train: 2.11048], [Test: 2.47885],\n",
      "          Accuracy: [prop score:  0.95631], [q1: 0.62082], [q0: 0.71770],\n",
      "          Effect: [ate-q], [train: 0.07022], [test: 0.07259]\n",
      "********************************************************************************\n",
      "epoch: 660 / 800, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.10969], [Test: 2.48099],\n",
      "          Accuracy: [prop score:  0.95629], [q1: 0.57352], [q0: 0.69077],\n",
      "          Effect: [ate-q], [train: 0.04761], [test: 0.04993]\n",
      "********************************************************************************\n",
      "epoch: 661 / 800, time cost: 47.47 sec, \n",
      "          Loss: [Train: 2.10938], [Test: 2.48036],\n",
      "          Accuracy: [prop score:  0.95633], [q1: 0.61319], [q0: 0.70385],\n",
      "          Effect: [ate-q], [train: 0.06388], [test: 0.06619]\n",
      "********************************************************************************\n",
      "epoch: 662 / 800, time cost: 45.38 sec, \n",
      "          Loss: [Train: 2.11005], [Test: 2.48194],\n",
      "          Accuracy: [prop score:  0.95629], [q1: 0.55699], [q0: 0.72142],\n",
      "          Effect: [ate-q], [train: 0.04858], [test: 0.05090]\n",
      "********************************************************************************\n",
      "epoch: 663 / 800, time cost: 45.35 sec, \n",
      "          Loss: [Train: 2.10913], [Test: 2.48275],\n",
      "          Accuracy: [prop score:  0.95634], [q1: 0.58390], [q0: 0.73139],\n",
      "          Effect: [ate-q], [train: 0.06160], [test: 0.06389]\n",
      "********************************************************************************\n",
      "epoch: 664 / 800, time cost: 47.78 sec, \n",
      "          Loss: [Train: 2.10813], [Test: 2.48155],\n",
      "          Accuracy: [prop score:  0.95631], [q1: 0.65776], [q0: 0.72566],\n",
      "          Effect: [ate-q], [train: 0.08732], [test: 0.08978]\n",
      "********************************************************************************\n",
      "epoch: 665 / 800, time cost: 45.65 sec, \n",
      "          Loss: [Train: 2.10828], [Test: 2.48427],\n",
      "          Accuracy: [prop score:  0.95636], [q1: 0.55158], [q0: 0.72360],\n",
      "          Effect: [ate-q], [train: 0.04734], [test: 0.04955]\n",
      "********************************************************************************\n",
      "epoch: 666 / 800, time cost: 47.76 sec, \n",
      "          Loss: [Train: 2.10858], [Test: 2.48371],\n",
      "          Accuracy: [prop score:  0.95630], [q1: 0.57534], [q0: 0.73315],\n",
      "          Effect: [ate-q], [train: 0.05813], [test: 0.06040]\n",
      "********************************************************************************\n",
      "epoch: 667 / 800, time cost: 45.61 sec, \n",
      "          Loss: [Train: 2.10778], [Test: 2.48442],\n",
      "          Accuracy: [prop score:  0.95626], [q1: 0.55272], [q0: 0.72354],\n",
      "          Effect: [ate-q], [train: 0.05045], [test: 0.05270]\n",
      "********************************************************************************\n",
      "epoch: 668 / 800, time cost: 45.70 sec, \n",
      "          Loss: [Train: 2.10757], [Test: 2.48452],\n",
      "          Accuracy: [prop score:  0.95636], [q1: 0.56961], [q0: 0.73706],\n",
      "          Effect: [ate-q], [train: 0.05790], [test: 0.06029]\n",
      "********************************************************************************\n",
      "epoch: 669 / 800, time cost: 47.85 sec, \n",
      "          Loss: [Train: 2.10681], [Test: 2.48526],\n",
      "          Accuracy: [prop score:  0.95634], [q1: 0.57395], [q0: 0.71610],\n",
      "          Effect: [ate-q], [train: 0.05384], [test: 0.05612]\n",
      "********************************************************************************\n",
      "epoch: 670 / 800, time cost: 45.93 sec, \n",
      "          Loss: [Train: 2.10635], [Test: 2.48462],\n",
      "          Accuracy: [prop score:  0.95638], [q1: 0.61939], [q0: 0.67716],\n",
      "          Effect: [ate-q], [train: 0.06328], [test: 0.06567]\n",
      "********************************************************************************\n",
      "epoch: 671 / 800, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.10643], [Test: 2.48558],\n",
      "          Accuracy: [prop score:  0.95634], [q1: 0.59820], [q0: 0.68453],\n",
      "          Effect: [ate-q], [train: 0.05820], [test: 0.06052]\n",
      "********************************************************************************\n",
      "epoch: 672 / 800, time cost: 45.92 sec, \n",
      "          Loss: [Train: 2.10578], [Test: 2.48756],\n",
      "          Accuracy: [prop score:  0.95640], [q1: 0.56265], [q0: 0.68246],\n",
      "          Effect: [ate-q], [train: 0.04255], [test: 0.04483]\n",
      "********************************************************************************\n",
      "epoch: 673 / 800, time cost: 45.95 sec, \n",
      "          Loss: [Train: 2.10491], [Test: 2.48639],\n",
      "          Accuracy: [prop score:  0.95635], [q1: 0.57775], [q0: 0.71907],\n",
      "          Effect: [ate-q], [train: 0.05863], [test: 0.06102]\n",
      "********************************************************************************\n",
      "epoch: 674 / 800, time cost: 47.75 sec, \n",
      "          Loss: [Train: 2.10533], [Test: 2.48728],\n",
      "          Accuracy: [prop score:  0.95639], [q1: 0.57148], [q0: 0.71878],\n",
      "          Effect: [ate-q], [train: 0.05374], [test: 0.05596]\n",
      "********************************************************************************\n",
      "epoch: 675 / 800, time cost: 45.70 sec, \n",
      "          Loss: [Train: 2.10490], [Test: 2.48944],\n",
      "          Accuracy: [prop score:  0.95639], [q1: 0.56506], [q0: 0.76609],\n",
      "          Effect: [ate-q], [train: 0.06382], [test: 0.06613]\n",
      "********************************************************************************\n",
      "epoch: 676 / 800, time cost: 47.99 sec, \n",
      "          Loss: [Train: 2.10377], [Test: 2.48741],\n",
      "          Accuracy: [prop score:  0.95643], [q1: 0.59089], [q0: 0.67882],\n",
      "          Effect: [ate-q], [train: 0.05231], [test: 0.05458]\n",
      "********************************************************************************\n",
      "epoch: 677 / 800, time cost: 45.71 sec, \n",
      "          Loss: [Train: 2.10387], [Test: 2.48886],\n",
      "          Accuracy: [prop score:  0.95642], [q1: 0.59192], [q0: 0.69108],\n",
      "          Effect: [ate-q], [train: 0.05419], [test: 0.05645]\n",
      "********************************************************************************\n",
      "epoch: 678 / 800, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.10295], [Test: 2.48936],\n",
      "          Accuracy: [prop score:  0.95646], [q1: 0.58791], [q0: 0.69769],\n",
      "          Effect: [ate-q], [train: 0.05374], [test: 0.05606]\n",
      "********************************************************************************\n",
      "epoch: 679 / 800, time cost: 46.19 sec, \n",
      "          Loss: [Train: 2.10308], [Test: 2.48894],\n",
      "          Accuracy: [prop score:  0.95641], [q1: 0.57769], [q0: 0.72801],\n",
      "          Effect: [ate-q], [train: 0.06061], [test: 0.06283]\n",
      "********************************************************************************\n",
      "epoch: 680 / 800, time cost: 44.29 sec, \n",
      "          Loss: [Train: 2.10274], [Test: 2.49299],\n",
      "          Accuracy: [prop score:  0.95646], [q1: 0.52444], [q0: 0.69570],\n",
      "          Effect: [ate-q], [train: 0.03566], [test: 0.03779]\n",
      "********************************************************************************\n",
      "epoch: 681 / 800, time cost: 46.24 sec, \n",
      "          Loss: [Train: 2.10275], [Test: 2.49088],\n",
      "          Accuracy: [prop score:  0.95639], [q1: 0.60293], [q0: 0.68809],\n",
      "          Effect: [ate-q], [train: 0.05893], [test: 0.06119]\n",
      "********************************************************************************\n",
      "epoch: 682 / 800, time cost: 46.17 sec, \n",
      "          Loss: [Train: 2.10218], [Test: 2.49144],\n",
      "          Accuracy: [prop score:  0.95641], [q1: 0.57566], [q0: 0.69607],\n",
      "          Effect: [ate-q], [train: 0.05091], [test: 0.05316]\n",
      "********************************************************************************\n",
      "epoch: 683 / 800, time cost: 46.09 sec, \n",
      "          Loss: [Train: 2.10153], [Test: 2.49182],\n",
      "          Accuracy: [prop score:  0.95642], [q1: 0.57749], [q0: 0.68962],\n",
      "          Effect: [ate-q], [train: 0.05121], [test: 0.05353]\n",
      "********************************************************************************\n",
      "epoch: 684 / 800, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.10118], [Test: 2.49213],\n",
      "          Accuracy: [prop score:  0.95643], [q1: 0.59345], [q0: 0.72237],\n",
      "          Effect: [ate-q], [train: 0.06305], [test: 0.06538]\n",
      "********************************************************************************\n",
      "epoch: 685 / 800, time cost: 46.05 sec, \n",
      "          Loss: [Train: 2.10122], [Test: 2.49310],\n",
      "          Accuracy: [prop score:  0.95649], [q1: 0.57532], [q0: 0.70522],\n",
      "          Effect: [ate-q], [train: 0.05302], [test: 0.05525]\n",
      "********************************************************************************\n",
      "epoch: 686 / 800, time cost: 48.19 sec, \n",
      "          Loss: [Train: 2.09985], [Test: 2.49351],\n",
      "          Accuracy: [prop score:  0.95644], [q1: 0.58810], [q0: 0.65708],\n",
      "          Effect: [ate-q], [train: 0.04809], [test: 0.05034]\n",
      "********************************************************************************\n",
      "epoch: 687 / 800, time cost: 45.81 sec, \n",
      "          Loss: [Train: 2.10035], [Test: 2.49380],\n",
      "          Accuracy: [prop score:  0.95636], [q1: 0.56218], [q0: 0.74575],\n",
      "          Effect: [ate-q], [train: 0.05955], [test: 0.06191]\n",
      "********************************************************************************\n",
      "epoch: 688 / 800, time cost: 45.42 sec, \n",
      "          Loss: [Train: 2.10000], [Test: 2.49723],\n",
      "          Accuracy: [prop score:  0.95654], [q1: 0.51429], [q0: 0.71115],\n",
      "          Effect: [ate-q], [train: 0.03464], [test: 0.03682]\n",
      "********************************************************************************\n",
      "epoch: 689 / 800, time cost: 46.29 sec, \n",
      "          Loss: [Train: 2.09971], [Test: 2.49402],\n",
      "          Accuracy: [prop score:  0.95634], [q1: 0.65354], [q0: 0.71526],\n",
      "          Effect: [ate-q], [train: 0.08434], [test: 0.08676]\n",
      "********************************************************************************\n",
      "epoch: 690 / 800, time cost: 44.16 sec, \n",
      "          Loss: [Train: 2.09915], [Test: 2.49501],\n",
      "          Accuracy: [prop score:  0.95650], [q1: 0.61178], [q0: 0.70413],\n",
      "          Effect: [ate-q], [train: 0.06422], [test: 0.06657]\n",
      "********************************************************************************\n",
      "epoch: 691 / 800, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.09905], [Test: 2.49563],\n",
      "          Accuracy: [prop score:  0.95647], [q1: 0.59135], [q0: 0.71037],\n",
      "          Effect: [ate-q], [train: 0.06041], [test: 0.06277]\n",
      "********************************************************************************\n",
      "epoch: 692 / 800, time cost: 44.12 sec, \n",
      "          Loss: [Train: 2.09849], [Test: 2.49517],\n",
      "          Accuracy: [prop score:  0.95662], [q1: 0.59355], [q0: 0.68376],\n",
      "          Effect: [ate-q], [train: 0.05428], [test: 0.05658]\n",
      "********************************************************************************\n",
      "epoch: 693 / 800, time cost: 44.00 sec, \n",
      "          Loss: [Train: 2.09776], [Test: 2.49541],\n",
      "          Accuracy: [prop score:  0.95660], [q1: 0.62754], [q0: 0.69956],\n",
      "          Effect: [ate-q], [train: 0.07103], [test: 0.07344]\n",
      "********************************************************************************\n",
      "epoch: 694 / 800, time cost: 46.32 sec, \n",
      "          Loss: [Train: 2.09732], [Test: 2.49786],\n",
      "          Accuracy: [prop score:  0.95653], [q1: 0.55298], [q0: 0.70700],\n",
      "          Effect: [ate-q], [train: 0.04948], [test: 0.05181]\n",
      "********************************************************************************\n",
      "epoch: 695 / 800, time cost: 43.88 sec, \n",
      "          Loss: [Train: 2.09793], [Test: 2.49762],\n",
      "          Accuracy: [prop score:  0.95645], [q1: 0.55239], [q0: 0.69435],\n",
      "          Effect: [ate-q], [train: 0.04658], [test: 0.04879]\n",
      "********************************************************************************\n",
      "epoch: 696 / 800, time cost: 45.99 sec, \n",
      "          Loss: [Train: 2.09766], [Test: 2.49737],\n",
      "          Accuracy: [prop score:  0.95665], [q1: 0.62476], [q0: 0.69024],\n",
      "          Effect: [ate-q], [train: 0.06741], [test: 0.06973]\n",
      "********************************************************************************\n",
      "epoch: 697 / 800, time cost: 44.60 sec, \n",
      "          Loss: [Train: 2.09688], [Test: 2.49884],\n",
      "          Accuracy: [prop score:  0.95638], [q1: 0.59998], [q0: 0.74565],\n",
      "          Effect: [ate-q], [train: 0.07113], [test: 0.07344]\n",
      "********************************************************************************\n",
      "epoch: 698 / 800, time cost: 47.77 sec, \n",
      "          Loss: [Train: 2.09617], [Test: 2.49916],\n",
      "          Accuracy: [prop score:  0.95635], [q1: 0.58488], [q0: 0.68912],\n",
      "          Effect: [ate-q], [train: 0.05154], [test: 0.05382]\n",
      "********************************************************************************\n",
      "epoch: 699 / 800, time cost: 55.18 sec, \n",
      "          Loss: [Train: 2.09643], [Test: 2.50025],\n",
      "          Accuracy: [prop score:  0.95645], [q1: 0.59715], [q0: 0.65438],\n",
      "          Effect: [ate-q], [train: 0.05012], [test: 0.05234]\n",
      "********************************************************************************\n",
      "epoch: 700 / 800, time cost: 52.68 sec, \n",
      "          Loss: [Train: 2.09565], [Test: 2.50273],\n",
      "          Accuracy: [prop score:  0.95658], [q1: 0.54489], [q0: 0.72675],\n",
      "          Effect: [ate-q], [train: 0.04713], [test: 0.04931]\n",
      "********************************************************************************\n",
      "epoch: 701 / 800, time cost: 55.22 sec, \n",
      "          Loss: [Train: 2.09558], [Test: 2.50008],\n",
      "          Accuracy: [prop score:  0.95660], [q1: 0.59250], [q0: 0.72284],\n",
      "          Effect: [ate-q], [train: 0.06471], [test: 0.06715]\n",
      "********************************************************************************\n",
      "epoch: 702 / 800, time cost: 50.33 sec, \n",
      "          Loss: [Train: 2.09554], [Test: 2.50069],\n",
      "          Accuracy: [prop score:  0.95655], [q1: 0.63592], [q0: 0.76172],\n",
      "          Effect: [ate-q], [train: 0.09239], [test: 0.09477]\n",
      "********************************************************************************\n",
      "epoch: 703 / 800, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.09519], [Test: 2.50288],\n",
      "          Accuracy: [prop score:  0.95654], [q1: 0.56033], [q0: 0.71832],\n",
      "          Effect: [ate-q], [train: 0.05090], [test: 0.05317]\n",
      "********************************************************************************\n",
      "epoch: 704 / 800, time cost: 47.93 sec, \n",
      "          Loss: [Train: 2.09464], [Test: 2.50269],\n",
      "          Accuracy: [prop score:  0.95649], [q1: 0.61660], [q0: 0.63820],\n",
      "          Effect: [ate-q], [train: 0.05490], [test: 0.05720]\n",
      "********************************************************************************\n",
      "epoch: 705 / 800, time cost: 45.61 sec, \n",
      "          Loss: [Train: 2.09413], [Test: 2.50285],\n",
      "          Accuracy: [prop score:  0.95650], [q1: 0.55476], [q0: 0.70555],\n",
      "          Effect: [ate-q], [train: 0.05070], [test: 0.05299]\n",
      "********************************************************************************\n",
      "epoch: 706 / 800, time cost: 47.66 sec, \n",
      "          Loss: [Train: 2.09381], [Test: 2.50259],\n",
      "          Accuracy: [prop score:  0.95655], [q1: 0.62862], [q0: 0.74005],\n",
      "          Effect: [ate-q], [train: 0.08275], [test: 0.08519]\n",
      "********************************************************************************\n",
      "epoch: 707 / 800, time cost: 46.01 sec, \n",
      "          Loss: [Train: 2.09292], [Test: 2.50608],\n",
      "          Accuracy: [prop score:  0.95662], [q1: 0.51796], [q0: 0.70254],\n",
      "          Effect: [ate-q], [train: 0.03699], [test: 0.03917]\n",
      "********************************************************************************\n",
      "epoch: 708 / 800, time cost: 46.03 sec, \n",
      "          Loss: [Train: 2.09267], [Test: 2.50357],\n",
      "          Accuracy: [prop score:  0.95650], [q1: 0.58658], [q0: 0.70874],\n",
      "          Effect: [ate-q], [train: 0.05982], [test: 0.06215]\n",
      "********************************************************************************\n",
      "epoch: 709 / 800, time cost: 48.03 sec, \n",
      "          Loss: [Train: 2.09271], [Test: 2.50697],\n",
      "          Accuracy: [prop score:  0.95656], [q1: 0.53498], [q0: 0.67279],\n",
      "          Effect: [ate-q], [train: 0.03456], [test: 0.03674]\n",
      "********************************************************************************\n",
      "epoch: 710 / 800, time cost: 45.88 sec, \n",
      "          Loss: [Train: 2.09263], [Test: 2.50475],\n",
      "          Accuracy: [prop score:  0.95651], [q1: 0.62068], [q0: 0.72695],\n",
      "          Effect: [ate-q], [train: 0.07749], [test: 0.07983]\n",
      "********************************************************************************\n",
      "epoch: 711 / 800, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.09211], [Test: 2.50566],\n",
      "          Accuracy: [prop score:  0.95650], [q1: 0.60416], [q0: 0.71684],\n",
      "          Effect: [ate-q], [train: 0.06771], [test: 0.07003]\n",
      "********************************************************************************\n",
      "epoch: 712 / 800, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.09192], [Test: 2.50582],\n",
      "          Accuracy: [prop score:  0.95657], [q1: 0.61648], [q0: 0.67485],\n",
      "          Effect: [ate-q], [train: 0.06242], [test: 0.06478]\n",
      "********************************************************************************\n",
      "epoch: 713 / 800, time cost: 45.77 sec, \n",
      "          Loss: [Train: 2.09111], [Test: 2.50687],\n",
      "          Accuracy: [prop score:  0.95654], [q1: 0.57807], [q0: 0.69205],\n",
      "          Effect: [ate-q], [train: 0.05142], [test: 0.05371]\n",
      "********************************************************************************\n",
      "epoch: 714 / 800, time cost: 47.82 sec, \n",
      "          Loss: [Train: 2.09117], [Test: 2.50790],\n",
      "          Accuracy: [prop score:  0.95665], [q1: 0.57617], [q0: 0.72550],\n",
      "          Effect: [ate-q], [train: 0.05994], [test: 0.06219]\n",
      "********************************************************************************\n",
      "epoch: 715 / 800, time cost: 45.74 sec, \n",
      "          Loss: [Train: 2.09070], [Test: 2.51000],\n",
      "          Accuracy: [prop score:  0.95666], [q1: 0.52294], [q0: 0.69790],\n",
      "          Effect: [ate-q], [train: 0.03536], [test: 0.03755]\n",
      "********************************************************************************\n",
      "epoch: 716 / 800, time cost: 47.56 sec, \n",
      "          Loss: [Train: 2.09005], [Test: 2.50755],\n",
      "          Accuracy: [prop score:  0.95667], [q1: 0.59589], [q0: 0.69022],\n",
      "          Effect: [ate-q], [train: 0.05716], [test: 0.05944]\n",
      "********************************************************************************\n",
      "epoch: 717 / 800, time cost: 45.69 sec, \n",
      "          Loss: [Train: 2.09032], [Test: 2.50783],\n",
      "          Accuracy: [prop score:  0.95662], [q1: 0.61406], [q0: 0.67264],\n",
      "          Effect: [ate-q], [train: 0.06104], [test: 0.06339]\n",
      "********************************************************************************\n",
      "epoch: 718 / 800, time cost: 45.64 sec, \n",
      "          Loss: [Train: 2.08998], [Test: 2.51023],\n",
      "          Accuracy: [prop score:  0.95656], [q1: 0.56384], [q0: 0.69522],\n",
      "          Effect: [ate-q], [train: 0.04863], [test: 0.05093]\n",
      "********************************************************************************\n",
      "epoch: 719 / 800, time cost: 47.55 sec, \n",
      "          Loss: [Train: 2.08893], [Test: 2.51037],\n",
      "          Accuracy: [prop score:  0.95666], [q1: 0.56108], [q0: 0.65314],\n",
      "          Effect: [ate-q], [train: 0.04221], [test: 0.04448]\n",
      "********************************************************************************\n",
      "epoch: 720 / 800, time cost: 45.55 sec, \n",
      "          Loss: [Train: 2.08846], [Test: 2.51134],\n",
      "          Accuracy: [prop score:  0.95661], [q1: 0.56676], [q0: 0.64361],\n",
      "          Effect: [ate-q], [train: 0.03904], [test: 0.04125]\n",
      "********************************************************************************\n",
      "epoch: 721 / 800, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.08915], [Test: 2.50995],\n",
      "          Accuracy: [prop score:  0.95659], [q1: 0.61857], [q0: 0.68472],\n",
      "          Effect: [ate-q], [train: 0.06557], [test: 0.06794]\n",
      "********************************************************************************\n",
      "epoch: 722 / 800, time cost: 45.40 sec, \n",
      "          Loss: [Train: 2.08865], [Test: 2.51165],\n",
      "          Accuracy: [prop score:  0.95667], [q1: 0.60203], [q0: 0.72189],\n",
      "          Effect: [ate-q], [train: 0.06590], [test: 0.06819]\n",
      "********************************************************************************\n",
      "epoch: 723 / 800, time cost: 45.41 sec, \n",
      "          Loss: [Train: 2.08773], [Test: 2.51143],\n",
      "          Accuracy: [prop score:  0.95668], [q1: 0.62497], [q0: 0.66955],\n",
      "          Effect: [ate-q], [train: 0.06376], [test: 0.06621]\n",
      "********************************************************************************\n",
      "epoch: 724 / 800, time cost: 47.11 sec, \n",
      "          Loss: [Train: 2.08751], [Test: 2.51410],\n",
      "          Accuracy: [prop score:  0.95654], [q1: 0.54904], [q0: 0.69612],\n",
      "          Effect: [ate-q], [train: 0.04369], [test: 0.04601]\n",
      "********************************************************************************\n",
      "epoch: 725 / 800, time cost: 44.25 sec, \n",
      "          Loss: [Train: 2.08743], [Test: 2.51240],\n",
      "          Accuracy: [prop score:  0.95660], [q1: 0.59383], [q0: 0.62964],\n",
      "          Effect: [ate-q], [train: 0.04605], [test: 0.04831]\n",
      "********************************************************************************\n",
      "epoch: 726 / 800, time cost: 47.80 sec, \n",
      "          Loss: [Train: 2.08667], [Test: 2.51289],\n",
      "          Accuracy: [prop score:  0.95660], [q1: 0.61728], [q0: 0.68151],\n",
      "          Effect: [ate-q], [train: 0.06523], [test: 0.06761]\n",
      "********************************************************************************\n",
      "epoch: 727 / 800, time cost: 44.41 sec, \n",
      "          Loss: [Train: 2.08638], [Test: 2.51478],\n",
      "          Accuracy: [prop score:  0.95666], [q1: 0.56970], [q0: 0.68845],\n",
      "          Effect: [ate-q], [train: 0.04865], [test: 0.05098]\n",
      "********************************************************************************\n",
      "epoch: 728 / 800, time cost: 44.22 sec, \n",
      "          Loss: [Train: 2.08679], [Test: 2.51412],\n",
      "          Accuracy: [prop score:  0.95671], [q1: 0.60813], [q0: 0.74789],\n",
      "          Effect: [ate-q], [train: 0.07745], [test: 0.07984]\n",
      "********************************************************************************\n",
      "epoch: 729 / 800, time cost: 46.40 sec, \n",
      "          Loss: [Train: 2.08550], [Test: 2.51418],\n",
      "          Accuracy: [prop score:  0.95663], [q1: 0.59761], [q0: 0.72720],\n",
      "          Effect: [ate-q], [train: 0.06871], [test: 0.07110]\n",
      "********************************************************************************\n",
      "epoch: 730 / 800, time cost: 45.13 sec, \n",
      "          Loss: [Train: 2.08574], [Test: 2.51621],\n",
      "          Accuracy: [prop score:  0.95665], [q1: 0.55731], [q0: 0.70809],\n",
      "          Effect: [ate-q], [train: 0.05024], [test: 0.05259]\n",
      "********************************************************************************\n",
      "epoch: 731 / 800, time cost: 47.57 sec, \n",
      "          Loss: [Train: 2.08524], [Test: 2.51700],\n",
      "          Accuracy: [prop score:  0.95672], [q1: 0.55414], [q0: 0.67234],\n",
      "          Effect: [ate-q], [train: 0.04175], [test: 0.04387]\n",
      "********************************************************************************\n",
      "epoch: 732 / 800, time cost: 45.39 sec, \n",
      "          Loss: [Train: 2.08488], [Test: 2.51723],\n",
      "          Accuracy: [prop score:  0.95665], [q1: 0.58433], [q0: 0.69112],\n",
      "          Effect: [ate-q], [train: 0.05256], [test: 0.05487]\n",
      "********************************************************************************\n",
      "epoch: 733 / 800, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.08443], [Test: 2.51547],\n",
      "          Accuracy: [prop score:  0.95667], [q1: 0.61237], [q0: 0.69448],\n",
      "          Effect: [ate-q], [train: 0.06699], [test: 0.06929]\n",
      "********************************************************************************\n",
      "epoch: 734 / 800, time cost: 47.31 sec, \n",
      "          Loss: [Train: 2.08437], [Test: 2.51893],\n",
      "          Accuracy: [prop score:  0.95670], [q1: 0.56811], [q0: 0.67467],\n",
      "          Effect: [ate-q], [train: 0.04557], [test: 0.04777]\n",
      "********************************************************************************\n",
      "epoch: 735 / 800, time cost: 45.30 sec, \n",
      "          Loss: [Train: 2.08347], [Test: 2.51822],\n",
      "          Accuracy: [prop score:  0.95668], [q1: 0.58907], [q0: 0.68527],\n",
      "          Effect: [ate-q], [train: 0.05794], [test: 0.06026]\n",
      "********************************************************************************\n",
      "epoch: 736 / 800, time cost: 47.20 sec, \n",
      "          Loss: [Train: 2.08287], [Test: 2.51983],\n",
      "          Accuracy: [prop score:  0.95662], [q1: 0.57447], [q0: 0.71764],\n",
      "          Effect: [ate-q], [train: 0.05877], [test: 0.06110]\n",
      "********************************************************************************\n",
      "epoch: 737 / 800, time cost: 45.29 sec, \n",
      "          Loss: [Train: 2.08254], [Test: 2.52093],\n",
      "          Accuracy: [prop score:  0.95674], [q1: 0.55607], [q0: 0.72973],\n",
      "          Effect: [ate-q], [train: 0.05390], [test: 0.05616]\n",
      "********************************************************************************\n",
      "epoch: 738 / 800, time cost: 45.13 sec, \n",
      "          Loss: [Train: 2.08297], [Test: 2.52190],\n",
      "          Accuracy: [prop score:  0.95668], [q1: 0.51735], [q0: 0.67700],\n",
      "          Effect: [ate-q], [train: 0.03159], [test: 0.03382]\n",
      "********************************************************************************\n",
      "epoch: 739 / 800, time cost: 47.32 sec, \n",
      "          Loss: [Train: 2.08268], [Test: 2.51938],\n",
      "          Accuracy: [prop score:  0.95667], [q1: 0.61516], [q0: 0.66022],\n",
      "          Effect: [ate-q], [train: 0.06011], [test: 0.06245]\n",
      "********************************************************************************\n",
      "epoch: 740 / 800, time cost: 45.15 sec, \n",
      "          Loss: [Train: 2.08213], [Test: 2.52099],\n",
      "          Accuracy: [prop score:  0.95678], [q1: 0.61608], [q0: 0.67734],\n",
      "          Effect: [ate-q], [train: 0.06409], [test: 0.06630]\n",
      "********************************************************************************\n",
      "epoch: 741 / 800, time cost: 47.21 sec, \n",
      "          Loss: [Train: 2.08239], [Test: 2.52084],\n",
      "          Accuracy: [prop score:  0.95664], [q1: 0.60039], [q0: 0.69309],\n",
      "          Effect: [ate-q], [train: 0.06256], [test: 0.06492]\n",
      "********************************************************************************\n",
      "epoch: 742 / 800, time cost: 45.42 sec, \n",
      "          Loss: [Train: 2.08210], [Test: 2.52350],\n",
      "          Accuracy: [prop score:  0.95677], [q1: 0.56702], [q0: 0.66324],\n",
      "          Effect: [ate-q], [train: 0.04373], [test: 0.04599]\n",
      "********************************************************************************\n",
      "epoch: 743 / 800, time cost: 45.13 sec, \n",
      "          Loss: [Train: 2.08093], [Test: 2.52235],\n",
      "          Accuracy: [prop score:  0.95668], [q1: 0.59579], [q0: 0.67751],\n",
      "          Effect: [ate-q], [train: 0.05589], [test: 0.05826]\n",
      "********************************************************************************\n",
      "epoch: 744 / 800, time cost: 47.10 sec, \n",
      "          Loss: [Train: 2.08140], [Test: 2.52265],\n",
      "          Accuracy: [prop score:  0.95667], [q1: 0.59677], [q0: 0.66458],\n",
      "          Effect: [ate-q], [train: 0.05579], [test: 0.05799]\n",
      "********************************************************************************\n",
      "epoch: 745 / 800, time cost: 45.33 sec, \n",
      "          Loss: [Train: 2.08016], [Test: 2.52413],\n",
      "          Accuracy: [prop score:  0.95657], [q1: 0.57409], [q0: 0.70654],\n",
      "          Effect: [ate-q], [train: 0.05516], [test: 0.05749]\n",
      "********************************************************************************\n",
      "epoch: 746 / 800, time cost: 47.13 sec, \n",
      "          Loss: [Train: 2.07981], [Test: 2.52676],\n",
      "          Accuracy: [prop score:  0.95670], [q1: 0.55038], [q0: 0.73234],\n",
      "          Effect: [ate-q], [train: 0.05194], [test: 0.05423]\n",
      "********************************************************************************\n",
      "epoch: 747 / 800, time cost: 45.09 sec, \n",
      "          Loss: [Train: 2.07993], [Test: 2.52700],\n",
      "          Accuracy: [prop score:  0.95664], [q1: 0.53903], [q0: 0.68426],\n",
      "          Effect: [ate-q], [train: 0.03967], [test: 0.04203]\n",
      "********************************************************************************\n",
      "epoch: 748 / 800, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.07979], [Test: 2.52781],\n",
      "          Accuracy: [prop score:  0.95677], [q1: 0.57399], [q0: 0.58144],\n",
      "          Effect: [ate-q], [train: 0.02933], [test: 0.03158]\n",
      "********************************************************************************\n",
      "epoch: 749 / 800, time cost: 48.29 sec, \n",
      "          Loss: [Train: 2.07938], [Test: 2.52802],\n",
      "          Accuracy: [prop score:  0.95662], [q1: 0.52747], [q0: 0.70289],\n",
      "          Effect: [ate-q], [train: 0.04314], [test: 0.04535]\n",
      "********************************************************************************\n",
      "epoch: 750 / 800, time cost: 46.26 sec, \n",
      "          Loss: [Train: 2.07861], [Test: 2.52638],\n",
      "          Accuracy: [prop score:  0.95671], [q1: 0.57788], [q0: 0.72094],\n",
      "          Effect: [ate-q], [train: 0.06042], [test: 0.06269]\n",
      "********************************************************************************\n",
      "epoch: 751 / 800, time cost: 48.62 sec, \n",
      "          Loss: [Train: 2.07825], [Test: 2.52872],\n",
      "          Accuracy: [prop score:  0.95674], [q1: 0.55665], [q0: 0.66279],\n",
      "          Effect: [ate-q], [train: 0.03705], [test: 0.03933]\n",
      "********************************************************************************\n",
      "epoch: 752 / 800, time cost: 45.98 sec, \n",
      "          Loss: [Train: 2.07776], [Test: 2.52901],\n",
      "          Accuracy: [prop score:  0.95684], [q1: 0.56806], [q0: 0.63061],\n",
      "          Effect: [ate-q], [train: 0.03814], [test: 0.04039]\n",
      "********************************************************************************\n",
      "epoch: 753 / 800, time cost: 45.51 sec, \n",
      "          Loss: [Train: 2.07801], [Test: 2.52767],\n",
      "          Accuracy: [prop score:  0.95679], [q1: 0.61028], [q0: 0.72587],\n",
      "          Effect: [ate-q], [train: 0.07308], [test: 0.07543]\n",
      "********************************************************************************\n",
      "epoch: 754 / 800, time cost: 48.24 sec, \n",
      "          Loss: [Train: 2.07697], [Test: 2.52922],\n",
      "          Accuracy: [prop score:  0.95683], [q1: 0.59192], [q0: 0.70911],\n",
      "          Effect: [ate-q], [train: 0.06245], [test: 0.06472]\n",
      "********************************************************************************\n",
      "epoch: 755 / 800, time cost: 46.16 sec, \n",
      "          Loss: [Train: 2.07731], [Test: 2.52958],\n",
      "          Accuracy: [prop score:  0.95677], [q1: 0.58482], [q0: 0.63753],\n",
      "          Effect: [ate-q], [train: 0.04567], [test: 0.04787]\n",
      "********************************************************************************\n",
      "epoch: 756 / 800, time cost: 51.39 sec, \n",
      "          Loss: [Train: 2.07745], [Test: 2.52896],\n",
      "          Accuracy: [prop score:  0.95682], [q1: 0.64229], [q0: 0.68779],\n",
      "          Effect: [ate-q], [train: 0.07831], [test: 0.08074]\n",
      "********************************************************************************\n",
      "epoch: 757 / 800, time cost: 46.10 sec, \n",
      "          Loss: [Train: 2.07691], [Test: 2.53058],\n",
      "          Accuracy: [prop score:  0.95686], [q1: 0.57059], [q0: 0.68528],\n",
      "          Effect: [ate-q], [train: 0.05029], [test: 0.05251]\n",
      "********************************************************************************\n",
      "epoch: 758 / 800, time cost: 46.06 sec, \n",
      "          Loss: [Train: 2.07611], [Test: 2.53162],\n",
      "          Accuracy: [prop score:  0.95684], [q1: 0.56106], [q0: 0.65715],\n",
      "          Effect: [ate-q], [train: 0.04069], [test: 0.04297]\n",
      "********************************************************************************\n",
      "epoch: 759 / 800, time cost: 47.16 sec, \n",
      "          Loss: [Train: 2.07580], [Test: 2.53149],\n",
      "          Accuracy: [prop score:  0.95678], [q1: 0.57171], [q0: 0.73213],\n",
      "          Effect: [ate-q], [train: 0.06214], [test: 0.06454]\n",
      "********************************************************************************\n",
      "epoch: 760 / 800, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.07537], [Test: 2.53095],\n",
      "          Accuracy: [prop score:  0.95672], [q1: 0.61213], [q0: 0.71197],\n",
      "          Effect: [ate-q], [train: 0.07321], [test: 0.07562]\n",
      "********************************************************************************\n",
      "epoch: 761 / 800, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.07570], [Test: 2.53266],\n",
      "          Accuracy: [prop score:  0.95679], [q1: 0.59007], [q0: 0.73382],\n",
      "          Effect: [ate-q], [train: 0.06946], [test: 0.07181]\n",
      "********************************************************************************\n",
      "epoch: 762 / 800, time cost: 45.85 sec, \n",
      "          Loss: [Train: 2.07482], [Test: 2.53331],\n",
      "          Accuracy: [prop score:  0.95683], [q1: 0.56403], [q0: 0.65670],\n",
      "          Effect: [ate-q], [train: 0.04248], [test: 0.04469]\n",
      "********************************************************************************\n",
      "epoch: 763 / 800, time cost: 45.37 sec, \n",
      "          Loss: [Train: 2.07470], [Test: 2.53195],\n",
      "          Accuracy: [prop score:  0.95674], [q1: 0.60964], [q0: 0.65614],\n",
      "          Effect: [ate-q], [train: 0.05909], [test: 0.06149]\n",
      "********************************************************************************\n",
      "epoch: 764 / 800, time cost: 47.14 sec, \n",
      "          Loss: [Train: 2.07461], [Test: 2.53572],\n",
      "          Accuracy: [prop score:  0.95675], [q1: 0.53982], [q0: 0.69430],\n",
      "          Effect: [ate-q], [train: 0.04318], [test: 0.04550]\n",
      "********************************************************************************\n",
      "epoch: 765 / 800, time cost: 45.52 sec, \n",
      "          Loss: [Train: 2.07421], [Test: 2.53892],\n",
      "          Accuracy: [prop score:  0.95682], [q1: 0.50756], [q0: 0.68307],\n",
      "          Effect: [ate-q], [train: 0.02865], [test: 0.03086]\n",
      "********************************************************************************\n",
      "epoch: 766 / 800, time cost: 47.67 sec, \n",
      "          Loss: [Train: 2.07372], [Test: 2.53376],\n",
      "          Accuracy: [prop score:  0.95681], [q1: 0.59864], [q0: 0.65336],\n",
      "          Effect: [ate-q], [train: 0.05392], [test: 0.05623]\n",
      "********************************************************************************\n",
      "epoch: 767 / 800, time cost: 45.67 sec, \n",
      "          Loss: [Train: 2.07323], [Test: 2.53552],\n",
      "          Accuracy: [prop score:  0.95681], [q1: 0.57077], [q0: 0.71911],\n",
      "          Effect: [ate-q], [train: 0.06088], [test: 0.06321]\n",
      "********************************************************************************\n",
      "epoch: 768 / 800, time cost: 45.96 sec, \n",
      "          Loss: [Train: 2.07257], [Test: 2.53469],\n",
      "          Accuracy: [prop score:  0.95688], [q1: 0.61867], [q0: 0.67976],\n",
      "          Effect: [ate-q], [train: 0.06696], [test: 0.06929]\n",
      "********************************************************************************\n",
      "epoch: 769 / 800, time cost: 48.22 sec, \n",
      "          Loss: [Train: 2.07206], [Test: 2.53846],\n",
      "          Accuracy: [prop score:  0.95681], [q1: 0.55769], [q0: 0.69596],\n",
      "          Effect: [ate-q], [train: 0.04770], [test: 0.05001]\n",
      "********************************************************************************\n",
      "epoch: 770 / 800, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.07307], [Test: 2.53977],\n",
      "          Accuracy: [prop score:  0.95688], [q1: 0.55583], [q0: 0.68161],\n",
      "          Effect: [ate-q], [train: 0.04204], [test: 0.04431]\n",
      "********************************************************************************\n",
      "epoch: 771 / 800, time cost: 48.03 sec, \n",
      "          Loss: [Train: 2.07303], [Test: 2.53698],\n",
      "          Accuracy: [prop score:  0.95686], [q1: 0.61354], [q0: 0.63196],\n",
      "          Effect: [ate-q], [train: 0.05418], [test: 0.05653]\n",
      "********************************************************************************\n",
      "epoch: 772 / 800, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.07133], [Test: 2.54038],\n",
      "          Accuracy: [prop score:  0.95681], [q1: 0.55266], [q0: 0.66944],\n",
      "          Effect: [ate-q], [train: 0.04036], [test: 0.04262]\n",
      "********************************************************************************\n",
      "epoch: 773 / 800, time cost: 46.11 sec, \n",
      "          Loss: [Train: 2.07107], [Test: 2.53776],\n",
      "          Accuracy: [prop score:  0.95678], [q1: 0.62522], [q0: 0.66239],\n",
      "          Effect: [ate-q], [train: 0.06606], [test: 0.06842]\n",
      "********************************************************************************\n",
      "epoch: 774 / 800, time cost: 47.97 sec, \n",
      "          Loss: [Train: 2.07097], [Test: 2.53973],\n",
      "          Accuracy: [prop score:  0.95697], [q1: 0.61255], [q0: 0.66810],\n",
      "          Effect: [ate-q], [train: 0.06159], [test: 0.06390]\n",
      "********************************************************************************\n",
      "epoch: 775 / 800, time cost: 46.39 sec, \n",
      "          Loss: [Train: 2.07025], [Test: 2.54133],\n",
      "          Accuracy: [prop score:  0.95690], [q1: 0.53465], [q0: 0.65718],\n",
      "          Effect: [ate-q], [train: 0.03325], [test: 0.03550]\n",
      "********************************************************************************\n",
      "epoch: 776 / 800, time cost: 48.41 sec, \n",
      "          Loss: [Train: 2.07028], [Test: 2.54055],\n",
      "          Accuracy: [prop score:  0.95693], [q1: 0.59230], [q0: 0.69351],\n",
      "          Effect: [ate-q], [train: 0.05986], [test: 0.06219]\n",
      "********************************************************************************\n",
      "epoch: 777 / 800, time cost: 46.05 sec, \n",
      "          Loss: [Train: 2.07056], [Test: 2.54204],\n",
      "          Accuracy: [prop score:  0.95708], [q1: 0.55822], [q0: 0.69516],\n",
      "          Effect: [ate-q], [train: 0.04669], [test: 0.04895]\n",
      "********************************************************************************\n",
      "epoch: 778 / 800, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.06961], [Test: 2.54253],\n",
      "          Accuracy: [prop score:  0.95693], [q1: 0.58894], [q0: 0.69535],\n",
      "          Effect: [ate-q], [train: 0.05610], [test: 0.05848]\n",
      "********************************************************************************\n",
      "epoch: 779 / 800, time cost: 48.47 sec, \n",
      "          Loss: [Train: 2.06918], [Test: 2.54272],\n",
      "          Accuracy: [prop score:  0.95697], [q1: 0.56481], [q0: 0.69935],\n",
      "          Effect: [ate-q], [train: 0.05119], [test: 0.05341]\n",
      "********************************************************************************\n",
      "epoch: 780 / 800, time cost: 46.02 sec, \n",
      "          Loss: [Train: 2.06942], [Test: 2.54355],\n",
      "          Accuracy: [prop score:  0.95694], [q1: 0.58860], [q0: 0.64052],\n",
      "          Effect: [ate-q], [train: 0.04559], [test: 0.04794]\n",
      "********************************************************************************\n",
      "epoch: 781 / 800, time cost: 48.21 sec, \n",
      "          Loss: [Train: 2.06831], [Test: 2.54309],\n",
      "          Accuracy: [prop score:  0.95697], [q1: 0.56522], [q0: 0.68653],\n",
      "          Effect: [ate-q], [train: 0.05144], [test: 0.05379]\n",
      "********************************************************************************\n",
      "epoch: 782 / 800, time cost: 46.20 sec, \n",
      "          Loss: [Train: 2.06837], [Test: 2.54572],\n",
      "          Accuracy: [prop score:  0.95697], [q1: 0.55503], [q0: 0.66835],\n",
      "          Effect: [ate-q], [train: 0.04023], [test: 0.04241]\n",
      "********************************************************************************\n",
      "epoch: 783 / 800, time cost: 46.14 sec, \n",
      "          Loss: [Train: 2.06777], [Test: 2.54440],\n",
      "          Accuracy: [prop score:  0.95682], [q1: 0.57247], [q0: 0.74339],\n",
      "          Effect: [ate-q], [train: 0.06956], [test: 0.07188]\n",
      "********************************************************************************\n",
      "epoch: 784 / 800, time cost: 48.11 sec, \n",
      "          Loss: [Train: 2.06837], [Test: 2.54478],\n",
      "          Accuracy: [prop score:  0.95696], [q1: 0.57613], [q0: 0.68078],\n",
      "          Effect: [ate-q], [train: 0.05053], [test: 0.05278]\n",
      "********************************************************************************\n",
      "epoch: 785 / 800, time cost: 53.31 sec, \n",
      "          Loss: [Train: 2.06797], [Test: 2.54488],\n",
      "          Accuracy: [prop score:  0.95690], [q1: 0.60152], [q0: 0.65252],\n",
      "          Effect: [ate-q], [train: 0.05326], [test: 0.05563]\n",
      "********************************************************************************\n",
      "epoch: 786 / 800, time cost: 55.08 sec, \n",
      "          Loss: [Train: 2.06768], [Test: 2.54636],\n",
      "          Accuracy: [prop score:  0.95687], [q1: 0.57705], [q0: 0.69686],\n",
      "          Effect: [ate-q], [train: 0.05592], [test: 0.05831]\n",
      "********************************************************************************\n",
      "epoch: 787 / 800, time cost: 48.38 sec, \n",
      "          Loss: [Train: 2.06704], [Test: 2.54719],\n",
      "          Accuracy: [prop score:  0.95689], [q1: 0.57365], [q0: 0.63454],\n",
      "          Effect: [ate-q], [train: 0.03773], [test: 0.04003]\n",
      "********************************************************************************\n",
      "epoch: 788 / 800, time cost: 46.12 sec, \n",
      "          Loss: [Train: 2.06751], [Test: 2.54819],\n",
      "          Accuracy: [prop score:  0.95704], [q1: 0.58382], [q0: 0.63639],\n",
      "          Effect: [ate-q], [train: 0.04340], [test: 0.04571]\n",
      "********************************************************************************\n",
      "epoch: 789 / 800, time cost: 48.15 sec, \n",
      "          Loss: [Train: 2.06664], [Test: 2.54812],\n",
      "          Accuracy: [prop score:  0.95706], [q1: 0.59216], [q0: 0.67684],\n",
      "          Effect: [ate-q], [train: 0.05479], [test: 0.05712]\n",
      "********************************************************************************\n",
      "epoch: 790 / 800, time cost: 45.75 sec, \n",
      "          Loss: [Train: 2.06579], [Test: 2.55039],\n",
      "          Accuracy: [prop score:  0.95709], [q1: 0.57340], [q0: 0.65084],\n",
      "          Effect: [ate-q], [train: 0.04132], [test: 0.04365]\n",
      "********************************************************************************\n",
      "epoch: 791 / 800, time cost: 48.17 sec, \n",
      "          Loss: [Train: 2.06565], [Test: 2.54674],\n",
      "          Accuracy: [prop score:  0.95695], [q1: 0.58973], [q0: 0.69654],\n",
      "          Effect: [ate-q], [train: 0.06016], [test: 0.06250]\n",
      "********************************************************************************\n",
      "epoch: 792 / 800, time cost: 46.36 sec, \n",
      "          Loss: [Train: 2.06520], [Test: 2.54876],\n",
      "          Accuracy: [prop score:  0.95702], [q1: 0.60136], [q0: 0.68978],\n",
      "          Effect: [ate-q], [train: 0.06341], [test: 0.06579]\n",
      "********************************************************************************\n",
      "epoch: 793 / 800, time cost: 46.60 sec, \n",
      "          Loss: [Train: 2.06557], [Test: 2.54912],\n",
      "          Accuracy: [prop score:  0.95704], [q1: 0.58724], [q0: 0.70619],\n",
      "          Effect: [ate-q], [train: 0.06432], [test: 0.06671]\n",
      "********************************************************************************\n",
      "epoch: 794 / 800, time cost: 47.79 sec, \n",
      "          Loss: [Train: 2.06496], [Test: 2.55128],\n",
      "          Accuracy: [prop score:  0.95699], [q1: 0.55997], [q0: 0.64574],\n",
      "          Effect: [ate-q], [train: 0.03711], [test: 0.03944]\n",
      "********************************************************************************\n",
      "epoch: 795 / 800, time cost: 44.90 sec, \n",
      "          Loss: [Train: 2.06469], [Test: 2.55514],\n",
      "          Accuracy: [prop score:  0.95694], [q1: 0.52733], [q0: 0.63608],\n",
      "          Effect: [ate-q], [train: 0.02571], [test: 0.02807]\n",
      "********************************************************************************\n",
      "epoch: 796 / 800, time cost: 48.40 sec, \n",
      "          Loss: [Train: 2.06458], [Test: 2.55390],\n",
      "          Accuracy: [prop score:  0.95692], [q1: 0.54083], [q0: 0.67139],\n",
      "          Effect: [ate-q], [train: 0.03624], [test: 0.03857]\n",
      "********************************************************************************\n",
      "epoch: 797 / 800, time cost: 44.40 sec, \n",
      "          Loss: [Train: 2.06390], [Test: 2.55195],\n",
      "          Accuracy: [prop score:  0.95691], [q1: 0.55936], [q0: 0.69679],\n",
      "          Effect: [ate-q], [train: 0.05209], [test: 0.05450]\n",
      "********************************************************************************\n",
      "epoch: 798 / 800, time cost: 44.28 sec, \n",
      "          Loss: [Train: 2.06372], [Test: 2.55185],\n",
      "          Accuracy: [prop score:  0.95700], [q1: 0.60599], [q0: 0.70726],\n",
      "          Effect: [ate-q], [train: 0.06843], [test: 0.07088]\n",
      "********************************************************************************\n",
      "epoch: 799 / 800, time cost: 47.87 sec, \n",
      "          Loss: [Train: 2.06358], [Test: 2.55477],\n",
      "          Accuracy: [prop score:  0.95697], [q1: 0.57132], [q0: 0.59454],\n",
      "          Effect: [ate-q], [train: 0.03156], [test: 0.03391]\n",
      "********************************************************************************\n",
      "epoch: 800 / 800, time cost: 45.33 sec, \n",
      "          Loss: [Train: 2.06312], [Test: 2.55564],\n",
      "          Accuracy: [prop score:  0.95699], [q1: 0.55992], [q0: 0.64874],\n",
      "          Effect: [ate-q], [train: 0.03981], [test: 0.04205]\n",
      "********************************************************************************\n",
      "Finish training...\n"
     ]
    }
   ],
   "source": [
    "for e in range(501, 801):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "    for idx, (tokens, treatment, response, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        prop_score, q1, q0 = model(tokens)\n",
    "        \n",
    "        loss = prop_score_loss(prop_score, treatment)\n",
    "        if len(response[treatment == 1]) > 0:\n",
    "            loss += q_loss(q1[treatment==1], response[treatment==1])# * pos_weight\n",
    "            \n",
    "        if len(response[treatment == 0]) > 0:\n",
    "            loss += q_loss(q0[treatment==0], response[treatment==0])\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        run_loss += loss.item()\n",
    "    run_idx = idx\n",
    "\n",
    "    # Evaluation.\n",
    "    train_loss = run_loss / (run_idx + 1)\n",
    "    train_effect, _, _, _, _, _, _ = est_casual_effect(train_loader, model, effect, estimation, evaluate=False)\n",
    "    test_effect, g_loss_test, q1_loss_test, q0_loss_test, prop_accu_test, q1_accu_test, q0_accu_test = est_casual_effect(test_loader, model, effect, estimation, evaluate=True, g_loss=prop_score_loss, q_loss=q_loss)\n",
    "    test_loss = q1_loss_test + q0_loss_test\n",
    "    test_loss += g_loss_test\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    est_effect.append(test_effect)\n",
    "    print(f'''epoch: {e} / 800, time cost: {(time.time() - start):.2f} sec, \n",
    "          Loss: [Train: {train_loss:.5f}], [Test: {test_loss:.5f}],\n",
    "          Accuracy: [prop score: {prop_accu_test: .5f}], [q1: {q1_accu_test:.5f}], [q0: {q0_accu_test:.5f}],\n",
    "          Effect: [{effect}-{estimation}], [train: {train_effect:.5f}], [test: {test_effect:.5f}]''')\n",
    "    print('*'* 80)\n",
    "    start = time.time()\n",
    "    run_loss = 0.\n",
    "\n",
    "print('Finish training...')\n",
    "\n",
    "# With only 1 group(s) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "real = true_casual_effect(test_loader)\n",
    "unadjust = (testset.response[testset.treatment == 1].mean() - testset.response[testset.treatment == 0].mean()).item()\n",
    "show_result(train_loss_hist, test_loss_hist, est_effect, real, unadjust, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
