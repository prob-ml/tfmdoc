{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import scipy.stats\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch import distributions\n",
    "from matplotlib.pylab import plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score, roc_curve, auc\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(y_test, y_prob):\n",
    "\n",
    "    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(false_positive_rate, true_positive_rate, color = 'red', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1], linestyle = '--')\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4\n",
    "\n",
    "$\\ Y_i $ ~ $Bernoulli(1/2)$\\\n",
    "$\\ Z_{it}|Z_{i,t-1}, Y_i$ ~ $\\ N$($0.9 Z_{i,t-1}$,$50.1^{Y_i}$)\\\n",
    "$\\ X_{it}|Z_{i,t}$ ~ $\\ Categorical$($\\sigma(\\Psi Z_{i,t}$))\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structural constants\n",
    "xdim = 100  # number of billing codes\n",
    "N = 2000  # number of patients\n",
    "T = 2  # total time steps\n",
    "\n",
    "# model parameters\n",
    "success_prob = 0.5\n",
    "Psi = torch.arange(0, xdim * 0.01, 0.01)\n",
    "Omega = torch.tensor((1., 50.1))\n",
    "\n",
    "# random variables\n",
    "Z_true = torch.zeros((N, T)) # latent \n",
    "X = torch.zeros((N, T)) # observed\n",
    "X_onehot = torch.zeros((N, T, xdim))\n",
    "Y = torch.distributions.Bernoulli(success_prob).sample((N,))\n",
    "\n",
    "for t in range(0,T):\n",
    "    # Zit | Zi,t-1, Yi\n",
    "    meanz = (0.9 * Z_true[:, t - 1]) if t != 0 else torch.zeros((N,))\n",
    "    zvar_true = Omega[0] * (1 - Y) + Omega[1] * Y\n",
    "    Zt = Normal(meanz, zvar_true.sqrt())\n",
    "    Z_true[:, t] = Zt.sample()\n",
    "\n",
    "    # Xit | Zit\n",
    "    Psi_z = Z_true[:,t].view(N, 1) * Psi.view(1, xdim)\n",
    "    PX = F.softmax(Psi_z, dim = 1)\n",
    "    Xt = Categorical(PX)\n",
    "    Xit = Xt.sample()\n",
    "    X[:, t] = Xit\n",
    "    X_onehot[:, t] = F.one_hot(Xit, num_classes = xdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(N)\n",
    "train_idx = idx[:math.floor(0.8*N)]\n",
    "test_idx = idx[math.floor(0.8*N):]\n",
    "X_train = X_onehot[train_idx,:,:]\n",
    "X_test = X_onehot[test_idx,:,:]\n",
    "xd1, xd2, xd3 = X_train.shape\n",
    "X_train_lr = X_train.view(xd1, xd2*xd3)\n",
    "xd1, xd2, xd3 = X_test.shape\n",
    "X_test_lr = X_test.view(xd1, xd2*xd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Y[train_idx]\n",
    "y_test = Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_accuracy = torch.zeros(T)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_lr, y_train)\n",
    "y_prob_lr = clf.predict_proba(X_test_lr)[:,1]\n",
    "y_pred_lr = 1*(y_prob_lr > 0.5)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test,y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression accuracy in test:\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(y_test, y_prob_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical integration approach (IS)\n",
    "\n",
    "$\\ Z_{it}^{(k)}|Z_{i,t-1}^{(k)}, Y_i$ ~ $\\ N$($0.9 Z_{i,t-1}^{(k)}$,$50.1^{Y_i}$)\\\n",
    "$\\ X_{it}|Z_{i,t}$ ~ $\\ Categorical$($\\sigma(\\Psi Z_{i,t}$))\n",
    "\n",
    "$P(X|Y = 1) = E_{P(Z|Y = 1)} [P(X|Z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} P(X|Z^{(k)}),  Z^{(k)} \\sim P(Z^{(k)}|Y = 1)$\\\n",
    "$P(X|Y = 0) = E_{P(Z|Y = 0)} [P(X|Z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} P(X|Z^{(k)}),  Z^{(k)} \\sim P(Z^{(k)}|Y = 0)$\n",
    "\n",
    "$P(Y = 1 |X) = \\frac{P(X|Y = 1)P(Y=1)}{P(X|Y = 1)P(Y=1) + P(X|Y = 0)P(Y=0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Inference Z and X generation\n",
    "$\\ Z_{it}^{(k)}|Z_{i,t-1}^{(k)}, Y_i$ ~ $\\ N$($0.9 Z_{i,t-1}^{(k)}$,$50.1^{Y_i}$)\\\n",
    "$\\ X_{it}|Z_{i,t}$ ~ $\\ Categorical$($\\sigma(\\Psi Z_{i,t}$))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1000 # number of simulations\n",
    "\n",
    "Z = torch.zeros((K, N, T)) \n",
    "logPX_y = torch.zeros((2, K, N, T))\n",
    "\n",
    "for t in range(0, T):\n",
    "    for y in range(2):\n",
    "        # Zit | Zi,t-1, y\n",
    "        meanz = (0.9 * Z[:, :, t - 1]) if t != 0 else torch.zeros((K, N))\n",
    "        var_y = Omega[y]\n",
    "        Zt = Normal(meanz, var_y.sqrt())\n",
    "        Z[:, :, t] = Zt.sample()\n",
    "\n",
    "        # Xit | Zit\n",
    "        Psi_z = Z[:, :, t].view(K, N, 1) * Psi.view(1, 1, xdim)\n",
    "        PX = F.softmax(Psi_z, dim = 2)\n",
    "        Xt = Categorical(PX)\n",
    "        logPX_y[y, :, :, t] = Xt.log_prob(X[:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Approximate P(X|Y)\n",
    "$P(X|Y = 1) = E_{P(Z|Y = 1)} [P(X|Z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} P(X|Z^{(k)}),  Z^{(k)} \\sim P(Z^{(k)}|Y = 1)$\\\n",
    "$P(X|Y = 0) = E_{P(Z|Y = 0)} [P(X|Z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} P(X|Z^{(k)}),  Z^{(k)} \\sim P(Z^{(k)}|Y = 0)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElogPX_1k = torch.logsumexp(logPX_y[1], dim = 0) - np.log(K)\n",
    "ElogPX_1 = torch.mean(ElogPX_1k,dim = 1)\n",
    "\n",
    "ElogPX_0k = torch.logsumexp(logPX_y[0], dim = 0) - np.log(K)\n",
    "ElogPX_0 = torch.mean(ElogPX_0k,dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Posterior distribution\n",
    "$P(Y = 1 |X) = \\frac{P(X|Y = 1)P(Y=1)}{P(X|Y = 1)P(Y=1) + P(X|Y = 0)P(Y=0)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logPX_Y1 = ElogPX_1 + np.log(success_prob)\n",
    "logPX_Y0 = ElogPX_0 + np.log(1-success_prob)\n",
    "logPY1_X = logPX_Y1 - (logPX_Y1.exp() + logPX_Y0.exp()).log()\n",
    "logPY1_X.exp()\n",
    "Y_pred = 1*(logPY1_X.exp() > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical integration classification accuracy in testing:\",torch.mean((Y_pred[test_idx] == y_test).float()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(y_test, logPY1_X.exp()[[test_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5\n",
    "\n",
    "\n",
    "$\\ Z_{it}|Z_{i,t-1}$ ~ $\\ N$($0.9 Z_{i,t-1}$,$1$)\\\n",
    "$\\exists t, \\ Y_i | Z_i = \\mathcal{I} \\{Z_{i,t} \\in [1,3]\\} * \\mathcal{I} \\{Z_{i,t-1} \\in [1,3]\\} * \\mathcal{I} \\{Z_{i,t-2} \\in [1,3]\\} $\\\n",
    "$\\ X_{it}|Z_{i,t}$ ~ $\\ Categorical$($\\sigma(\\Psi Z_{i,t}$))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_consecutive(z,a=1,b=3):\n",
    "    return (z >= a) * (z <= b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdim = 100  # number of billing codes\n",
    "N = 2000  # number of patients\n",
    "T = 5  # total time steps\n",
    "\n",
    "# model parameters\n",
    "Psi = torch.arange(0, xdim * 0.01, 0.01)\n",
    "\n",
    "# random variables\n",
    "Z_true = torch.zeros((N, T)) # latent \n",
    "X = torch.zeros((N, T)) # observed\n",
    "X_onehot = torch.zeros((N, T, xdim))\n",
    "Y = torch.zeros((N,1))\n",
    "\n",
    "for t in range(0,T):\n",
    "    # Zit | Zi,t-1, Yi\n",
    "    meanz = (0.9 * Z_true[:, t - 1]) if t != 0 else torch.zeros((N,))\n",
    "    Zt = Normal(meanz, 1)\n",
    "    Z_true[:, t] = Zt.sample()\n",
    "    \n",
    "    # Xit | Zit\n",
    "    Psi_z = Z_true[:,t].view(N, 1) * Psi.view(1, xdim)\n",
    "    PX = F.softmax(Psi_z, dim = 1)\n",
    "    Xt = Categorical(PX)\n",
    "    Xit = Xt.sample()\n",
    "    X[:, t] = Xit\n",
    "    X_onehot[:, t] = F.one_hot(Xit, num_classes = xdim)\n",
    "\n",
    "for t in range(T-2):\n",
    "    Y[:,0] += is_consecutive(Z_true[:,t]) * is_consecutive(Z_true[:,t+1]) * is_consecutive(Z_true[:,t+2])\n",
    "\n",
    "# truncate the Y values which are greater than 1.\n",
    "Y = torch.cat((Y, torch.ones((N,1))), 1).min(dim = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(N)\n",
    "train_idx = idx[:math.floor(0.8*N)]\n",
    "test_idx = idx[math.floor(0.8*N):]\n",
    "X_train = X_onehot[train_idx,:,:]\n",
    "X_test = X_onehot[test_idx,:,:]\n",
    "xd1, xd2, xd3 = X_train.shape\n",
    "X_train_lr = X_train.view(xd1, xd2*xd3)\n",
    "xd1, xd2, xd3 = X_test.shape\n",
    "X_test_lr = X_test.view(xd1, xd2*xd3)\n",
    "y_train = Y[train_idx]\n",
    "y_test = Y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_accuracy = torch.zeros(T)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train_lr, y_train)\n",
    "y_prob_lr = clf.predict_proba(X_test_lr)[:,1]\n",
    "y_pred_lr = 1*(y_prob_lr > 0.5)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test,y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression accuracy in test:\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(y_test, y_prob_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Importance Sampling approach\n",
    "\n",
    "\n",
    "$\\ Z_{it}^{(k)}|Z_{i,t-1}^{(k)}$ ~ $\\ N$($0.9 Z_{i,t-1}^{(k)}$,$1$)\\\n",
    "$\\exists t, \\ Y_i^{(k)} | Z_i^{(k)} = \\mathcal{I} \\{Z_{i,t}^{(k)} \\in [1,3]\\} * \\mathcal{I} \\{Z_{i,t-1}^{(k)} \\in [1,3]\\} * \\mathcal{I} \\{Z_{i,t-2}^{(k)} \\in [1,3]\\} $\\\n",
    "$\\ X_{it}|Z_{i,t}^{(k)}$ ~ $\\ Categorical$($\\sigma(\\Psi Z_{i,t}^{(k)}$))\n",
    "\n",
    "$P(X|Y = 1) = E_{P(Z|Y = 1)} [P(X|Z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} P(X|Z^{(k)}),  Z^{(k)} \\sim P(Z^{(k)}|Y^{(k)} = 1)$\\\n",
    "$P(X|Y = 0) = E_{P(Z|Y = 0)} [P(X|Z)] \\approx \\frac{1}{K} \\sum_{k=1}^{K} P(X|Z^{(k)}),  Z^{(k)} \\sim P(Z^{(k)}|Y^{(k)} = 0)$\n",
    "\n",
    "$P(Y = 1 |X) = \\frac{P(X|Y = 1)P(Y=1)}{P(X|Y = 1)P(Y=1) + P(X|Y = 0)P(Y=0)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1000 # number of simulations\n",
    "\n",
    "Z = torch.zeros((K, N, T)) \n",
    "Y = torch.zeros((K, N, 1))\n",
    "logPX_y = torch.zeros((2, K, N, T))\n",
    "\n",
    "for t in range(T):\n",
    "    # Zit | Zi,t-1\n",
    "    meanz = (0.9 * Z[:, :, t - 1]) if t != 0 else torch.zeros((K, N))\n",
    "    sd_y = 1\n",
    "    Zt = Normal(meanz, sd_y)\n",
    "    Z[:, :, t] = Zt.sample()\n",
    "    \n",
    "    if t < T-2:\n",
    "        # Yi | Zi\n",
    "        Y[:, :,0] += is_consecutive(Z[:, :,t]) * is_consecutive(Z[:, :,t+1]) * is_consecutive(Z[:, :,t+2])\n",
    "        \n",
    "\n",
    "for t in range(T):\n",
    "    for y in range(2):\n",
    "        Y_2 = y*(Y >= 1) + (1-y)*(Y < 1)\n",
    "        # Xit | Zit\n",
    "        Psi_z = Y_2 * Z[:, :, t].view(K, N, 1) * Psi.view(1, 1, xdim)\n",
    "        PX = F.softmax(Psi_z, dim = 2)\n",
    "        Xt = Categorical(PX)\n",
    "        logPX_y[y, :, :, t] = Xt.log_prob(X[:, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElogPX_1k = torch.logsumexp(logPX_y[1], dim = 0) - np.log(K)\n",
    "ElogPX_1 = torch.mean(ElogPX_1k,dim = 1)\n",
    "\n",
    "ElogPX_0k = torch.logsumexp(logPX_y[0], dim = 0) - np.log(K)\n",
    "ElogPX_0 = torch.mean(ElogPX_0k,dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logPX_Y1 = ElogPX_1 + np.log(success_prob)\n",
    "logPX_Y0 = ElogPX_0 + np.log(1-success_prob)\n",
    "logPY1_X = logPX_Y1 - (logPX_Y1.exp() + logPX_Y0.exp()).log()\n",
    "logPY1_X.exp()\n",
    "Y_pred = 1*(logPY1_X.exp() > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numerical integration classification accuracy in testing:\",torch.mean((Y_pred[test_idx] == y_test).float()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC(y_test, logPY1_X.exp()[[test_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
